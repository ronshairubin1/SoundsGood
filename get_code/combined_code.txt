SoundClassifier_v08
└── .
    .
    ├── config
    │   ├── analysis
    │   │   └── analysis_THREE_WORDS_20250122_232557.json
    │   ├── dictionaries.json
    │   └── active_dictionary.json
    ├── get_code
    │   ├── concat_code.py
    │   ├── files_to_concat.py
    │   └── create_code_tree.py
    ├── models/ (10 files)
    ├── src
    │   ├── checkpoints
    │   │   └── model_checkpoint.h5
    │   ├── ml
    │   │   ├── __init__.py
    │   │   ├── audio_processing.py
    │   │   ├── augmentation_manager.py
    │   │   ├── cnn_classifier.py
    │   │   ├── constants.py
    │   │   ├── data_augmentation.py
    │   │   ├── ensemble_classifier.py
    │   │   ├── feature_extractor.py
    │   │   ├── inference.py
    │   │   ├── model_paths.py
    │   │   ├── rf_classifier.py
    │   │   ├── sound_detector_ensemble.py
    │   │   ├── sound_detector_rf.py
    │   │   └── trainer.py
    │   ├── routes
    │   │   ├── __init__.py
    │   │   ├── ml_routes.py
    │   │   └── train_app.py
    │   ├── static
    │   │   ├── css
    │   │   │   └── style.css
    │   │   ├── goodsounds
    │   │   │   ├── ah
    │   │   │   ├── bah
    │   │   │   ├── bee
    │   │   │   ├── beh
    │   │   │   ├── boh
    │   │   │   ├── boo
    │   │   │   ├── ee
    │   │   │   ├── eh
    │   │   │   ├── lah
    │   │   │   ├── lee
    │   │   │   ├── leh
    │   │   │   ├── loh
    │   │   │   ├── loo
    │   │   │   ├── mah
    │   │   │   ├── mee
    │   │   │   ├── meh
    │   │   │   ├── moh
    │   │   │   ├── moo
    │   │   │   ├── oh
    │   │   │   ├── oo
    │   │   │   └── ooah
    │   │   └── temp
    │   ├── templates
    │   │   ├── css
    │   │   │   └── style.css
    │   │   ├── 404.html
    │   │   ├── base.html
    │   │   ├── index.html
    │   │   ├── inference.html
    │   │   ├── inference_statistics.html
    │   │   ├── list_recordings.html
    │   │   ├── login.html
    │   │   ├── manage_dictionaries.html
    │   │   ├── model_status.html
    │   │   ├── model_summary.html
    │   │   ├── model_summary_enhanced.html
    │   │   ├── process_flow.html
    │   │   ├── record.html
    │   │   ├── register.html
    │   │   ├── train_model.html
    │   │   ├── upload_sounds.html
    │   │   ├── verify.html
    │   │   └── view_analysis.html
    │   ├── app.py
    │   ├── audio_chunker.py
    │   ├── config.py
    │   ├── main_app.py
    │   └── test_mic.py
    ├── .cursorignore
    ├── .python-version
    ├── SoundClassifiersv08.code-workspace
    ├── app_output.log
    ├── run.py
    ├── Miniconda3-latest-MacOSX-arm64.sh
    ├── requirements.txt
    ├── conda-requirements.txt
    └── Routes.txt


### File: run.py ###

import os
import sys
import logging
from multiprocessing import Process

# Add the src directory to Python path
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

project_root = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(project_root, 'src')
sys.path.insert(0, src_path)

from app import app       # your main Flask app
from src.routes.train_app import train_app  
# import theseparate training app

def run_main_app():
    """
    Runs the main Flask app in debug mode on port 5001.
    """
    app.run(debug=True, port=5001, use_reloader=False)

def run_train_app():
    """
    Runs the separate training Flask app without debug mode on port 5002.
    """
    train_app.run(debug=False, port=5002, use_reloader=False)

def main():
    logging.basicConfig(level=logging.DEBUG)

    # Spawn the training app in a separate process
    p = Process(target=run_train_app)
    p.daemon = True
    p.start()

    # Run the main app in the main thread
    run_main_app()

if __name__ == "__main__":
    main()
********************************************************************************

### File: src/app.py ###

import os
import io
import json
import logging
import threading
from datetime import datetime

from flask import (
    Flask, render_template, request, session, redirect,
    url_for, flash, jsonify
)
from flask_cors import CORS

from config import Config  # Use the real config.py

# Import our new ML blueprint
from routes.ml_routes import ml_bp

# --------------------------------------------------------------------
# Set up Flask
# --------------------------------------------------------------------
app = Flask(
    __name__,
    static_url_path='/static',
    static_folder=os.path.join(Config.CURRENT_DIR, 'static'),
    template_folder=os.path.join(Config.CURRENT_DIR, 'templates')
)
app.secret_key = 'your-secret-key'
CORS(app, supports_credentials=True)

# Register the ML blueprint at /ml
app.register_blueprint(ml_bp, url_prefix='/ml')

# --------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------
logging.basicConfig(level=logging.DEBUG)
app.logger.debug("Starting Sound Classifier app.py (no ML duplication)...")
app.logger.debug(f"Template folder: {app.template_folder}")
app.logger.debug(f"Static folder: {app.static_folder}")

# --------------------------------------------------------------------
# Initialize directories
# --------------------------------------------------------------------
Config.init_directories()

# --------------------------------------------------------------------
# Basic Routes: index, login, logout, register
# --------------------------------------------------------------------
@app.route('/')
def index():
    # If not logged in, ask for login
    if 'username' not in session:
        return render_template('login.html')
    # If logged in, go to record page by default
    return render_template('record.html', sounds=Config.get_dictionary()['sounds'])

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'GET':
        return render_template('login.html')

    # Distinguish admin vs user
    if request.form.get('type') == 'admin':
        if request.form.get('password') == 'Michal':
            session['username'] = 'admin'
            session['is_admin'] = True
        else:
            flash('Invalid admin password')
            return render_template('login.html')
    else:
        username = request.form.get('username')
        if username:
            session['username'] = username
            session['is_admin'] = False
        else:
            flash('Username required')
            return render_template('login.html')
    return redirect(url_for('index'))

@app.route('/logout')
def logout():
    session.clear()
    return redirect(url_for('index'))

@app.route('/register', methods=['GET', 'POST'])
def register():
    """Simple registration route (demo only)."""
    if request.method == 'POST':
        # In a real app, you'd store the new user in a database
        username = request.form.get('username')
        if not username:
            flash("Please provide a username")
            return render_template('register.html')
        # Just log them in
        session['username'] = username
        session['is_admin'] = False
        flash(f"User '{username}' registered and logged in")
        return redirect(url_for('index'))
    return render_template('register.html')

# Basic test route
# --------------------------------------------------------------------
@app.route('/test')
def test():
    return "Server is working!"
# --------------------------------------------------------------------
# Before request: check login
# --------------------------------------------------------------------
@app.before_request
def check_login():
    if request.endpoint in ['login', 'register', 'static', 'test']:
        return
    if 'ml.' in str(request.endpoint):
        # ML routes can also require login
        pass
    if 'username' not in session:
        return redirect(url_for('login'))

# --------------------------------------------------------------------
# Error handlers
# --------------------------------------------------------------------
@app.errorhandler(403)
def forbidden_error(e):
    return redirect(url_for('login'))

@app.errorhandler(401)
def unauthorized_error(e):
    return redirect(url_for('login'))

@app.errorhandler(404)
def not_found_error(e):
    return render_template('404.html'), 404

********************************************************************************

### File: src/audio_chunker.py ###

# File: SoundClassifier_v08/src/audio_chunker.py

import os
import numpy as np
from scipy.io import wavfile

class SoundProcessor:
    def __init__(self):
        self.min_chunk_duration = 0.2
        self.silence_threshold = 0.1
        self.min_silence_duration = 0.1
        self.max_silence_duration = 2.0

    def chop_recording(self, filename):
        """
        Splits a single wav file into smaller chunks based on silence detection,
        saving them to disk, then returning the list of chunk filenames.
        """
        print(f"Processing file: {filename}")
        rate, data = wavfile.read(filename)
        if len(data.shape) > 1:
            data = np.mean(data, axis=1)

        data = data / np.max(np.abs(data))

        is_silence = np.abs(data) < self.silence_threshold
        print(f"Found {np.sum(~is_silence)} non-silent samples out of {len(data)}")
        print(f"Silence threshold: {self.silence_threshold}")

        silence_starts = []
        silence_ends = []
        current_silence_start = None

        for i, val in enumerate(is_silence):
            if val and current_silence_start is None:
                current_silence_start = i
            elif not val and current_silence_start is not None:
                silence_duration = (i - current_silence_start) / rate
                if self.min_silence_duration <= silence_duration <= self.max_silence_duration:
                    silence_starts.append(current_silence_start)
                    silence_ends.append(i)
                    print(f"Found silence: {silence_duration:.2f}s")
                current_silence_start = None

        print(f"Found {len(silence_starts)} valid silences")

        chunk_starts = []
        chunk_ends = []
        if not silence_starts and (len(data)/rate > self.min_chunk_duration):
            chunk_starts = [0]
            chunk_ends = [len(data)]
        else:
            if silence_starts:
                chunk_starts.append(0)
            for s, e in zip(silence_starts, silence_ends):
                chunk_ends.append(s)
                chunk_starts.append(e)
            if silence_ends:
                chunk_ends.append(len(data))
        
        chunk_files = []
        for i, (start, end) in enumerate(zip(chunk_starts, chunk_ends)):
            duration = (end - start) / rate
            print(f"Chunk {i}: duration = {duration:.2f}s")
            if duration > self.min_chunk_duration:
                chunk_filename = filename.replace('.wav', f'_chunk_{i}.wav')
                self._save_chunk(data[start:end], rate, chunk_filename)
                chunk_files.append(os.path.basename(chunk_filename))
            else:
                print(f"Rejecting chunk {i}: too short ({duration:.2f}s)")

        return chunk_files

    def _save_chunk(self, data, rate, filename):
        data_out = data * 32767
        wavfile.write(filename, rate, data_out.astype(np.int16))

********************************************************************************

### File: src/config.py ###

# File: src/config.py

import os
import json
import logging

class Config:
    # Get absolute paths
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
    STATIC_DIR = os.path.join(CURRENT_DIR, 'static')
    TEMP_DIR = os.path.join(STATIC_DIR, 'temp')
    GOOD_SOUNDS_DIR = os.path.join(STATIC_DIR, 'goodsounds')
    CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')

    @classmethod
    def init_directories(cls):
        """Create all necessary directories"""
        for directory in [cls.STATIC_DIR, cls.TEMP_DIR, cls.GOOD_SOUNDS_DIR, cls.CONFIG_DIR]:
            os.makedirs(directory, mode=0o755, exist_ok=True)
            logging.debug(f"Created directory: {directory}")

    @staticmethod
    def get_dictionary():
        try:
            config_file = os.path.join(Config.CONFIG_DIR, 'active_dictionary.json')
            with open(config_file, 'r') as f:
                return json.load(f)
        except:
            return {
                "name": "Default",
                "sounds": ["ah", "eh", "ee", "oh", "oo"]
            }

    @classmethod
    def get_dictionaries(cls):
        try:
            with open(os.path.join(cls.CONFIG_DIR, 'dictionaries.json'), 'r') as f:
                return json.load(f)['dictionaries']
        except:
            return [{"name": "Default", "sounds": ["ah", "eh", "ee", "oh", "oo"]}]

    @classmethod
    def save_dictionaries(cls, dictionaries):
        with open(os.path.join(cls.CONFIG_DIR, 'dictionaries.json'), 'w') as f:
            json.dump({"dictionaries": dictionaries}, f, indent=4)

********************************************************************************

### File: src/main_app.py ###

from flask import Flask

app = Flask(__name__)

@app.route('/')
def home():
    return "Main App (Debug Mode)."

if __name__ == "__main__":
    # Run this app on port 5001 with debug mode on
    app.run(port=5001, debug=True, use_reloader=True)

********************************************************************************

### File: src/ml/audio_processing.py ###

# SoundClassifier_v08/src/ml/audio_processing.py

import numpy as np
import librosa
from scipy.signal import find_peaks
from .constants import SAMPLE_RATE

class SoundProcessor:
    """
    Unified SoundProcessor for audio preprocessing in both training and inference.
    This ensures consistency: the same trimming, centering, RMS normalization,
    time-stretch, and mel-spectrogram creation are used.
    """
    def __init__(self, sample_rate=SAMPLE_RATE):
        self.sample_rate = sample_rate
        self.sound_threshold = 0.1  # Threshold for sound detection

    def detect_sound(self, audio):
        """Detect if audio contains significant sound."""
        frame_rms = np.sqrt(np.mean(audio**2))
        peaks, _ = find_peaks(np.abs(audio), height=self.sound_threshold)
        has_sound = frame_rms > self.sound_threshold or len(peaks) > 0
        sound_location = None
        if len(peaks) > 0:
            sound_location = peaks[np.argmax(np.abs(audio)[peaks])]
        return has_sound, sound_location

    def detect_sound_boundaries(self, audio):
        frame_length = int(0.02 * self.sample_rate)  # 20ms windows
        hop_length = frame_length // 2
        rms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]
        
        # Interpolate RMS
        rms_interp = np.interp(
            np.linspace(0, len(audio), len(audio)),
            np.linspace(0, len(audio), len(rms)),
            rms
        )
        is_sound = rms_interp > (self.sound_threshold * np.max(rms_interp))
        if not np.any(is_sound):
            return 0, len(audio), False
        
        sound_indices = np.where(is_sound)[0]
        start_idx = sound_indices[0]
        end_idx = sound_indices[-1]
        
        margin = int(0.1 * self.sample_rate)
        start_idx = max(0, start_idx - margin)
        end_idx = min(len(audio), end_idx + margin)
        return start_idx, end_idx, True

    def center_audio(self, audio):
        start_idx, end_idx, has_sound = self.detect_sound_boundaries(audio)
        if not has_sound:
            center = len(audio) // 2
            window_size = self.sample_rate
            start_idx = max(0, center - window_size // 2)
            end_idx = min(len(audio), center + window_size // 2)
        audio = audio[start_idx:end_idx]

        target_rms = 0.1
        current_rms = np.sqrt(np.mean(audio**2))
        if current_rms > 0:
            audio = audio * (target_rms / current_rms)

        # Time-stretch to exactly 1 second
        target_length = self.sample_rate
        if len(audio) > 0:
            stretch_factor = target_length / len(audio)
            audio = librosa.effects.time_stretch(y=audio, rate=stretch_factor)
        
        if len(audio) > self.sample_rate:
            audio = audio[:self.sample_rate]
        elif len(audio) < self.sample_rate:
            audio = np.pad(audio, (0, self.sample_rate - len(audio)), 'constant')
        return audio

    def extract_features(self, audio):
        # Compute mel-spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=audio, sr=self.sample_rate,
            n_mels=64, n_fft=1024,
            hop_length=256
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        mel_spec_db = mel_spec_db[1:, :]  # remove the first mel band
        target_width = 64
        if mel_spec_db.shape[1] != target_width:
            mel_spec_db = np.array([np.interp(
                np.linspace(0, 100, target_width),
                np.linspace(0, 100, mel_spec_db.shape[1]),
                row
            ) for row in mel_spec_db])
        features = mel_spec_db[..., np.newaxis]
        return features

    def process_audio(self, audio):
        preprocessed_audio = self.center_audio(audio)
        features = self.extract_features(preprocessed_audio)
        return features

********************************************************************************

### File: src/ml/augmentation_manager.py ###

import numpy as np
import librosa
import logging
from .data_augmentation import (
    time_shift,
    change_pitch,
    change_speed,
    add_colored_noise,
    dynamic_range_compression,
    add_reverb,
    equalize
)
from .constants import (
    AUG_DO_TIME_SHIFT, AUG_TIME_SHIFT_COUNT, AUG_SHIFT_MAX,
    AUG_DO_PITCH_SHIFT, AUG_PITCH_SHIFT_COUNT, AUG_PITCH_RANGE,
    AUG_DO_SPEED_CHANGE, AUG_SPEED_CHANGE_COUNT, AUG_SPEED_RANGE,
    AUG_DO_NOISE, AUG_NOISE_COUNT, AUG_NOISE_TYPE, AUG_NOISE_FACTOR,
    AUG_DO_COMPRESSION, AUG_COMPRESSION_COUNT,
    AUG_DO_REVERB, AUG_REVERB_COUNT,
    AUG_DO_EQUALIZE, AUG_EQUALIZE_COUNT
)

def augment_audio(
    audio, 
    sr, 
    do_time_shift=True, 
    do_pitch_shift=True, 
    do_speed_change=True,
    do_noise=True,
    noise_type='white',
    noise_factor=0.005,
    pitch_range=2.0
):
    """
    Takes an original audio array and applies one or more data augmentations,
    returning a list of new augmented audio arrays.

    Args:
        audio (np.array): Original audio samples.
        sr (int): Sample rate.
        do_time_shift (bool): If True, apply time shift.
        do_pitch_shift (bool): If True, apply pitch shifting.
        do_speed_change (bool): If True, apply time stretching/compressing.
        do_noise (bool): If True, add colored noise.
        noise_type (str): Type of noise if do_noise=True.
        noise_factor (float): Noise amplitude scaling factor.
        pitch_range (float): Range (±) in semitones for pitch shift.

    Returns:
        List[np.array]: a list of augmented audio arrays.
    """
    augmented_audios = []
    
    original_count = 0  # track how many augmentations succeed

    # 1) Time Shift
    if do_time_shift:
        shifted = time_shift(audio)
        augmented_audios.append(shifted)
        original_count += 1

    # 2) Pitch Shift
    if do_pitch_shift:
        pitched = change_pitch(audio, sr=sr, pitch_range=pitch_range)
        augmented_audios.append(pitched)
        original_count += 1

    # 3) Speed Change
    if do_speed_change:
        sped = change_speed(audio)
        augmented_audios.append(sped)
        original_count += 1

    # 4) Noise Injection
    if do_noise:
        noisy = add_colored_noise(audio, noise_type=noise_type, noise_factor=noise_factor)
        augmented_audios.append(noisy)
        original_count += 1

    logging.info(f"Created {original_count} augmentations for one audio clip.")
    return augmented_audios

def augment_audio_with_repetitions(
    audio, sr,
    # Use constants.py as defaults, so you can override if desired
    do_time_shift=AUG_DO_TIME_SHIFT, 
    time_shift_count=AUG_TIME_SHIFT_COUNT, 
    shift_max=AUG_SHIFT_MAX,

    do_pitch_shift=AUG_DO_PITCH_SHIFT,
    pitch_shift_count=AUG_PITCH_SHIFT_COUNT,
    pitch_range=AUG_PITCH_RANGE,

    do_speed_change=AUG_DO_SPEED_CHANGE,
    speed_change_count=AUG_SPEED_CHANGE_COUNT,
    speed_range=AUG_SPEED_RANGE,

    do_noise=AUG_DO_NOISE,
    noise_count=AUG_NOISE_COUNT,
    noise_type=AUG_NOISE_TYPE,
    noise_factor=AUG_NOISE_FACTOR,

    # NEW: Additional augmentation toggles
    do_compression=AUG_DO_COMPRESSION,
    compression_count=AUG_COMPRESSION_COUNT,

    do_reverb=AUG_DO_REVERB,
    reverb_count=AUG_REVERB_COUNT,

    do_equalize=AUG_DO_EQUALIZE,
    equalize_count=AUG_EQUALIZE_COUNT
):
    """
    Loops multiple times per augmentation type, using default values from constants.py.
    Each call is random, so you get different results for each loop iteration.
    Now includes optional dynamic range compression, reverb, and equalization.
    """
    augmented_audios = []

    # Multiple random time shifts
    if do_time_shift:
        for _ in range(time_shift_count):
            shifted = time_shift(audio, shift_max=shift_max)
            augmented_audios.append(shifted)

    # Multiple random pitch shifts
    if do_pitch_shift:
        for _ in range(pitch_shift_count):
            pitched = change_pitch(audio, sr=sr, pitch_range=pitch_range)
            augmented_audios.append(pitched)

    # Multiple random speed changes
    if do_speed_change:
        for _ in range(speed_change_count):
            sped = change_speed(audio, speed_range=speed_range)
            augmented_audios.append(sped)

    # Multiple random noise injections
    if do_noise:
        for _ in range(noise_count):
            noisy = add_colored_noise(audio, noise_type=noise_type, noise_factor=noise_factor)
            augmented_audios.append(noisy)

    # NEW: Multiple dynamic range compressions
    if do_compression:
        for _ in range(compression_count):
            compressed = dynamic_range_compression(audio)
            augmented_audios.append(compressed)

    # NEW: Multiple reverb passes
    if do_reverb:
        for _ in range(reverb_count):
            reverbed = add_reverb(audio)
            augmented_audios.append(reverbed)

    # NEW: Multiple equalizations
    if do_equalize:
        for _ in range(equalize_count):
            eq_clip = equalize(audio)
            augmented_audios.append(eq_clip)

    logging.info(f"Created {len(augmented_audios)} augmented clips for one original audio.")
    return augmented_audios

********************************************************************************

### File: src/ml/cnn_classifier.py ###

# SoundClassifier_v08/src/ml/cnn_classifier.py

import os
import numpy as np
import random
import librosa
import logging
import json
from io import StringIO

import tensorflow as tf
from tensorflow.keras import layers, models

from config import Config
from .audio_processing import SoundProcessor
from .constants import (
    SAMPLE_RATE, AUDIO_DURATION, AUDIO_LENGTH, BATCH_SIZE, EPOCHS,
    AUG_DO_TIME_SHIFT, AUG_TIME_SHIFT_COUNT, AUG_SHIFT_MAX,
    AUG_DO_PITCH_SHIFT, AUG_PITCH_SHIFT_COUNT, AUG_PITCH_RANGE,
    AUG_DO_SPEED_CHANGE, AUG_SPEED_CHANGE_COUNT, AUG_SPEED_RANGE,
    AUG_DO_NOISE, AUG_NOISE_COUNT, AUG_NOISE_TYPE, AUG_NOISE_FACTOR,
    PITCH_SHIFTS_OUTER_VALUES, PITCH_SHIFTS_CENTER_START, PITCH_SHIFTS_CENTER_END, PITCH_SHIFTS_CENTER_NUM,
    NOISE_TYPES_LIST, NOISE_LEVELS_MIN, NOISE_LEVELS_MAX, NOISE_LEVELS_COUNT,
    AUG_DO_COMPRESSION, AUG_COMPRESSION_COUNT,
    AUG_DO_REVERB, AUG_REVERB_COUNT,
    AUG_DO_EQUALIZE, AUG_EQUALIZE_COUNT 
)

from .augmentation_manager import augment_audio_with_repetitions

"""
Merged CNN Classifier code with data augmentation, 
consistent references to Config and SoundProcessor.
"""

# -----------------------------
# Main function to build dataset
# -----------------------------
def build_dataset(sound_folder):
    """
    Build a dataset for CNN training by loading audio from
    `sound_folder` (the 'goodsounds' directory) and using the
    active dictionary from config. Applies data augmentation
    to each original file.
    
    Returns:
        X: np.array of shape (N, 63, 64, 1) [example shape]
        y: np.array of labels
        class_names: list of sound labels
        stats: dict with info on original/augmented counts
    """
    X = []
    y = []
    total_samples = 0

    # Initialize SoundProcessor
    sound_processor = SoundProcessor(sample_rate=SAMPLE_RATE)

    # Load the active dictionary from config
    config_file = os.path.join(Config.CONFIG_DIR, 'active_dictionary.json')
    logging.info(f"Looking for config file at: {config_file}")
    with open(config_file, 'r') as f:
        active_dict = json.load(f)
    class_names = active_dict['sounds']
    logging.info(f"Found class names: {class_names}")

    # Map each class_name to an integer
    class_indices = {name: i for i, name in enumerate(class_names)}
    logging.info(f"Class indices mapping: {class_indices}")

    # Example pitch shifting steps and noise arrays from constants.py
    pitch_shifts_center = np.linspace(
        PITCH_SHIFTS_CENTER_START,
        PITCH_SHIFTS_CENTER_END,
        PITCH_SHIFTS_CENTER_NUM
    )
    pitch_shifts_outer = np.array(PITCH_SHIFTS_OUTER_VALUES)
    pitch_shifts = np.concatenate([pitch_shifts_outer, pitch_shifts_center])

    noise_types = NOISE_TYPES_LIST
    noise_levels = np.linspace(NOISE_LEVELS_MIN, NOISE_LEVELS_MAX, NOISE_LEVELS_COUNT)

    # For each sound in the active dictionary
    for class_name in class_names:
        class_path = os.path.join(sound_folder, class_name)
        logging.info(f"Processing class directory: {class_path}")
        if not os.path.exists(class_path):
            logging.warning(f"Directory {class_path} does not exist.")
            continue

        # Gather .wav files
        files = [f for f in os.listdir(class_path) if f.endswith('.wav')]
        logging.info(f"Found {len(files)} files for class {class_name}")

        for file_name in files:
            file_path = os.path.join(class_path, file_name)
            try:
                logging.info(f"Loading file: {file_path}")
                audio, _ = librosa.load(file_path, sr=SAMPLE_RATE)

                # Original
                features = sound_processor.process_audio(audio)
                if features is None:
                    continue
                X.append(features)
                y.append(class_indices[class_name])
                total_samples += 1

                # Multi-run augmentation with explicit parameter names:
                augmented_clips = augment_audio_with_repetitions(
                    audio=audio,
                    sr=SAMPLE_RATE,
                    do_time_shift=AUG_DO_TIME_SHIFT,
                    time_shift_count=AUG_TIME_SHIFT_COUNT,
                    shift_max=AUG_SHIFT_MAX,

                    do_pitch_shift=AUG_DO_PITCH_SHIFT,
                    pitch_shift_count=AUG_PITCH_SHIFT_COUNT,
                    pitch_range=AUG_PITCH_RANGE,

                    do_speed_change=AUG_DO_SPEED_CHANGE,
                    speed_change_count=AUG_SPEED_CHANGE_COUNT,
                    speed_range=AUG_SPEED_RANGE,

                    do_noise=AUG_DO_NOISE,
                    noise_count=AUG_NOISE_COUNT,
                    noise_type=AUG_NOISE_TYPE,
                    noise_factor=AUG_NOISE_FACTOR,

                    do_compression=AUG_DO_COMPRESSION,
                    compression_count=AUG_COMPRESSION_COUNT,

                    do_reverb=AUG_DO_REVERB,
                    reverb_count=AUG_REVERB_COUNT,

                    do_equalize=AUG_DO_EQUALIZE,
                    equalize_count=AUG_EQUALIZE_COUNT
                )
                for aug_audio in augmented_clips:
                    features_aug = sound_processor.process_audio(aug_audio)
                    if features_aug is not None:
                        X.append(features_aug)
                        y.append(class_indices[class_name])
                        total_samples += 1

            except Exception as e:
                logging.error(f"Error processing {file_path}: {e}")
                continue

    # Final arrays
    if not X:
        logging.error("No valid samples were processed.")
        return None, None, None, None

    X = np.array(X)
    y = np.array(y)
    logging.info(f"Total samples after augmentation: {total_samples}")
    logging.info(f"Final dataset shapes: X={X.shape}, y={y.shape}")

    # Minimal stats
    stats = {
        'original_counts': {},
        'augmented_counts': {}
    }
    # You can fill these if needed for advanced logging

    return X, y, class_names, stats

# -----------------------------
# Build CNN model
# -----------------------------
def build_model(input_shape, num_classes):
    """
    Build a CNN model optimized for short audio classification.
    Returns (model, model_summary_string).
    """
    inputs = layers.Input(shape=input_shape)

    # First Conv Block
    x = layers.Conv2D(16, (3, 3), padding='same',
                      kernel_regularizer=tf.keras.regularizers.l2(1e-5))(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.2)(x)

    # Second Conv Block
    x = layers.Conv2D(32, (3, 3), padding='same',
                      kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.2)(x)

    # Dense layers
    x = layers.Flatten()(x)
    x = layers.Dense(256, activation='relu',
                     kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs=inputs, outputs=outputs)

    # Adam with a lower LR
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)

    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Capture the summary
    summary_io = StringIO()
    model.summary(print_fn=lambda s: summary_io.write(s + '\n'))
    model_summary = summary_io.getvalue()

    return model, model_summary

# -----------------------------
# Optional: training test
# -----------------------------
if __name__ == "__main__":
    """
    If you want to do a quick test run from the command line:
    python cnn_classifier.py
    (You might have to adjust data_path or feed real audio.)
    """
    data_path = "data"  # adjust to your actual data folder

    # Build dataset
    X, y, class_names, stats = build_dataset(data_path)
    if X is None or y is None:
        print("No data found. Exiting.")
        exit(0)

    # Shuffle
    indices = np.arange(len(X))
    np.random.shuffle(indices)
    X = X[indices]
    y = y[indices]

    # Split
    val_split = 0.2
    split_idx = int(len(X) * (1 - val_split))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    # Build the model
    input_shape = X_train.shape[1:]  # e.g. (63, 64, 1)
    model, model_summary = build_model(input_shape, num_classes=len(class_names))
    print(model_summary)

    # Example training
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    )

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        callbacks=[early_stopping]
    )

    # Save the model
    model.save("audio_classifier.h5")

    # Evaluate
    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
    print(f"Validation accuracy: {val_acc*100:.2f}%")

********************************************************************************

### File: src/ml/constants.py ###

# SoundClassifier_v08/src/ml/constants.py

# -----------------------------
# Global settings
# -----------------------------
SAMPLE_RATE = 16000
N_MFCC = 13
AUDIO_DURATION = 1.0  # Use fixed duration of 1 second
AUDIO_LENGTH = int(SAMPLE_RATE * AUDIO_DURATION)
BATCH_SIZE = 32  # Increased batch size
EPOCHS = 50      # Increased epochs 

# -----------------------------
# Test settings
# -----------------------------
TEST_DURATION = 5  # seconds
TEST_FS = 44100  # Sample rate

# -----------------------------
# AUDIO SCALING CONSTANTS
# -----------------------------
INT16_MAX = 32767  # Maximum value for int16

# -----------------------------
# AUGMENTATION DEFAULTS
# -----------------------------
# You can modify these defaults in one place for all models:
AUG_DO_TIME_SHIFT = True
AUG_TIME_SHIFT_COUNT = 3
AUG_SHIFT_MAX = 0.2

AUG_DO_PITCH_SHIFT = True
AUG_PITCH_SHIFT_COUNT = 3
AUG_PITCH_RANGE = 2.0

AUG_DO_SPEED_CHANGE = True
AUG_SPEED_CHANGE_COUNT = 3
AUG_SPEED_RANGE = 0.1

AUG_DO_NOISE = True
AUG_NOISE_COUNT = 3
AUG_NOISE_TYPE = "white"
AUG_NOISE_FACTOR = 0.005

# -----------------------------
# ADDITIONAL AUGMENTATION CONSTANTS
# -----------------------------

# For advanced pitch shifting arrays:
PITCH_SHIFTS_OUTER_VALUES = [-3.0, -2.0, 2.0, 3.0]
PITCH_SHIFTS_CENTER_START = -1.0
PITCH_SHIFTS_CENTER_END = 1.0
PITCH_SHIFTS_CENTER_NUM = 9

# For advanced noise experimentation:
NOISE_TYPES_LIST = ['white', 'pink', 'brown']
NOISE_LEVELS_MIN = 0.001
NOISE_LEVELS_MAX = 0.01
NOISE_LEVELS_COUNT = 5

# ---------------------------------
# NEW: Additional effect toggles

# ---------------------------------
AUG_DO_COMPRESSION = False
AUG_COMPRESSION_COUNT = 1

AUG_DO_REVERB = True
AUG_REVERB_COUNT = 1

AUG_DO_EQUALIZE = True
AUG_EQUALIZE_COUNT = 1
********************************************************************************

### File: src/ml/data_augmentation.py ###

# SoundClassifier_v08/src/ml/data_augmentation.py

import numpy as np
import librosa
import logging
from .constants import SAMPLE_RATE

def time_shift(audio, shift_max=0.2):
    """Shift the audio by a random fraction of total length."""
    shift = np.random.randint(
        int(SAMPLE_RATE * -shift_max), 
        int(SAMPLE_RATE * shift_max)
    )
    return np.roll(audio, shift)

def change_pitch(audio, sr=SAMPLE_RATE, pitch_range=2.0):
    """Randomly shift pitch by ±pitch_range semitones."""
    n_steps = np.random.uniform(-pitch_range, pitch_range)
    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)

def change_speed(audio, speed_range=0.1):
    """Randomly time-stretch or compress by ±speed_range around 1.0."""
    speed_factor = np.random.uniform(1 - speed_range, 1 + speed_range)
    return librosa.effects.time_stretch(audio, rate=speed_factor)

def add_colored_noise(audio, noise_type='white', noise_factor=0.005):
    """
    Add colored noise to an audio signal.
    Args:
        noise_type: 'white', 'pink', or 'brown'
        noise_factor: scaling factor
    """
    if noise_type == 'white':
        noise = np.random.randn(len(audio))
    elif noise_type == 'pink':
        f = np.fft.fftfreq(len(audio))
        f = np.abs(f)
        f[0] = 1e-6  # Avoid division by zero
        pink = np.random.randn(len(audio)) / np.sqrt(f)
        noise = np.fft.ifft(pink).real
    elif noise_type == 'brown':
        noise = np.cumsum(np.random.randn(len(audio)))
        noise = noise / np.max(np.abs(noise))

    return audio + noise_factor * noise

def dynamic_range_compression(audio):
    """
    Use librosa.effects.percussive to isolate percussive components,
    which can act like a simple dynamic range compression approach.
    """
    return librosa.effects.percussive(audio)

def add_reverb(audio):
    """
    A simple 'reverb-like' effect using librosa's preemphasis as a stand-in.
    For a real reverb, you'd convolve with an IR, but this is a placeholder.
    """
    return librosa.effects.preemphasis(audio)

def equalize(audio):
    """
    Random amplitude scaling (0.8x to 1.2x) to mimic a simple EQ or loudness shift.
    """
    scale = np.random.uniform(0.8, 1.2)
    return audio * scale

********************************************************************************

### File: src/ml/ensemble_classifier.py ###

# File: src/ml/ensemble_classifier.py

import numpy as np

class EnsembleClassifier:
    """
    A simple ensemble that merges predictions from a CNN model 
    and from a RandomForest model, taking either an average 
    or a weighted average of probabilities.
    """
    def __init__(self, rf_classifier, cnn_model, class_names, rf_weight=0.5):
        """
        Args:
            rf_classifier: An instance of RandomForestClassifier
            cnn_model: A loaded Keras model
            class_names: The list of classes in both approaches
            rf_weight: The weight to assign to RF predictions, 
                       the CNN gets (1 - rf_weight)
        """
        self.rf_classifier = rf_classifier
        self.cnn_model = cnn_model
        self.class_names = class_names
        self.rf_weight = rf_weight

    def predict(self, X_rf, X_cnn):
        """
        X_rf: feature vectors for RF 
        X_cnn: CNN input (mel-spectrograms)
        
        Returns final predicted class and combined confidence
        """
        # 1) Get RF probabilities
        _, rf_probs = self.rf_classifier.predict(X_rf)

        # 2) Get CNN probabilities
        cnn_probs = self.cnn_model.predict(X_cnn)

        # 3) Weighted average
        combined_probs = self.rf_weight * rf_probs + (1 - self.rf_weight) * cnn_probs

        # 4) Final prediction
        preds = []
        for row in combined_probs:
            idx = np.argmax(row)
            confidence = row[idx]
            preds.append((self.class_names[idx], confidence))
        return preds

    def get_top_predictions(self, X_rf, X_cnn, top_n=3):
        _, rf_probs = self.rf_classifier.predict(X_rf)
        cnn_probs = self.cnn_model.predict(X_cnn)
        combined_probs = self.rf_weight * rf_probs + (1 - self.rf_weight) * cnn_probs

        results = []
        for row in combined_probs:
            top_indices = np.argsort(row)[-top_n:][::-1]
            pred_list = []
            for i in top_indices:
                pred_list.append({
                    'sound': self.class_names[i],
                    'probability': float(row[i])
                })
            results.append(pred_list)
        return results

********************************************************************************

### File: src/ml/feature_extractor.py ###

# SoundClassifier_v08/src/ml/feature_extractor.py

import librosa
import numpy as np
import logging

class AudioFeatureExtractor:
    """
    A consolidated feature extractor that merges old_code/feature_extractor.py 
    style MFCC + spectral features with optional formants/pitch. 
    This can be used by the RandomForest approach or for 
    a 'classical' feature-based method.
    """
    def __init__(self, sr=22050, duration=None):
        """
        Args:
            sr (int): Sample rate for audio processing
            duration (float): Duration to load from audio file (None for full file)
        """
        self.sr = sr
        self.duration = duration
        self.n_mfcc = 13
        self.hop_length = 512

    def extract_features(self, audio_path):
        """
        Extract features from an audio file on disk.
        
        Args:
            audio_path (str): Path to the .wav (or other) audio file.
        Returns:
            dict or None: A dictionary of extracted features, or None if there's an error.
        """
        try:
            # Load audio from file
            y, sr = librosa.load(audio_path, sr=self.sr, duration=self.duration)

            # Time-stretch short audio to ensure at least 9 frames
            required_len = self.hop_length * 9
            if len(y) < required_len:
                stretch_factor = 1.05 * required_len / len(y)
                logging.warning(
                    f"Audio too short ({len(y)} < {required_len}). "
                    f"Time-stretching by factor={stretch_factor:.2f}"
                )
                y = librosa.effects.time_stretch(y, rate=1 / stretch_factor)
                logging.warning(f"Audio now {len(y)} samples!")

            features = {}

            # --------------------
            # MFCC + deltas 
            # --------------------
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, hop_length=self.hop_length)
            mfcc_delta = librosa.feature.delta(mfccs)
            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)

            for i in range(self.n_mfcc):
                features[f'mfcc_{i}_mean'] = float(np.mean(mfccs[i]))
                features[f'mfcc_{i}_std'] = float(np.std(mfccs[i]))
                features[f'mfcc_delta_{i}_mean'] = float(np.mean(mfcc_delta[i]))
                features[f'mfcc_delta_{i}_std'] = float(np.std(mfcc_delta[i]))
                features[f'mfcc_delta2_{i}_mean'] = float(np.mean(mfcc_delta2[i]))
                features[f'mfcc_delta2_{i}_std'] = float(np.std(mfcc_delta2[i]))

            # --------------------
            # Formant approximation
            # --------------------
            formants = librosa.effects.preemphasis(y)
            features['formant_mean'] = float(np.mean(formants))
            features['formant_std'] = float(np.std(formants))

            # --------------------
            # Pitch
            # --------------------
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
            pitch_vals = pitches[magnitudes > np.median(magnitudes)]
            if len(pitch_vals) > 0:
                features['pitch_mean'] = float(np.mean(pitch_vals))
                features['pitch_std'] = float(np.std(pitch_vals))
            else:
                features['pitch_mean'] = 0.0
                features['pitch_std'] = 0.0

            # --------------------
            # Spectral Centroid
            # --------------------
            cent = librosa.feature.spectral_centroid(y=y, sr=sr)
            features['spectral_centroid_mean'] = float(np.mean(cent))
            features['spectral_centroid_std'] = float(np.std(cent))

            # --------------------
            # Zero Crossing Rate
            # --------------------
            zcr = librosa.feature.zero_crossing_rate(y)
            features['zcr_mean'] = float(np.mean(zcr))
            features['zcr_std'] = float(np.std(zcr))

            # --------------------
            # RMS
            # --------------------
            rms = librosa.feature.rms(y=y)
            features['rms_mean'] = float(np.mean(rms))
            features['rms_std'] = float(np.std(rms))

            # --------------------
            # Spectral Rolloff
            # --------------------
            rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            features['rolloff_mean'] = float(np.mean(rolloff))
            features['rolloff_std'] = float(np.std(rolloff))

            return features
        except Exception as e:
            logging.error(f"Error extracting features from {audio_path}: {str(e)}")
            return None

    def extract_features_from_array(self, y, sr=None):
        """
        Extract features from a raw numpy array (y) already in memory.
        
        Args:
            y (np.ndarray): Raw audio array.
            sr (int, optional): If provided, overrides self.sr for this extraction.
        Returns:
            dict or None: A dictionary of extracted features, or None if there's an error.
        """
        try:
            # Use the passed sr if present, otherwise self.sr
            if sr is None:
                sr = self.sr

            # Time-stretch short audio to ensure at least 9 frames
            required_len = self.hop_length * 9
            if len(y) < required_len:
                stretch_factor = 1.05 * required_len / len(y)
                logging.warning(
                    f"Audio too short ({len(y)} < {required_len}). "
                    f"Time-stretching by factor={stretch_factor:.2f}"
                )
                y = librosa.effects.time_stretch(y, rate=1 / stretch_factor)

            features = {}

            # --------------------
            # MFCC + deltas 
            # --------------------
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, hop_length=self.hop_length)
            mfcc_delta = librosa.feature.delta(mfccs)
            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)

            for i in range(self.n_mfcc):
                features[f'mfcc_{i}_mean'] = float(np.mean(mfccs[i]))
                features[f'mfcc_{i}_std'] = float(np.std(mfccs[i]))
                features[f'mfcc_delta_{i}_mean'] = float(np.mean(mfcc_delta[i]))
                features[f'mfcc_delta_{i}_std'] = float(np.std(mfcc_delta[i]))
                features[f'mfcc_delta2_{i}_mean'] = float(np.mean(mfcc_delta2[i]))
                features[f'mfcc_delta2_{i}_std'] = float(np.std(mfcc_delta2[i]))

            # --------------------
            # Formant approximation
            # --------------------
            formants = librosa.effects.preemphasis(y)
            features['formant_mean'] = float(np.mean(formants))
            features['formant_std'] = float(np.std(formants))

            # --------------------
            # Pitch
            # --------------------
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
            pitch_vals = pitches[magnitudes > np.median(magnitudes)]
            if len(pitch_vals) > 0:
                features['pitch_mean'] = float(np.mean(pitch_vals))
                features['pitch_std'] = float(np.std(pitch_vals))
            else:
                features['pitch_mean'] = 0.0
                features['pitch_std'] = 0.0

            # --------------------
            # Spectral Centroid
            # --------------------
            cent = librosa.feature.spectral_centroid(y=y, sr=sr)
            features['spectral_centroid_mean'] = float(np.mean(cent))
            features['spectral_centroid_std'] = float(np.std(cent))

            # --------------------
            # Zero Crossing Rate
            # --------------------
            zcr = librosa.feature.zero_crossing_rate(y)
            features['zcr_mean'] = float(np.mean(zcr))
            features['zcr_std'] = float(np.std(zcr))

            # --------------------
            # RMS
            # --------------------
            rms = librosa.feature.rms(y=y)
            features['rms_mean'] = float(np.mean(rms))
            features['rms_std'] = float(np.std(rms))

            # --------------------
            # Spectral Rolloff
            # --------------------
            rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            features['rolloff_mean'] = float(np.mean(rolloff))
            features['rolloff_std'] = float(np.std(rolloff))

            return features
        except Exception as e:
            logging.error(f"Error extracting features from raw audio array: {e}")
            return None

    def get_feature_names(self):
        """
        Return a list of feature names in the order they appear
        in the extracted feature dictionaries.
        """
        feature_names = []
        for i in range(self.n_mfcc):
            feature_names.extend([
                f'mfcc_{i}_mean', f'mfcc_{i}_std',
                f'mfcc_delta_{i}_mean', f'mfcc_delta_{i}_std',
                f'mfcc_delta2_{i}_mean', f'mfcc_delta2_{i}_std'
            ])
        feature_names.extend([
            'formant_mean', 'formant_std',
            'pitch_mean', 'pitch_std',
            'spectral_centroid_mean', 'spectral_centroid_std',
            'zcr_mean', 'zcr_std',
            'rms_mean', 'rms_std',
            'rolloff_mean', 'rolloff_std'
        ])
        return feature_names

    def save_wav(self, audio_data, sr, filename):
        """
        Save audio_data (numpy array) to a .wav file at the specified sample rate.
        """
        import soundfile as sf
        try:
            sf.write(filename, audio_data, sr)
        except Exception as e:
            logging.error(f"Error saving wav file {filename}: {e}")

********************************************************************************

### File: src/ml/inference.py ###

# SoundClassifier_v08/src/ml/inference.py

import numpy as np
import librosa
import sounddevice as sd
import tensorflow as tf
import threading
import time
import logging
from scipy.io import wavfile
from flask import current_app
import os
from src.ml.model_paths import get_cnn_model_path

# Import shared constants and the SoundProcessor from audio_processing
from .constants import SAMPLE_RATE, AUDIO_DURATION, AUDIO_LENGTH, TEST_DURATION, TEST_FS, INT16_MAX
from .audio_processing import SoundProcessor

# Set up logging
logging.basicConfig(level=logging.INFO)

# Print available audio devices for reference
print(sd.query_devices())

class SoundDetector:
    """
    Continuously listens to the microphone for short bursts of sound.
    Accumulates audio in a queue; once enough audio is detected,
    runs inference with the loaded CNN model.
    """
    def __init__(self, model, class_names):
        self.model = model
        self.class_names = class_names
        self.audio_queue = []
        self.is_recording = False
        self.predictions = []
        self.callback = None
        self.stream = None
        self.thread = None
        self.buffer = bytearray()
        self.sample_width = 2  # 16-bit audio
        self.frame_duration_ms = 30   # Reduced for finer granularity
        self.frame_size = int(SAMPLE_RATE * self.frame_duration_ms / 1000)
        self.frame_duration = self.frame_duration_ms / 1000.0
        self.speech_buffer = bytearray()
        self.speech_detected = False
        self.silence_duration = 0
        self.silence_threshold_ms = 500  # For better word detection
        self.auto_stop = False
        self.is_speech_active = False
        self.speech_duration = 0
        self.min_sound_duration = 0.3  # Minimum duration for a complete sound

        # SoundProcessor with adjusted threshold
        self.sound_processor = SoundProcessor(sample_rate=SAMPLE_RATE)
        self.sound_processor.sound_threshold = 0.08  # Slightly lower threshold

        self.audio_queue_lock = threading.Lock()

        logging.info(f"SoundDetector initialized with classes: {class_names}")
        devices = sd.query_devices()
        logging.info("Available audio devices:")
        for i, device in enumerate(devices):
            logging.info(f"[{i}] {device['name']} (inputs: {device['max_input_channels']})")

        # Confidence threshold for final predictions
        self.confidence_threshold = 0.40
        self.amplitude_threshold = 0.08
        self.min_prediction_threshold = 0.3

        # Circular buffer for a small "pre-buffer"
        self.pre_buffer_duration_ms = 100
        self.pre_buffer_size = int(SAMPLE_RATE * self.pre_buffer_duration_ms / 1000)
        self.circular_buffer = np.zeros(self.pre_buffer_size, dtype=np.float32)
        self.buffer_index = 0

        self._stop_event = threading.Event()

    def process_audio(self):
        """
        Called typically after enough audio is accumulated in the queue.
        Concatenates all frames, extracts features, predicts with the model.
        """
        try:
            with self.audio_queue_lock:
                if not self.audio_queue:
                    logging.info("No audio data in queue to process")
                    return
                logging.info(f"Processing audio queue of size: {len(self.audio_queue)}")
                audio_data = np.concatenate(self.audio_queue)
                self.audio_queue.clear()

            logging.info(f"Concatenated audio shape: {audio_data.shape}")

            # Normalize to -1..1 if needed
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            # Extract features
            features = self.sound_processor.process_audio(audio_data)
            if features is None:
                logging.info("No features extracted (features=None).")
                return

            logging.info(f"Extracted features shape: {features.shape}")

            # Model prediction
            features = np.expand_dims(features, axis=0)
            predictions = self.model.predict(features, verbose=0)
            logging.info(f"Raw predictions: {predictions[0]}")

            top_idx = np.argmax(predictions[0])
            top_label = self.class_names[top_idx]
            top_conf = float(predictions[0][top_idx])

            if top_conf > self.confidence_threshold:
                logging.info(f"Prediction above threshold: {top_label} (conf: {top_conf:.4f})")
                self.predictions.append((top_label, top_conf))

                if self.callback:
                    self.callback({
                        "class": top_label,
                        "confidence": top_conf
                    })

            else:
                logging.info(f"Confidence {top_conf:.4f} < threshold {self.confidence_threshold}")
                # ============ ADDED BELOW ============ 
                # Even if confidence is low, let the callback happen,
                # so the user can see/confirm/correct the guess.
                if self.callback:
                    self.callback({
                        "class": top_label,
                        "confidence": top_conf,
                        "low_confidence": True
                    })
                # ============ END NEW CODE ============

        except Exception as e:
            logging.error(f"Error in process_audio: {e}", exc_info=True)


    def audio_callback(self, indata, frames, time_info, status):
        """
        Sounddevice callback. Called with new chunks of mic data.
        """
        try:
            if status:
                logging.warning(f"Audio callback status: {status}")

            audio_data = indata.flatten().astype(np.float32)
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            # Sound detection
            has_sound, _ = self.sound_processor.detect_sound(audio_data)

            # Update circular buffer
            start_idx = self.buffer_index
            end_idx = start_idx + len(audio_data)
            if end_idx > self.pre_buffer_size:
                first_part = self.pre_buffer_size - start_idx
                self.circular_buffer[start_idx:] = audio_data[:first_part]
                self.circular_buffer[:end_idx - self.pre_buffer_size] = audio_data[first_part:]
            else:
                self.circular_buffer[start_idx:end_idx] = audio_data
            self.buffer_index = (self.buffer_index + len(audio_data)) % self.pre_buffer_size

            # If there's sound
            if has_sound:
                if not self.is_speech_active:
                    logging.info("Sound detected!")
                    self.is_speech_active = True
                    self.speech_duration = 0

                    # Include the pre-buffer data for context
                    with self.audio_queue_lock:
                        pre_buffer = np.concatenate([
                            self.circular_buffer[self.buffer_index:],
                            self.circular_buffer[:self.buffer_index]
                        ])
                        self.audio_queue.append(pre_buffer)

                with self.audio_queue_lock:
                    self.audio_queue.append(audio_data)

                self.speech_duration += len(audio_data) / SAMPLE_RATE

                # If we've collected enough audio, process
                if self.speech_duration >= AUDIO_DURATION:
                    self.process_audio()
                    self.is_speech_active = False

            else:
                # If we were capturing speech but now silent, finalize
                if self.is_speech_active:
                    with self.audio_queue_lock:
                        self.audio_queue.append(audio_data)
                    self.process_audio()
                    self.is_speech_active = False
                    logging.info("Sound ended")

        except Exception as e:
            logging.error(f"Error in audio_callback: {str(e)}", exc_info=True)

    def start_listening(self, callback=None, auto_stop=False):
        """
        Start capturing audio from microphone in real-time,
        applying audio_callback, and processing when enough data is found.
        """
        if self.is_recording:
            return False

        try:
            self.is_recording = True
            self.callback = callback
            self.auto_stop = auto_stop
            self.predictions = []

            # Clear flags
            self.buffer = bytearray()
            self.speech_buffer = bytearray()
            self.speech_detected = False
            self.silence_duration = 0
            self.speech_duration = 0

            logging.info("Starting audio stream with:")
            logging.info(f"Sample rate: {SAMPLE_RATE}")
            logging.info(f"Frame duration: {self.frame_duration_ms} ms")
            logging.info(f"Frame size: {self.frame_size} samples")

            # Open stream
            self.stream = sd.InputStream(
                channels=1,
                dtype=np.float32,
                samplerate=SAMPLE_RATE,
                blocksize=self.frame_size,
                callback=self.audio_callback
            )

            # Start a thread that can occasionally run process_audio
            # (Though we also trigger it in audio_callback.)
            self.thread = threading.Thread(target=self.process_audio, daemon=True)
            self.thread.start()

            self.stream.start()
            return True

        except Exception as e:
            logging.error(f"Error starting listener: {e}", exc_info=True)
            self.is_recording = False
            return False

    def stop_listening(self):
        """
        Stop the stream, clear buffers, finalize.
        """
        try:
            self.is_recording = False

            with self.audio_queue_lock:
                self.audio_queue.clear()

            if self.stream:
                self.stream.stop()
                self.stream.close()
                self.stream = None

            if self.thread:
                if threading.current_thread() != self.thread:
                    self.thread.join()
                self.thread = None

            logging.info("Stopped listening successfully.")
            self.speech_buffer.clear()
            return {"status": "success", "message": "Stopped listening successfully"}

        except Exception as e:
            msg = f"Error stopping listener: {e}"
            logging.error(msg, exc_info=True)
            return {"status": "error", "message": msg}


def record_audio(duration=AUDIO_DURATION):
    """
    Records from the microphone for `duration` seconds 
    and returns a 1D NumPy array of float32 samples.
    """
    logging.info(f"Recording for {duration:.2f} second(s)...")
    recording = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE,channels=1,blocking=True)
    sd.wait()
    return recording.flatten().astype(np.float32)


def predict_sound(model, input_source, class_names, use_microphone=False):
    """
    Predict sound from either a file path or microphone input (CNN approach).
    If `use_microphone` is True, it records from mic for AUDIO_DURATION.
    Otherwise, `input_source` is interpreted as a filepath (or raw data).
    """
    try:
        sp = SoundProcessor(sample_rate=SAMPLE_RATE)
        if use_microphone:
            audio = record_audio(AUDIO_DURATION)
        else:
            # Load from a file path, resampling to SAMPLE_RATE
            audio, _ = librosa.load(input_source, sr=SAMPLE_RATE)

        # Process
        features = sp.process_audio(audio)
        if features is None:
            return None, 0.0

        features = np.expand_dims(features, axis=0)
        predictions = model.predict(features, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        predicted_label = class_names[predicted_class]
        return predicted_label, confidence

    except Exception as e:
        logging.error(f"Error in predict_sound: {str(e)}", exc_info=True)
        return None, 0.0


def run_inference_loop(model, class_names):
    """
    Simple interactive loop for testing predictions on command line.
    Type 'mic' to record from microphone, 'file <path>' for a WAV file, or 'quit' to exit.
    """
    print("\nSound Prediction Mode")
    print("--------------------")
    print("Commands:")
    print("  'mic' - Record from microphone")
    print("  'file <path>' - Predict from audio file")
    print("  'quit' - Exit the program")

    while True:
        try:
            command = input("\nEnter command >>> ").strip().lower()
            if command == 'quit':
                print("Exiting...")
                break
            elif command == 'mic':
                label, conf = predict_sound(model, None, class_names, use_microphone=True)
                if label:
                    print(f"Predicted: '{label}' (confidence: {conf:.3f})")
            elif command.startswith('file '):
                file_path = command[5:].strip()
                label, conf = predict_sound(model, file_path, class_names, use_microphone=False)
                if label:
                    print(f"Predicted: '{label}' (confidence: {conf:.3f})")
            else:
                print("Unknown command. Use 'mic', 'file <path>', or 'quit'")
        except KeyboardInterrupt:
            print("\nStopping...")
            break
        except Exception as e:
            print(f"Error: {str(e)}")


def test_microphone():
    """
    Record for TEST_DURATION seconds at TEST_FS sample rate, then play it back and optionally save.
    """
    print(f"Recording for {TEST_DURATION} second(s) at {TEST_FS} Hz...")
    test_recording = sd.rec(int(TEST_DURATION * TEST_FS), samplerate=TEST_FS, channels=1)
    sd.wait()
    print("Recording complete. Playing back...")

    sd.play(test_recording, samplerate=TEST_FS)
    sd.wait()

    # Save the recording to a WAV file
    test_recording_int = np.int16(test_recording * INT16_MAX)
    wavfile.write("test_recording.wav", TEST_FS, test_recording_int)
    print("Playback complete.")


if __name__ == "__main__":
    # Old logic
    try:
        model = tf.keras.models.load_model("models/audio_classifier.h5")
        
        dictionary_name = "Two_words"  # or read from config
        version = "v1"
        new_cnn_path = get_cnn_model_path("models", dictionary_name, version)

        if os.path.exists(new_cnn_path):
            print(f"Loading new CNN path: {new_cnn_path}")
            model = tf.keras.models.load_model(new_cnn_path)
        else:
            print("New CNN path not found, falling back to 'audio_classifier.h5'")
            model = tf.keras.models.load_model("audio_classifier.h5")

        class_names = np.load("models/class_names.npy", allow_pickle=True)
        print(f"Loaded class names: {class_names}")

        run_inference_loop(model, class_names)
        test_microphone()

    except Exception as e:
        print(f"Error loading model or class names: {str(e)}")

********************************************************************************

### File: src/ml/model_paths.py ###

import os
import json
import datetime

def get_model_dir(base_dir, dictionary_name, model_type, version="v1"):
    """
    Build a folder path like:
      base_dir / dictionary_name / model_type / version
    Example:  models/Two_words/CNN/v1
    """
    safe_dict = dictionary_name.replace(' ', '_')
    path = os.path.join(base_dir, safe_dict, model_type, version)
    os.makedirs(path, exist_ok=True)
    return path

def get_cnn_model_path(base_dir, dictionary_name, version="v1"):
    """
    Returns the .h5 path for CNN model
    Example:  models/Two_words/CNN/v1/cnn_model.h5
    """
    folder = get_model_dir(base_dir, dictionary_name, "CNN", version)
    return os.path.join(folder, "cnn_model.h5")

def get_rf_model_path(base_dir, dictionary_name, version="v1"):
    """
    Returns the .joblib path for RF model
    Example:  models/Two_words/RF/v1/rf_model.joblib
    """
    folder = get_model_dir(base_dir, dictionary_name, "RF", version)
    return os.path.join(folder, "rf_model.joblib")

def get_ensemble_model_path(base_dir, dictionary_name, version="v1"):
    """
    Example for an ensemble approach, if you want a single file:
    models/Two_words/ENSEMBLE/v1/ensemble_model.json
    or some other approach. Right now, just return a JSON (or .h5).
    """
    folder = get_model_dir(base_dir, dictionary_name, "ENSEMBLE", version)
    return os.path.join(folder, "ensemble_info.json")

def save_model_metadata(folder_path, metadata):
    """
    Creates/updates a file named 'model_info.json' in `folder_path`
    containing training parameters, augmentation info, etc.
    """
    path = os.path.join(folder_path, "model_info.json")
    # Add a timestamp to the metadata
    if "timestamp" not in metadata:
        metadata["timestamp"] = str(datetime.datetime.now())
    with open(path, 'w') as f:
        json.dump(metadata, f, indent=2)
    return path

********************************************************************************

### File: src/ml/rf_classifier.py ###

# File: src/ml/rf_classifier.py

import joblib
import os
import logging
import numpy as np
from sklearn.ensemble import RandomForestClassifier as SKRF

class RandomForestClassifier:
    """
    A RandomForest-based classifier adapted from old_code/model.py
    """

    def __init__(self, model_dir='models'):
        self.model_dir = model_dir
        self.model = None
        self.classes_ = None
        self.feature_count = None
        os.makedirs(model_dir, exist_ok=True)

    def train(self, X, y):
        """
        Train a Random Forest classifier
        """
        try:
            self.model = SKRF(
                n_estimators=200,
                max_depth=None,
                min_samples_split=4,
                min_samples_leaf=2,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
            self.model.fit(X, y)
            self.classes_ = self.model.classes_
            self.feature_count = X.shape[1]
            logging.info(f"RF model trained with {len(self.classes_)} classes.")
            return True
        except Exception as e:
            logging.error(f"Error training RandomForest: {str(e)}")
            return False

    def predict(self, X):
        if self.model is None:
            logging.error("RandomForest model not trained or loaded.")
            return None, None
        try:
            X = np.array(X)
            if X.shape[1] != self.feature_count:
                logging.error("Feature count mismatch for RF.")
                return None, None
            predictions = self.model.predict(X)
            probabilities = self.model.predict_proba(X)
            return predictions, probabilities
        except Exception as e:
            logging.error(f"Error in RF prediction: {str(e)}")
            return None, None

    def get_top_predictions(self, X, top_n=3):
        X = np.array(X)
        if X.ndim == 1:
            X = X.reshape(1, -1)
        probs = self.model.predict_proba(X)
        top_indices = np.argsort(probs, axis=1)[:, -top_n:][:, ::-1]
        top_probabilities = np.take_along_axis(probs, top_indices, axis=1)
        top_labels = self.model.classes_[top_indices]
        results = []
        for labels, pvals in zip(top_labels, top_probabilities):
            pred_list = []
            for label, val in zip(labels, pvals):
                pred_list.append({"sound": label, "probability": float(val)})
            results.append(pred_list)
        return results

    def save(self, filename='rf_sound_classifier.joblib'):
        if self.model is None:
            logging.error("No RF model to save.")
            return False
        try:
            path = os.path.join(self.model_dir, filename)
            joblib.dump({
                'model': self.model,
                'classes': self.classes_,
                'feature_count': self.feature_count
            }, path)
            logging.info(f"RF model saved to {path}")
            return True
        except Exception as e:
            logging.error(f"Error saving RF model: {str(e)}")
            return False

    def load(self, filename='rf_sound_classifier.joblib'):
        try:
            path = os.path.join(self.model_dir, filename)
            model_data = joblib.load(path)
            self.model = model_data['model']
            self.classes_ = model_data['classes']
            self.feature_count = model_data['feature_count']
            logging.info(f"RF model loaded from {path}")
            return True
        except Exception as e:
            logging.error(f"Error loading RF model: {str(e)}")
            return False

********************************************************************************

### File: src/ml/sound_detector_ensemble.py ###

import numpy as np
import logging
import threading
import sounddevice as sd
from .audio_processing import SoundProcessor
from .feature_extractor import AudioFeatureExtractor
from .constants import SAMPLE_RATE, AUDIO_DURATION

class SoundDetectorEnsemble:
    """
    Real-time sound detector that uses an EnsembleClassifier for inference.
    It calls ensemble_model.predict(X_rf, X_cnn).
    """
    def __init__(self, ensemble_model):
        # ensemble_model is an EnsembleClassifier instance
        self.ensemble_model = ensemble_model
        
        # Queues and state
        self.audio_queue = []
        self.callback = None
        self.is_speech_active = False
        self.speech_duration = 0
        
        self.audio_queue_lock = threading.Lock()
        self.pre_buffer_duration_ms = 100
        self.buffer_index = 0
        self._stop_event = threading.Event()
        
        # This SoundProcessor might produce mel-spectrograms for the CNN
        self.sound_processor = SoundProcessor(sample_rate=SAMPLE_RATE)
        self.sound_processor.sound_threshold = 0.08
        
        # For the RF portion, we can use an 
        # (assuming you have something like this in your codebase)
        self.rf_extractor = AudioFeatureExtractor(sr=SAMPLE_RATE)
        self.feature_names = self.rf_extractor.get_feature_names()

        # Pre-buffer for short-latency detection
        self.circular_buffer = np.zeros(
            int(SAMPLE_RATE * self.pre_buffer_duration_ms / 1000),
            dtype=np.float32
        )

        self.confidence_threshold = 0.4
        self.stream = None
        self.is_recording = False

        logging.info("SoundDetectorEnsemble initialized.")

    def process_audio(self):
        try:
            with self.audio_queue_lock:
                if not self.audio_queue:
                    logging.info("No audio data in queue to process.")
                    return
                audio_data = np.concatenate(self.audio_queue)
                self.audio_queue.clear()

            # Normalize -1..1 if needed
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            # 1) Extract CNN features
            cnn_features = self.sound_processor.process_audio(audio_data)
            if cnn_features is None:
                logging.info("Could not extract CNN features.")
                return
            X_cnn = np.expand_dims(cnn_features, axis=0)

            # 2) Extract RF features
            # If your AudioFeatureExtractor requires a temp WAV file, you might write audio_data to disk
            # For an in-memory approach, adapt your AudioFeatureExtractor.  Example is simplified:
            temp_wav = "temp_ensemble_chunk.wav"
            self.rf_extractor.save_wav(audio_data, SAMPLE_RATE, temp_wav)
            feats = self.rf_extractor.extract_features(temp_wav)
            # os.remove(temp_wav)  # If you wish to clean up once done

            if not feats:
                logging.info("Could not extract RF features.")
                return
            row = [feats[fn] for fn in self.feature_names]
            X_rf = [row]

            # 3) Run Ensemble prediction
            # .predict(...) returns a list of (class_label, confidence)
            results = self.ensemble_model.predict(X_rf, X_cnn)
            predicted_label, top_conf = results[0]  # single row

            # 4) Decide if we pass it along
            if top_conf >= self.confidence_threshold:
                logging.info(
                    f"Ensemble predicted: {predicted_label} (conf: {float(top_conf):.4f})"
                )
            else:
                logging.info(
                    f"Ensemble predicted (low conf {float(top_conf):.4f}): {predicted_label}"
                )

            if self.callback:
                self.callback({
                    "class": predicted_label,
                    "confidence": float(top_conf)
                })

        except Exception as e:
            logging.error(f"Error in SoundDetectorEnsemble.process_audio: {e}", exc_info=True)

    def audio_callback(self, indata, frames, time_info, status):
        try:
            # Convert to float32
            audio_data = indata.flatten().astype(np.float32)
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            has_sound, _ = self.sound_processor.detect_sound(audio_data)

            # Update our circular buffer
            start_idx = self.buffer_index
            end_idx = start_idx + len(audio_data)
            if end_idx > len(self.circular_buffer):
                first_part = len(self.circular_buffer) - start_idx
                self.circular_buffer[start_idx:] = audio_data[:first_part]
                self.circular_buffer[:end_idx - len(self.circular_buffer)] = audio_data[first_part:]
            else:
                self.circular_buffer[start_idx:end_idx] = audio_data
            self.buffer_index = (self.buffer_index + len(audio_data)) % len(self.circular_buffer)

            if has_sound:
                if not self.is_speech_active:
                    logging.info("Ensemble: Sound detected!")
                    self.is_speech_active = True
                    self.speech_duration = 0

                    with self.audio_queue_lock:
                        pre_buffer = np.concatenate([
                            self.circular_buffer[self.buffer_index:], 
                            self.circular_buffer[:self.buffer_index]
                        ])
                        self.audio_queue.append(pre_buffer)

                with self.audio_queue_lock:
                    self.audio_queue.append(audio_data)

                self.speech_duration += len(audio_data) / SAMPLE_RATE

                if self.speech_duration >= AUDIO_DURATION:
                    self.process_audio()
                    self.is_speech_active = False
            else:
                if self.is_speech_active:
                    with self.audio_queue_lock:
                        self.audio_queue.append(audio_data)
                    self.process_audio()
                    self.is_speech_active = False
                    logging.info("Ensemble: Sound ended.")

        except Exception as e:
            logging.error(f"Error in SoundDetectorEnsemble.audio_callback: {e}", exc_info=True)

    def start_listening(self, callback=None, auto_stop=False):
        if self.is_recording:
            return False

        try:
            self.is_recording = True
            self.callback = callback

            logging.info("Starting ensemble audio stream...")

            self.stream = sd.InputStream(
                channels=1, dtype=np.float32, samplerate=SAMPLE_RATE,
                blocksize=int(SAMPLE_RATE * 0.03),  # 30 ms chunk
                callback=self.audio_callback
            )
            self.stream.start()
            return True
        except Exception as e:
            logging.error(f"Error starting ensemble listening: {e}", exc_info=True)
            self.is_recording = False
            return False

    def stop_listening(self):
        try:
            self.is_recording = False
            if self.stream:
                self.stream.stop()
                self.stream.close()
                self.stream = None
            logging.info("Stopped ensemble listening.")
        except Exception as e:
            logging.error(f"Error stopping ensemble listening: {e}", exc_info=True) 
********************************************************************************

### File: src/ml/sound_detector_rf.py ###

import numpy as np
import logging
import threading
import sounddevice as sd
from .audio_processing import SoundProcessor
from .constants import SAMPLE_RATE, AUDIO_DURATION

class SoundDetectorRF:
    """
    Real-time sound detector that uses a RandomForestClassifier for inference.
    Similar to SoundDetector, but calls rf_model.predict() on extracted features.
    """
    def __init__(self, rf_model):
        self.rf_model = rf_model
        self.audio_queue = []
        self.callback = None
        self.is_speech_active = False
        self.speech_duration = 0
        self.min_sound_duration = 0.3

        # NEW: track recording state
        self.is_recording = False
        self.stream = None

        # Same thresholds / buffering as your CNN SoundDetector
        self.audio_queue_lock = threading.Lock()
        self.pre_buffer_duration_ms = 100
        self.buffer_index = 0
        self._stop_event = threading.Event()

        # Setup SoundProcessor
        self.sound_processor = SoundProcessor(sample_rate=SAMPLE_RATE)
        self.sound_processor.sound_threshold = 0.08
        self.circular_buffer = np.zeros(int(SAMPLE_RATE * self.pre_buffer_duration_ms / 1000), dtype=np.float32)

        # Confidence threshold is optional; RF returns predicted labels but not a direct "confidence"
        self.confidence_threshold = 0.4

        logging.info("SoundDetectorRF initialized.")

    def process_audio(self):
        try:
            with self.audio_queue_lock:
                if not self.audio_queue:
                    logging.info("No audio data in queue to process.")
                    return
                audio_data = np.concatenate(self.audio_queue)
                self.audio_queue.clear()

            # Normalize -1..1 if needed
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            # Extract features (assuming your rf_classifier expects a feature row)
            features = self.sound_processor.process_audio(audio_data)
            if features is None:
                logging.info("No features extracted.")
                return

            # The RF classifier expects a 2D array: shape (n_samples, n_features)
            features_2d = np.expand_dims(features, axis=0)

            # Call the RF model's predict
            predictions, probabilities = self.rf_model.predict(features_2d)
            top_label = predictions[0]   # The predicted class label
            # probabilities is shape (1, n_classes), so:
            top_conf = max(probabilities[0]) if probabilities is not None else 1.0

            # If you want a threshold:
            if top_conf >= self.confidence_threshold:
                logging.info(f"RF predict: {top_label} (conf: {top_conf:.4f})")
            else:
                logging.info(f"RF predict (low conf {top_conf:.4f}): {top_label}")

            # If there's a callback, pass the result
            if self.callback:
                self.callback({
                    "class": top_label,
                    "confidence": float(top_conf)
                })

        except Exception as e:
            logging.error(f"Error in SoundDetectorRF.process_audio: {e}", exc_info=True)

    def audio_callback(self, indata, frames, time_info, status):
        """
        This is similar to your CNN code, except we still call self.process_audio()
        once enough audio is gathered.
        """
        try:
            audio_data = indata.flatten().astype(np.float32)
            if np.abs(audio_data).max() > 1.0:
                audio_data = audio_data / np.abs(audio_data).max()

            has_sound, _ = self.sound_processor.detect_sound(audio_data)

            # Update the circular buffer (just like your CNN-based code)
            start_idx = self.buffer_index
            end_idx = start_idx + len(audio_data)
            if end_idx > len(self.circular_buffer):
                first_part = len(self.circular_buffer) - start_idx
                self.circular_buffer[start_idx:] = audio_data[:first_part]
                self.circular_buffer[:end_idx - len(self.circular_buffer)] = audio_data[first_part:]
            else:
                self.circular_buffer[start_idx:end_idx] = audio_data
            self.buffer_index = (self.buffer_index + len(audio_data)) % len(self.circular_buffer)

            # If there's sound
            if has_sound:
                if not self.is_speech_active:
                    logging.info("RF: Sound detected!")
                    self.is_speech_active = True
                    self.speech_duration = 0

                    # Include pre-buffer for context
                    with self.audio_queue_lock:
                        pre_buffer = np.concatenate([
                            self.circular_buffer[self.buffer_index:],
                            self.circular_buffer[:self.buffer_index]
                        ])
                        self.audio_queue.append(pre_buffer)

                with self.audio_queue_lock:
                    self.audio_queue.append(audio_data)

                self.speech_duration += len(audio_data) / SAMPLE_RATE

                # If we've collected enough audio, process
                if self.speech_duration >= AUDIO_DURATION:
                    self.process_audio()
                    self.is_speech_active = False

            else:
                if self.is_speech_active:
                    # finalize
                    with self.audio_queue_lock:
                        self.audio_queue.append(audio_data)
                    self.process_audio()
                    self.is_speech_active = False
                    logging.info("RF: Sound ended.")

        except Exception as e:
            logging.error(f"Error in SoundDetectorRF.audio_callback: {e}", exc_info=True)

    def start_listening(self, callback=None, auto_stop=False):
        """
        Opens an audio stream (just like your CNN/Ensemble) so that audio_callback
        is continuously invoked, and we can queue/process data in real time.
        """
        if self.is_recording:
            return False
        try:
            self.is_recording = True
            self.callback = callback

            logging.info("Starting RF audio stream...")
            self.stream = sd.InputStream(
                channels=1,
                dtype=np.float32,
                samplerate=SAMPLE_RATE,
                blocksize=int(SAMPLE_RATE * 0.03),  # ~30 ms block
                callback=self.audio_callback
            )
            self.stream.start()
            return True
        except Exception as e:
            logging.error(f"Error starting RF listening: {e}", exc_info=True)
            self.is_recording = False
            return False

    def stop_listening(self):
        try:
            self.is_recording = False
            if self.stream:
                self.stream.stop()
                self.stream.close()
                self.stream = None
            logging.info("Stopped RF listening.")
            return {"status": "success", "message": "RF detector stopped."}
        except Exception as e:
            logging.error(f"Error stopping RF listener: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}

********************************************************************************

### File: src/ml/trainer.py ###

# SoundClassifier_v08/src/ml/trainer.py

import os
import logging
import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from .rf_classifier import RandomForestClassifier
from .cnn_classifier import build_model, build_dataset
from .feature_extractor import AudioFeatureExtractor
from .audio_processing import SoundProcessor
from config import Config
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from .data_augmentation import (
    time_shift,
    change_pitch,
    change_speed,
    add_colored_noise
)
from .constants import (
    AUG_DO_TIME_SHIFT, AUG_TIME_SHIFT_COUNT, AUG_SHIFT_MAX,
    AUG_DO_PITCH_SHIFT, AUG_PITCH_SHIFT_COUNT, AUG_PITCH_RANGE,
    AUG_DO_SPEED_CHANGE, AUG_SPEED_CHANGE_COUNT, AUG_SPEED_RANGE,
    AUG_DO_NOISE, AUG_NOISE_COUNT, AUG_NOISE_TYPE, AUG_NOISE_FACTOR,
    PITCH_SHIFTS_OUTER_VALUES, PITCH_SHIFTS_CENTER_START, 
    PITCH_SHIFTS_CENTER_END, PITCH_SHIFTS_CENTER_NUM,
    NOISE_TYPES_LIST, NOISE_LEVELS_MIN, NOISE_LEVELS_MAX, NOISE_LEVELS_COUNT,
    AUG_DO_COMPRESSION, AUG_COMPRESSION_COUNT,
    AUG_DO_REVERB, AUG_REVERB_COUNT,
    AUG_DO_EQUALIZE, AUG_EQUALIZE_COUNT
)
from .augmentation_manager import augment_audio_with_repetitions

#from .model import SoundClassifier
#import librosa
#import soundfile as sf
#from datetime import datetime
#from .sound_preprocessor import SoundPreprocessor
#from pydub import AudioSegment
#from sklearn.preprocessing import StandardScaler
#import pandas as pd
#import joblib
#import tempfile
#from tqdm import tqdm

class Trainer:
    def __init__(self, model_dir='models'):
        self.model_dir = model_dir
        self.rf_classifier = RandomForestClassifier(model_dir=model_dir)
        self.cnn_model = None
        self.class_names = None

    def collect_data_for_rf(self, goodsounds_dir):
        """
        Gather wavefiles from goodsounds_dir and extract "classical" features 
        (MFCC, pitch, etc.) for RandomForest training, with multi-run data augmentation.
        """
        logging.info(f"# NEW LOG: Starting collect_data_for_rf with goodsounds_dir={goodsounds_dir}")

        extractor = AudioFeatureExtractor(sr=16000)
        X = []
        y = []
        dictionary = Config.get_dictionary()

        # Create a SoundProcessor if you need it for boundary detection
        sound_proc = SoundProcessor(sample_rate=16000)

        # Counters for logging how many samples come from original vs. augmentation
        file_count = 0
        aug_count = 0

        for sound in dictionary['sounds']:
            path_sound = os.path.join(goodsounds_dir, sound)
            if not os.path.exists(path_sound):
                continue
            files = [f for f in os.listdir(path_sound) if f.endswith('.wav')]

            logging.info(f"# NEW LOG: Sound class='{sound}' has {len(files)} .wav files. path_sound={path_sound}")

            for filename in files:
                file_path = os.path.join(path_sound, filename)
                logging.info(f"# NEW LOG: Loading file {file_path}")
                try:
                    audio_samples, sr_ = librosa.load(
                        file_path, sr=extractor.sr, duration=extractor.duration
                    )

                    # Process audio (e.g., detect sound boundaries)
                    start_idx, end_idx, has_sound = sound_proc.detect_sound_boundaries(audio_samples)
                    logging.info(f"# NEW LOG: Boundaries -> start_idx={start_idx}, end_idx={end_idx}, has_sound={has_sound}")
                    audio_samples = audio_samples[start_idx:end_idx]

                    # Original features
                    feats_orig = extractor.extract_features_from_array(audio_samples, sr=sr_)
                    if feats_orig is not None:
                        row_orig = self._assemble_feature_row(feats_orig, extractor)
                        X.append(row_orig)
                        y.append(sound)
                        file_count += 1
                        logging.info(f"# NEW LOG: Added ORIG sample for {filename} (class: {sound})")
                    else:
                        logging.warning(f"# NEW LOG: Original sample from {filename} was skipped (no features).")

                    # Multi-run augmentation with explicit defaults:
                    logging.info(f"# NEW LOG: Attempting augmentation for {filename}")
                    augmented_versions = augment_audio_with_repetitions(
                        audio=audio_samples,
                        sr=sr_,
                        do_time_shift=AUG_DO_TIME_SHIFT,
                        time_shift_count=AUG_TIME_SHIFT_COUNT,
                        shift_max=AUG_SHIFT_MAX,

                        do_pitch_shift=AUG_DO_PITCH_SHIFT,
                        pitch_shift_count=AUG_PITCH_SHIFT_COUNT,
                        pitch_range=AUG_PITCH_RANGE,

                        do_speed_change=AUG_DO_SPEED_CHANGE,
                        speed_change_count=AUG_SPEED_CHANGE_COUNT,
                        speed_range=AUG_SPEED_RANGE,

                        do_noise=AUG_DO_NOISE,
                        noise_count=AUG_NOISE_COUNT,
                        noise_type=AUG_NOISE_TYPE,
                        noise_factor=AUG_NOISE_FACTOR,

                        do_compression=AUG_DO_COMPRESSION,
                        compression_count=AUG_COMPRESSION_COUNT,

                        do_reverb=AUG_DO_REVERB,
                        reverb_count=AUG_REVERB_COUNT,

                        do_equalize=AUG_DO_EQUALIZE,
                        equalize_count=AUG_EQUALIZE_COUNT
                    )
                    logging.info(f"# NEW LOG: Created {len(augmented_versions)} augmented clips for {filename}")

                    for aug_audio in augmented_versions:
                        aug_feats = extractor.extract_features_from_array(aug_audio, sr=sr_)
                        if aug_feats is not None:
                            row_aug = self._assemble_feature_row(aug_feats, extractor)
                            X.append(row_aug)
                            y.append(sound)
                            aug_count += 1
                            logging.info(f"# NEW LOG: Added AUG sample for {filename} (class: {sound})")
                        else:
                            logging.warning(f"# NEW LOG: Augmented clip was too short/invalid for {filename}, skipping.")
                except Exception as e:
                    logging.error(f"Error processing {file_path}: {str(e)}")
                    continue

        # Convert to numpy arrays
        X = np.array(X)
        y = np.array(y)

        # Final logs for clarity
        logging.info(f"Total original samples: {file_count}")
        logging.info(f"Total augmentation samples: {aug_count}")
        logging.info(f"Final dataset shape: X={X.shape}, y={y.shape}")

        return X, y

    def _assemble_feature_row(self, feats, extractor):
        """
        Helper method that replicates your 'row' building logic, 
        matching the feature_names from extractor.
        """
        feature_names = extractor.get_feature_names()
        row = []
        for fn in feature_names:
            row.append(feats.get(fn, 0.0))  # or do your custom MFCC index logic
        return row

    def train_rf(self, goodsounds_dir):
        """
        Collect data, train random forest, and save the model.
        """
        X, y = self.collect_data_for_rf(goodsounds_dir)
        success = self.rf_classifier.train(X, y)
        if success:
            self.rf_classifier.save()
        # Evaluate quickly
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        self.rf_classifier.train(X_train, y_train)
        preds, _ = self.rf_classifier.predict(X_test)
        acc = accuracy_score(y_test, preds)
        logging.info(f"RF accuracy on test: {acc:.3f}")

    def train_cnn(self, goodsounds_dir):
        """
        Train the CNN model from your existing pipeline, ensuring we call
        the same build_dataset(...) from cnn_classifier.py, which uses SoundProcessor
        for all audio transformations.
        """
        X, y, class_names, stats = build_dataset(goodsounds_dir)
        if X is None or y is None:
            logging.error("No data found for CNN training.")
            return None, None
        
        self.class_names = class_names

        # Shuffle data
        idx = np.arange(len(X))
        np.random.shuffle(idx)
        X = X[idx]
        y = y[idx]

        # Split
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        input_shape = X_train.shape[1:]
        
        # Build and compile model
        model, summary_str = build_model(input_shape, num_classes=len(class_names))
        
        checkpoint_path = os.path.join(self.model_dir, 'cnn_checkpoint.h5')
        callbacks = [
            EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-5),
            ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
        ]
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            batch_size=32,
            epochs=30,
            callbacks=callbacks
        )
        
        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
        logging.info(f"CNN val_acc: {val_acc:.3f}")

        # Save final
        model_path = os.path.join(self.model_dir, "audio_classifier.h5")
        model.save(model_path)
        self.cnn_model = model
        return model, class_names

    def train_ensemble_example(self):
        """
        Example code to show how you could load both trained models 
        and use the ensemble. You will adapt as needed in your real code.
        """
        # Load trained RF
        self.rf_classifier.load(filename='rf_sound_classifier.joblib')
        # Load trained CNN
        from tensorflow.keras.models import load_model
        model_path = os.path.join(self.model_dir, "audio_classifier.h5")
        self.cnn_model = load_model(model_path)

        # Then create an EnsembleClassifier, if desired
        from .ensemble_classifier import EnsembleClassifier
        ensemble = EnsembleClassifier(self.rf_classifier, self.cnn_model, self.class_names, rf_weight=0.5)
        ensemble.predict(X_rf, X_cnn)
        # ...

def preprocess_audio(filepath, temp_dir):
    #Preprocess audio to match inference preprocessing
    audio = AudioSegment.from_file(filepath, format="wav")
    # Normalize audio
    audio = audio.normalize()
    # Convert to mono if stereo
    if audio.channels > 1:
        audio = audio.set_channels(1)
    # Trim silence
    audio = audio.strip_silence(silence_thresh=-40)
    # Export preprocessed audio to a temporary file
    base_name = os.path.basename(filepath).replace('.wav', '_preprocessed.wav')
    temp_path = os.path.join(temp_dir, base_name)
    audio.export(temp_path, format="wav")
    return temp_path

# class SoundTrainer:
#     def __init__(self, good_sounds_dir, model_dir='models'):
#     #Initialize the trainer.  
#     #    Args:
#     #        good_sounds_dir (str): Directory containing verified sound files
#     #          model_dir (str): Directory to save/load model files
#     #
#         self.good_sounds_dir = good_sounds_dir
#         self.model_dir = model_dir
#         self.classifier = SoundClassifier(model_dir=model_dir)  # Create new instance
#         self.scaler = StandardScaler()
#         self.preprocessor = SoundPreprocessor(sample_rate=16000)  # Use same sample rate as inference
        
#     def augment_audio(self, y, sr):
#     #Apply enhanced audio augmentation techniques.
#         augmented = []
#         # Existing pitch shift augmentations
#         augmented.append(librosa.effects.pitch_shift(y, sr=sr, n_steps=1))
#         augmented.append(librosa.effects.pitch_shift(y, sr=sr, n_steps=-1))
        
#         # Add random noise
#         noise_factor = 0.005
#         noise = np.random.randn(len(y))
#         augmented.append(y + noise_factor * noise)
        
#         # Add background noise
#         def add_background_noise(y, noise_level=0.02):
#             noise = np.random.normal(0, 1, len(y))
#             return y + noise_level * np.max(np.abs(y)) * noise

#         augmented.append(add_background_noise(y))
        
#         # Dynamic range compression
#         compressed = librosa.effects.percussive(y)
#         augmented.append(compressed)
        
#         # Time shifting
#         def shift_time(y, shift_max=0.2):
#             shift = np.random.randint(int(sr * -shift_max), int(sr * shift_max))
#             return np.roll(y, shift)

#         augmented.append(shift_time(y))
        
#         # Equalization (simulate different recording devices)
#         def equalize(y):
#             return y * np.random.uniform(0.8, 1.2)

#         augmented.append(equalize(y))
        
#         # Reverberation (simulate different environments)
#         def add_reverb(y, sr):
#             return librosa.effects.preemphasis(y)

#         augmented.append(add_reverb(y, sr))
        
#         return augmented
    
#     def _format_features(self, features):
#         #Convert feature dictionary to list in consistent order
#         feature_values = []
#         for name in self.feature_extractor.get_feature_names():
#             if name.startswith('mfcc_'):
#                 parts = name.split('_')
#                 # Extract the feature type (mfcc, mfcc_delta, or mfcc_delta2)
#                 if 'delta2' in name:
#                     base_type = 'mfcc_delta2'
#                     idx = int(parts[2])
#                 elif 'delta' in name:
#                     base_type = 'mfcc_delta'
#                     idx = int(parts[2])
#                 else:
#                     base_type = 'mfcc'
#                     idx = int(parts[1])
                
#                 # Add the stat type (mean or std)
#                 stat_type = parts[-1]  # mean or std
#                 feature_values.append(features[f'{base_type}_{stat_type}'][idx])
#             else:
#                 feature_values.append(features[name])
#         return feature_values
    
#     def collect_training_data(self):
#         #Collect and process training data from verified sounds.
#         features_list = []
#         labels = []
        
#         # Process each file in the good sounds directory
#         for filename in os.listdir(self.good_sounds_dir):
#             if not filename.endswith('.wav'):
#                 continue
#             # Skip temporary files
#             if filename.startswith('temp_') or filename.endswith('_preprocessed.wav'):
#                 continue

#             sound_type = filename.split('_')[0]
#             filepath = os.path.join(self.good_sounds_dir, filename)
            
#             try:
#                 # Load audio
#                 y, sr = librosa.load(filepath, sr=16000)  # Use same sample rate as inference
                
#                 # Get features using shared preprocessor
#                 features = self.preprocessor.preprocess_array(y)
                
#                 if features is not None:
#                     features_list.append(features)
#                     labels.append(sound_type)

#                     # Add augmented versions
#                     augmented_audio = self.augment_audio(y, sr)
#                     for aug_y in augmented_audio:
#                         aug_features = self.preprocessor.preprocess_array(aug_y)
#                         if aug_features is not None:
#                             features_list.append(aug_features)
#                             labels.append(sound_type)
#             except Exception as e:
#                 logging.error(f"Error processing file {filepath}: {e}")
#                 continue

#         if not features_list:
#             logging.error("No valid training data found")
#             return None, None

#         # Stack features and convert labels
#         features_array = np.stack(features_list)
#         labels_array = np.array(labels)

#         return features_array, labels_array
    
#     def train_model(self, progress_callback=None):
#         #Train the model on verified sounds.
#         try:
#             # Data collection and preprocessing
#             if progress_callback:
#                 progress_callback(10)  # Data collection started

#             X, y = self.collect_training_data()
#             if X is None or len(X) == 0:
#                 logging.error("No training data collected")
#                 return False

#             logging.info(f"Collected training data: X shape={X.shape}, y shape={y.shape}")
#             logging.info(f"Unique classes in training data: {np.unique(y)}")

#             if progress_callback:
#                 progress_callback(50)  # Data collection completed

#             # Feature scaling
#             try:
#                 self.scaler.fit(X)
#                 X_scaled = self.scaler.transform(X)
#                 logging.info(f"Data scaled successfully. X_scaled shape={X_scaled.shape}")
#             except Exception as e:
#                 logging.error(f"Error during feature scaling: {str(e)}")
#                 raise

#             # Model training
#             if progress_callback:
#                 progress_callback(60)  # Model training started

#             success = self.classifier.train(X_scaled, y)

#             if progress_callback:
#                 progress_callback(90)  # Model training completed

#             # Model saving
#             if success:
#                 # Save dictionary info with model
#                 current_dict = Config.get_dictionary()
#                 model_info = {
#                     'dictionary_name': current_dict['name'],
#                     'dictionary_sounds': current_dict['sounds'],
#                     'training_time': datetime.now().isoformat(),
#                     'feature_names': self.feature_extractor.get_feature_names(),
#                     'feature_count': X_scaled.shape[1],
#                     'scaler': self.scaler  # Save the scaler
#                 }
#                 logging.info(f"Saving model with info: {model_info}")
#                 self.classifier.save_with_info(model_info)

#                 if progress_callback:
#                     progress_callback(100)  # Training complete

#             return success

#         except Exception as e:
#             logging.error(f"Error training model: {str(e)}")
#             logging.error("Full traceback:", exc_info=True)
#             if progress_callback:
#                 progress_callback(-1)  # Indicate error
#             return False
    
#     def evaluate_model(self, test_size=0.2):
#         #Evaluate model performance using train-test split.
        
#         #    Args:
#         #        test_size (float): Proportion of data to use for testing
            
#         #    Returns:
#         #        dict: Dictionary of evaluation metrics
        
#         from sklearn.model_selection import train_test_split
#         from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
        
#         X, y = self.collect_training_data()
#         if X is None or len(X) == 0:
#             return None
            
#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(
#             X, y, test_size=test_size, random_state=42
#         )
        
#         # Train on training set
#         self.classifier.train(X_train, y_train)
        
#         # Predict on test set
#         y_pred, y_proba = self.classifier.predict(X_test)
        
#         # Calculate metrics
#         unique_sounds = sorted(set(y))
#         per_sound_metrics = {}
        
#         for sound in unique_sounds:
#             mask = y_test == sound
#             if any(mask):
#                 sound_metrics = {
#                     'accuracy': accuracy_score(y_test[mask], y_pred[mask]),
#                     'samples': sum(mask),
#                     'correct': sum((y_test == y_pred) & mask),
#                     'confidence': np.mean(np.max(y_proba[mask], axis=1))
#                 }
#                 per_sound_metrics[sound] = sound_metrics
        
#         metrics = {
#             'accuracy': accuracy_score(y_test, y_pred),
#             'report': classification_report(y_test, y_pred),
#             'confusion_matrix': confusion_matrix(y_test, y_pred),
#             'per_sound': per_sound_metrics,
#             'class_names': unique_sounds
#         }
        
#         return metrics

    # def time_stretch_audio(self, y, sr, rate=1.0):
    # Time-stretch an audio signal using resampling.
    # Calculate the target sample rate
    # target_sr = int(sr / rate)
    # Resample the audio signal
    # y_stretched = librosa.resample(y, sr, target_sr)
    # return y_stretched
********************************************************************************

### File: src/routes/ml_routes.py ###

# SoundClassifier_v08/src/routes/ml_routes.py

LATEST_PREDICTION = None

# Standard library imports
import os
import json
import logging
import time
import threading
from datetime import datetime

# Flask imports
from flask import (
    Blueprint, render_template, request, session,
    redirect, url_for, flash, jsonify, Response, stream_with_context
)
from flask import current_app

# Scientific/ML imports
import numpy as np
import tensorflow as tf
from tensorflow.keras import models
from tensorflow.keras.models import load_model
from pydub import AudioSegment
import librosa

# Local application imports
from config import Config
from src.ml.cnn_classifier import build_model, build_dataset
from src.ml.rf_classifier import RandomForestClassifier
from src.ml.ensemble_classifier import EnsembleClassifier
from src.ml.inference import predict_sound, SoundDetector
from src.ml.audio_processing import SoundProcessor
from src.ml.trainer import Trainer
from src.ml.sound_detector_rf import SoundDetectorRF
from src.ml.sound_detector_ensemble import SoundDetectorEnsemble
from src.ml.feature_extractor import AudioFeatureExtractor
from src.ml.model_paths import get_model_dir, get_cnn_model_path, get_rf_model_path, save_model_metadata
           
ml_bp = Blueprint('ml', __name__)

@ml_bp.before_app_request
def init_app_inference_stats():
    """
    Ensures that current_app.inference_stats is set before handling ML routes,
    but only after the Flask application has a valid context.
    """

    if not hasattr(current_app, 'inference_stats'):
        current_app.inference_stats = {
            'total_predictions': 0,
            'class_counts': {},
            'confidence_levels': [],
            'confusion_matrix': {},
            'misclassifications': [],
            'correct_classifications': []
        }
        logging.debug("Initialized current_app.inference_stats.")

    # Minimal addition:
    if not hasattr(current_app, 'latest_prediction'):
        LATEST_PREDICTION = None
        logging.debug("Initialized LATEST_PREDICTION = None.")

# Store your dictionary here at module level
INFERENCE_STATS_DEFAULT = {
    'total_predictions': 0,
    'class_counts': {},
    'confidence_levels': []
}

# Global references (optional)
sound_detector = None
detector_lock = threading.Lock()

training_stats = None
training_history = None
model_summary_str = None

# For SSE
class DebugLogHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.logs = []
    def emit(self, record):
        self.logs.append(self.format(record))

debug_log_handler = DebugLogHandler()
debug_log_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
debug_log_handler.setFormatter(formatter)
logging.getLogger().addHandler(debug_log_handler)

# Global variable to avoid app context for SSE:
#LATEST_PREDICTION = None

def prediction_callback(prediction):
    """
    Called by the SoundDetector when a new inference is made in real-time.
    We update `current_app.inference_stats` and set `LATEST_PREDICTION`.
    """
    global LATEST_PREDICTION
    LATEST_PREDICTION = prediction
    logging.info(f"Got prediction: {prediction}")
   

    stats = current_app.inference_stats
    stats['total_predictions'] += 1
    c = prediction['class']
    conf = prediction['confidence']
    actual = prediction.get('actual_sound')

    stats['class_counts'].setdefault(c, 0)
    stats['class_counts'][c] += 1
    stats['confidence_levels'].append(conf)

    # More advanced confusion matrix if you want
    if 'confusion_matrix' not in stats:
        stats['confusion_matrix'] = {}
    if 'misclassifications' not in stats:
        stats['misclassifications'] = []
    if 'correct_classifications' not in stats:
        stats['correct_classifications'] = []

    cm = stats['confusion_matrix']
    if actual:
        if actual not in cm:
            cm[actual] = {}
        if c not in cm[actual]:
            cm[actual][c] = 0
        cm[actual][c] += 1

        detail = {
            'predicted': c,
            'actual': actual,
            'confidence': conf,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        if c == actual:
            stats['correct_classifications'].append(detail)
        else:
            stats['misclassifications'].append(detail)

    # Also store top-level prediction in a global var for SSE
    #global LATEST_PREDICTION
    #LATEST_PREDICTION = prediction
@ml_bp.route('/train_model', methods=['GET', 'POST'])
def train_model():
    """
    Allows the user to choose between CNN, RF, Ensemble, or 'All'.
    CNN logic was already present. We add minimal new code
    for the 'rf' option, an 'ensemble' option, and ensure 'all' calls both.
    """
    if request.method == 'GET':
        # Display the form (train_model.html) with the dropdown to select method
        # Existing code: no changes here
        sounds = get_sound_list()  # Example function to list sounds
        return render_template('train_model.html', sounds=sounds)

    # POST: user clicked "Start Training" with a chosen method
    train_method = request.form.get('train_method', 'cnn')  # defaults to 'cnn'
    goodsounds_dir = get_goodsounds_dir_path()  # however you get your data path
    #global training_stats, training_history, model_summary_str

    try:
        # Turn off debug if needed (per your existing code)
        original_debug = current_app.config['DEBUG']
        current_app.config['DEBUG'] = False

        # Create an instance of your Trainer (if you have one)
        trainer = Trainer(model_dir=os.path.join(Config.PROJECT_ROOT, 'models'))

        # We will store some results in these variables
        rf_result = None
        cnn_result = None
        ensemble_result = None

        # --------------------------------------------------------------------
        # 1) CNN Only
        # --------------------------------------------------------------------
        if train_method == 'cnn':
            logging.info("# NEW LOG: Entering CNN training block (cnn only).")

            # (A) Build dataset for CNN
            X, y, class_names, stats = build_dataset(goodsounds_dir)
            if X is None or y is None:
                flash("No training data found for CNN.")
                return redirect(url_for('ml.train_model'))

            # (B) Shuffle / split data
            idx = np.arange(len(X))
            np.random.shuffle(idx)
            X = X[idx]
            y = y[idx]
            split_idx = int(len(X)*0.8)
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            input_shape = X_train.shape[1:]

            # (C) Build & train the CNN model
            model, model_summary_cnn = build_model(input_shape, num_classes=len(class_names))
            class_weights = {}
            total_samples = len(y_train)
            for i in range(len(class_names)):
                ccount = (y_train == i).sum()
                class_weights[i] = total_samples / (len(class_names)*ccount)

            callback = tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=5,
                min_delta=0.01,
                restore_best_weights=True
            )
            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_accuracy',
                factor=0.2,
                patience=3,
                min_lr=1e-5
            )

            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=50,
                class_weight=class_weights,
                callbacks=[callback, reduce_lr],
                batch_size=32,
                verbose=1
            )

            # (D) Optionally evaluate on val set
            val_loss, val_acc = model.evaluate(X_val, y_val)

            # (E) Build training_stats & training_history from the CNN
            training_stats, training_history = build_training_stats_and_history(
                X_train, y_train, X_val, y_val,
                class_names, stats, model_summary_cnn, history
            )
            model_summary_str = model_summary_cnn
            cnn_result = {'val_acc': val_acc, 'model': model}  # keep for ensemble?

        # --------------------------------------------------------------------
        # 2) RF Only
        # --------------------------------------------------------------------
        elif train_method == 'rf':  # NEW
            logging.info("# NEW LOG: Entering RF training block (rf only).")

            # Actually call trainer.train_rf(...) or whichever function
            # does your random forest data prep + training.
            success = trainer.train_rf(goodsounds_dir)

            # You might get an accuracy or some stats back from the trainer
            # Possibly store them in training_stats if your summary references them
            rf_accuracy = trainer.get_rf_accuracy()  # Example of how you might retrieve

            # Minimal example of building "training_stats"
            training_stats = {
                'method': 'Random Forest Only',
                'rf_acc': rf_accuracy,
                # Add more needed fields, e.g. #samples, label mapping, etc.
            }
            training_history = None
            model_summary_str = "Random Forest does not have a Keras-style summary."
            rf_result = {'val_acc': rf_accuracy}

        # --------------------------------------------------------------------
        # 3) Ensemble Only
        # --------------------------------------------------------------------
        elif train_method == 'ensemble':  # NEW
            logging.info("# NEW LOG: Entering Ensemble training block (cnn+rf).")

            # You might need a current CNN model and a current RF model
            # If you want to train them freshly here, do so, or load from disk
            # Minimal approach: 
            success_rf = trainer.train_rf(goodsounds_dir)
            X, y, class_names, stats = build_dataset(goodsounds_dir)
            input_shape = X.shape[1:]
            model, model_summary_cnn = build_model(input_shape, len(class_names))
            # Maybe skip the full .fit() if you're just loading existing weights
            # or do a short training. Example:
            history = model.fit(X, y, epochs=2, verbose=1)  # short train or load

            # Then do your ensemble logic, e.g. weighted average:
            # predictions_cnn = model.predict(...)
            # predictions_rf = trainer.rf_classifier.predict_proba(...)
            # ensemble_preds = 0.5 * predictions_cnn + 0.5 * predictions_rf
            # ...
            ensemble_acc = 0.99  # example placeholder

            # Build training_stats for summary
            training_stats = {
                'method': 'Ensemble Only',
                'ensemble_acc': ensemble_acc,
                # Possibly store partial stats for CNN, RF as well
            }
            model_summary_str = "Ensemble combines CNN + RF results."
            training_history = None
            ensemble_result = {'val_acc': ensemble_acc}

        # --------------------------------------------------------------------
        # 4) All: CNN + RF + Output Ensemble
        # --------------------------------------------------------------------
        elif train_method == 'all':  # NEW
            logging.info("# NEW LOG: Entering ALL training block (cnn + rf + ensemble).")

            #  A) Train CNN
            X, y, class_names, stats = build_dataset(goodsounds_dir)
            ...
            # The same CNN code from above, or
            model, model_summary_cnn, c_acc = train_cnn_briefly(X, y, class_names, stats)
            cnn_result = {'val_acc': c_acc, 'model': model}

            #  B) Train RF
            success_rf = trainer.train_rf(goodsounds_dir)
            rf_accuracy = trainer.get_rf_accuracy()

            #  C) Do ensemble
            # E.g. do a weighted average from both newly trained models
            # ensemble_acc = compute_ensemble_accuracy(model, trainer.rf_classifier)
            ensemble_acc = 0.98  # placeholder

            #  D) Build training_stats with all 3 results
            training_stats = {
                'method': 'All (CNN + RF + Ensemble)',
                'cnn_acc': c_acc,
                'rf_acc': rf_accuracy,
                'ensemble_acc': ensemble_acc,
                # add shapes, stats, etc. if needed
            }
            training_history = None
            model_summary_str = "Trained CNN, trained RF, computed ensemble."
            ensemble_result = {'val_acc': ensemble_acc}

        else:
            # If the user somehow selected something else, default to CNN or redirect
            logging.warning(f"Unknown train method {train_method}, defaulting to CNN.")
            return redirect(url_for('ml.train_model'))

    except Exception as e:
        logging.error(f"Error during training: {e}", exc_info=True)
        flash(str(e), 'error')
    finally:
        current_app.config['DEBUG'] = original_debug

    # Once we finish training, redirect to the summary page
    return redirect(url_for('ml.model_summary'))
    
# @ml_bp.route('/train_model', methods=['GET', 'POST'])
# def train_model():
#     # Turn off debug
#     original_debug = current_app.config['DEBUG']
#     current_app.config['DEBUG'] = False

#     try:
#         # NEW LOG to confirm this route is running and the request method
#         logging.info(f"# NEW LOG: Entered 'train_model' route, request.method={request.method}")

#         # Make sure user is admin
#         if 'username' not in session or not session.get('is_admin'):
#             flash("Admin access required.")
#             return redirect(url_for('index'))

#         if request.method == 'GET':
#             # Render the form
#             active_dict = Config.get_dictionary()
#             return render_template('train_model.html', sounds=active_dict.get('sounds', []))

#         # ---------- POST -----------
#         # (A) We read the train_method from the form
#         train_method = request.form.get('train_method', 'cnn').lower()
#         logging.info(f"# NEW LOG: train_method='{train_method}'")

#         goodsounds_dir = os.path.join(Config.STATIC_DIR, 'goodsounds')
#         dict_name = Config.get_dictionary().get('name','Default')
#         safe_dict_name = dict_name.replace(' ','_')

#         # Global variables 
#         #global training_stats, training_history, model_summary_str
#         training_stats = None
#         training_history = None
#         model_summary_str = None

#         # Create the trainer
#         trainer = Trainer(model_dir='models')
#         logging.info("# NEW LOG: Trainer instance created.")

#         # ----------------------------------------------------------------
#         # 1) CNN training block (unchanged)
#         # ----------------------------------------------------------------
#         if train_method in ['cnn', 'all', 'ensemble']:
#             logging.info("# NEW LOG: Entering CNN training block.")
#             X, y, class_names, stats = build_dataset(goodsounds_dir)
#             if X is None or y is None:
#                 flash("No training data found for CNN.")
#                 return redirect(url_for('ml.train_model'))

#             # Shuffle
#             idx = np.arange(len(X))
#             np.random.shuffle(idx)
#             X = X[idx]
#             y = y[idx]

#             # Split
#             split_idx = int(len(X)*0.8)
#             X_train, X_val = X[:split_idx], X[split_idx:]
#             y_train, y_val = y[:split_idx], y[split_idx:]

#             input_shape = X_train.shape[1:]
#             model, model_summary = build_model(input_shape, num_classes=len(class_names))

#             # Class balancing
#             class_weights = {}
#             total_samples = len(y_train)
#             for i in range(len(class_names)):
#                 ccount = (y_train == i).sum()
#                 class_weights[i] = total_samples / (len(class_names)*ccount)

#             checkpoint_path = os.path.join(Config.PROJECT_ROOT, 'models', f'{safe_dict_name}_checkpoint.h5')
#             os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)

#             callback = tf.keras.callbacks.EarlyStopping(
#                 monitor='val_accuracy',
#                 patience=5,
#                 min_delta=0.01,
#                 restore_best_weights=True
#             )
#             reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
#                 monitor='val_accuracy',
#                 factor=0.2,
#                 patience=3,
#                 min_lr=1e-5
#             )
#             checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
#                 checkpoint_path,
#                 monitor='val_accuracy',
#                 save_best_only=True,
#                 save_weights_only=False,
#                 mode='max',
#                 verbose=1
#             )

#             history = model.fit(
#                 X_train, y_train,
#                 validation_data=(X_val, y_val),
#                 batch_size=32,
#                 epochs=30,
#                 class_weight=class_weights,
#                 callbacks=[callback, reduce_lr, checkpoint_cb]
#             )
#             val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
#             logging.info(f"CNN val_acc: {val_acc:.3f}")

#             # Save final
#             final_model_path = os.path.join('models', f"{safe_dict_name}_model.h5")
#             model.save(final_model_path)
#             model_summary_str = model_summary

#             # Update global stats for the summary page
#             mfcc_min = float(X.min())
#             mfcc_max = float(X.max())
#             training_stats = {
#                 'total_samples': len(X_train)+len(X_val),
#                 'class_distribution': {
#                     class_names[i]: int((y == i).sum())
#                     for i in range(len(class_names))
#                 },
#                 'original_counts': stats.get('original_counts', {}),
#                 'augmented_counts': stats.get('augmented_counts', {}),
#                 'input_shape': str(input_shape),
#                 'mfcc_range': (mfcc_min, mfcc_max),
#             }

#             # ------------------ ADDED: store classes, short explanation ----
#             training_stats['classes'] = class_names  # <-- ADDED (stores the actual class labels)
#             training_stats['label_mapping'] = {i: class_names[i] for i in range(len(class_names))}
#             training_stats['simple_explanation'] = (
#                 f"The model was trained on {len(class_names)} classes: {', '.join(class_names)}. "
#                 f"We used {len(X)} audio samples (including augmented), then split them 80/20 "
#                 "into training and validation sets."
#             )  # <-- ADDED

#             # --------------------------------------------------------------

#             logging.info(f"Original counts: {training_stats['original_counts']}")
#             logging.info(f"Augmented counts: {training_stats['augmented_counts']}")

#             training_history = {
#                 'epochs': len(history.history['accuracy']),
#                 'accuracy': history.history['accuracy'],
#                 'val_accuracy': history.history['val_accuracy'],
#                 'loss': history.history['loss'],
#                 'val_loss': history.history['val_loss']
#             }

#             # Save class_names
#             np.save(os.path.join('models','class_names.npy'), np.array(class_names))

#             # Store a CNN summary dictionary
#             training_stats['cnn_summary'] = {
#                 'num_samples': len(X),
#                 'num_classes': len(np.unique(y)),
#                 'cnn_acc': float(history.history['accuracy'][-1]) if 'accuracy' in history.history else None,
#                 'val_acc': float(history.history['val_accuracy'][-1]) if 'val_accuracy' in history.history else None,
#                 'model_path': os.path.join('models', 'Two_words_model.h5'),  # wherever you saved the CNN .h5
#                 'architecture': model_summary,  # or any text describing the CNN architecture
#                 'notes': "Trained with 2 classes (eh, oh), performed 10 epochs. Potential overfitting if dataset is small."
#             }
#             logging.info(f"Stored cnn_summary: {training_stats['cnn_summary']}")

#             # ================= NEW CODE - minimal addition =================
#             from src.ml.model_paths import get_cnn_model_path, save_model_metadata

#             # Build the new folder path based on dictionary name & version
#             folder = get_model_dir('models', dict_name, "CNN", version="v1")
#             new_cnn_path = get_cnn_model_path('models', dict_name, version="v1")

#             # Save model *also* into new structured path
#             model.save(new_cnn_path)
#             logging.info(f"CNN model also saved to: {new_cnn_path}")

#             # Optionally store training metadata as JSON
#             meta = {
#                 "model_type": "CNN",
#                 "dictionary": dict_name,
#                 "model_path": new_cnn_path,
#                 "epochs": len(history.history['accuracy']),
#                 "train_params": {
#                     "class_weights": class_weights,
#                     "augmentation": stats.get("augmented_counts", {})
#                 }
#             }
#             model_info_path = save_model_metadata(os.path.dirname(new_cnn_path), meta)
#             logging.info(f"Model metadata saved to: {model_info_path}")
#             # ================= END NEW CODE =================

#             flash("CNN training completed.")



#         if train_method in ['rf', 'all', 'ensemble']:
#             ...
#             # no lines removed, your existing code remains here

#         # ----------------------------------------------------------------
#         # 3) Ensemble ...
#         # ----------------------------------------------------------------
#         if train_method == 'ensemble':
#             ...
#             # no lines removed

#         if train_method == 'all':
#             ...
#             # no lines removed

#         logging.info("# NEW LOG: Training completed, redirecting to 'model_summary'.")
#         return redirect(url_for('ml.model_summary'))

#     except Exception as e:
#         logging.error(f"Error in training: {str(e)}", exc_info=True)
#         flash(f"Training error: {str(e)}")
#         return redirect(url_for('ml.train_model'))

#     finally:
#         # Turn debug back on
#         current_app.config['DEBUG'] = original_debug

@ml_bp.route('/model_summary')
def model_summary():
    #global training_stats, training_history, model_summary_str
    if not training_stats:
        training_stats = {
            'input_shape': '(Unknown)',
            'total_samples': 0,
            'original_counts': {},
            'augmented_counts': {}
        }
    # ------------------ ADDED: store broad console logs if wanted ----
    # In general, you might want to store console logs in a global
    # variable or a file. That's more advanced. For now, let's just pass
    # training_stats to the template, which now includes 'classes', 'simple_explanation', etc.
    # ---------------------------------------------------------------
    return render_template(
        'model_summary.html',
        training_stats=training_stats,
        model_summary=model_summary_str,
        training_history=training_history
    )

@ml_bp.route('/model_summary_enhanced', methods=['GET'])
def model_summary_enhanced():
    #global training_stats, training_history, model_summary_str

    if training_stats is None:
        # Make sure it's at least an empty dictionary
        training_stats = {}
    
    return render_template(
        "model_summary_enhanced.html",
        training_stats=training_stats,
        training_history=training_history,
        model_summary=model_summary_str
    )

@ml_bp.route('/predict', methods=['GET', 'POST'])
def predict():
    """
    GET: Show a simple form
    POST: handle a file or mic input
    """
    active_dict = Config.get_dictionary()
    if 'username' not in session:
        session['username'] = 'guest'

    if request.method == 'POST':
        try:
            use_microphone = (request.form.get('input_type') == 'microphone')
            dict_name = active_dict['name']
            model_path = os.path.join('models', f"{dict_name.replace(' ','_')}_model.h5")
            if not os.path.exists(model_path):
                flash("No CNN model found for current dictionary.")
                return redirect(url_for('ml.predict'))

            model = models.load_model(model_path)
            class_names = active_dict['sounds']

            if use_microphone:
                # Microphone approach
                predicted_class, confidence = predict_sound(model, None, class_names, use_microphone=True)
                if predicted_class:
                    flash(f"Predicted: {predicted_class} (conf={confidence:.3f})")
            else:
                # File upload
                audio_file = request.files.get('audio_data')
                if not audio_file or audio_file.filename == '':
                    flash("No audio file provided.")
                    return redirect(url_for('ml.predict'))

                temp_path = os.path.join(Config.TEMP_DIR, 'temp_audio.wav')
                with open(temp_path, 'wb') as f:
                    f.write(audio_file.read())

                pred_class, confidence = predict_sound(model, temp_path, class_names, use_microphone=False)
                os.remove(temp_path)
                if pred_class:
                    flash(f"Predicted: {pred_class} (conf={confidence:.3f})")

            return redirect(url_for('ml.predict'))
        except Exception as e:
            logging.error(f"Predict error: {e}", exc_info=True)
            flash(f"Error: {str(e)}")
            return redirect(url_for('ml.predict'))
    return render_template('inference.html', active_dict=active_dict)

@ml_bp.route('/predict_rf', methods=['POST'])
def predict_rf():
    """
    This route processes an uploaded audio file for RandomForest prediction.
    It loads the file, uses a SoundProcessor instance to optionally trim silence,
    writes the trimmed audio back to disk, and then extracts classical features
    using AudioFeatureExtractor. Finally, it runs RF inference and returns the result.
    
    NOTE: Make sure you always instantiate SoundProcessor (and similar classes)
    rather than calling class methods directly. For example, use:
         sp = SoundProcessor(sample_rate=16000)
         features = sp.process_audio(audio)
    not
         SoundProcessor.process_audio(audio)
    """
    # 1) Load the active dictionary and get the list of sounds
    active_dict = Config.get_dictionary()
    class_names = active_dict.get('sounds', [])

    # 2) Load the Random Forest model
    rf_path = os.path.join('models', f"{active_dict['name'].replace(' ', '_')}_rf.joblib")
    if not os.path.exists(rf_path):
        return jsonify({"error": "No random forest model for current dictionary."}), 400

    rf = RandomForestClassifier(model_dir='models')
    if not rf.load(filename=os.path.basename(rf_path)):
        return jsonify({"error": "Failed loading RF model."}), 500

    # 3) Read the uploaded audio file
    uploaded_file = request.files.get('audio')
    if not uploaded_file:
        return jsonify({"error": "No audio file"}), 400

    temp_path = os.path.join(Config.TEMP_DIR, 'temp_for_rf.wav')
    uploaded_file.save(temp_path)

    try:
        # 4) Load audio from the temporary file
        # Note: This call creates an instance of SoundProcessor so that we can use its methods.
        sp = SoundProcessor(sample_rate=16000)
        audio_data, _ = librosa.load(temp_path, sr=16000)

        # 5) Use SoundProcessor.detect_sound_boundaries() to trim silence
        start_idx, end_idx, has_sound = sp.detect_sound_boundaries(audio_data)
        trimmed_data = audio_data[start_idx:end_idx]
        if len(trimmed_data) == 0:
            return jsonify({"error": "No sound detected in audio"}), 400

        # Overwrite the temp file with the trimmed audio
        wavfile.write(temp_path, 16000, np.int16(trimmed_data * 32767))

        # 6) Extract classical features using AudioFeatureExtractor.
        # AudioFeatureExtractor.extract_features() expects a file path.
        extractor = AudioFeatureExtractor(sr=16000)
        feats = extractor.extract_features(temp_path)
        if not feats:
            return jsonify({"error": "Feature extraction failed"}), 500

        # 7) Create a feature row in the same order as expected by the RF model
        feature_names = extractor.get_feature_names()
        row = [feats[fn] for fn in feature_names]

        # 8) Run RandomForest prediction
        preds, probs = rf.predict([row])
        if preds is None:
            return jsonify({"error": "RF predict() returned None"}), 500

        predicted_class = preds[0]
        confidence = float(np.max(probs[0]))  # highest probability

        return jsonify({
            "predictions": [{
                "sound": predicted_class,
                "probability": confidence
            }]
        })

    except Exception as e:
        logging.error(f"Error during RF prediction: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

    finally:
        # Clean up the temporary file if it exists
        if os.path.exists(temp_path):
            os.remove(temp_path)
         
@ml_bp.route('/predict_ensemble', methods=['POST'])
def predict_ensemble():
    active_dict = Config.get_dictionary()
    class_names = active_dict['sounds']
    
    # 1) load CNN
    cnn_path = os.path.join('models', f"{active_dict['name'].replace(' ','_')}_model.h5")
    if not os.path.exists(cnn_path):
        return jsonify({"error":"No CNN model"}), 400
    cnn_model = load_model(cnn_path)
    
    # 2) load RF
    rf_path = os.path.join('models', f"{active_dict['name'].replace(' ','_')}_rf.joblib")
    if not os.path.exists(rf_path):
        return jsonify({"error":"No RF model"}), 400
    
    rf = RandomForestClassifier(model_dir='models')
    rf.load(filename=os.path.basename(rf_path))
    
    # 3) create ensemble
    ensemble = EnsembleClassifier(rf, cnn_model, class_names, rf_weight=0.5)
    
    # 4) get the posted audio, do feature extraction for RF and for CNN
    file = request.files.get('audio')
    if not file:
        return jsonify({"error":"No audio file"}), 400
    
    temp_path = os.path.join(Config.TEMP_DIR, 'temp_ensemble.wav')
    file.save(temp_path)
    
    # For CNN: process with SoundProcessor
    sp = SoundProcessor(sample_rate=16000)
    wav_data, _ = librosa.load(temp_path, sr=16000)
    cnn_features = sp.process_audio(wav_data)
    if cnn_features is None:
        return jsonify({"error":"No features for CNN"}), 500
    X_cnn = np.expand_dims(cnn_features, axis=0)
    
    # For RF: use AudioFeatureExtractor
    feats = AudioFeatureExtractor(sr=16000).extract_features(temp_path)
    os.remove(temp_path)
    if not feats:
        return jsonify({"error":"Feature extraction failed for RF"}), 500
    
    feature_names = AudioFeatureExtractor(sr=16000).get_feature_names()
    row = [feats[fn] for fn in feature_names]
    X_rf = [row]
    
    # 5) get ensemble predictions
    top_preds = ensemble.get_top_predictions(X_rf, X_cnn, top_n=1)[0]
    # top_preds is something like [{"sound":..., "probability":...}]
    
    return jsonify({
        "predictions": top_preds
    })

@ml_bp.route('/predict_sound', methods=['POST'])
def predict_sound_endpoint():
    """
    For chunk-based or single-file predictions posted via AJAX:
    Expects a field 'audio' with a wav/blob, uses active dictionary's CNN model.
    Returns JSON with predictions.
    """
    active_dict = Config.get_dictionary()
    dict_name = active_dict['name']
    model_path = os.path.join('models', f"{dict_name.replace(' ','_')}_model.h5")
    if not os.path.exists(model_path):
        return jsonify({"error": f"No model for dictionary {dict_name}"}), 400

    audio_file = request.files.get('audio')
    if not audio_file:
        return jsonify({"error": "No audio file"}), 400

    model = models.load_model(model_path)
    class_names = active_dict['sounds']

    temp_path = os.path.join(Config.TEMP_DIR, 'predict_temp.wav')
    with open(temp_path, 'wb') as f:
        f.write(audio_file.read())

    pred_class, confidence = predict_sound(model, temp_path, class_names, use_microphone=False)
    os.remove(temp_path)

    # Return top-1 for now
    return jsonify({
        "predictions": [{
            "sound": pred_class,
            "probability": float(confidence)
        }]
    })

@ml_bp.route('/start_listening', methods=['POST'])
def start_listening():
    if 'username' not in session:
        return jsonify({'status': 'error', 'message': 'Please log in first'}), 401

    # Reset stats
    current_app.inference_stats = {
        'total_predictions': 0,
        'class_counts': {},
        'confidence_levels': [],
        'confusion_matrix': {},
        'misclassifications': [],
        'correct_classifications': []
    }

    global sound_detector
    try:
        # 1) figure out which model
        model_choice = request.args.get('model', 'cnn')  # "cnn", "rf", or "ensemble"
        
        active_dict = Config.get_dictionary()
        class_names = active_dict.get('sounds', [])
        dict_name = active_dict.get('name', 'Default')
        if not class_names:
            return jsonify({'status': 'error', 'message': 'No sounds in dictionary'})

        if model_choice == 'rf':
            # Load the RF model
            rf_path = os.path.join('models', f"{dict_name.replace(' ','_')}_rf.joblib")
            if not os.path.exists(rf_path):
                return jsonify({'status': 'error', 'message': 'No RF model available for dictionary.'}), 400

            rf_classifier = RandomForestClassifier(model_dir='models')
            rf_classifier.load(filename=os.path.basename(rf_path))

            sound_detector = SoundDetectorRF(rf_classifier)
            # Start listening with the same callback
            sound_detector.start_listening(callback=prediction_callback)

            return jsonify({'status': 'success', 'message': 'Real-time RF started'})

        elif model_choice == 'ensemble':

            
            # Load the CNN model
            cnn_path = os.path.join('models', f"{dict_name.replace(' ','_')}_model.h5")
            if not os.path.exists(cnn_path):
                return jsonify({'status': 'error', 'message': 'No CNN model found for this dictionary'}), 400
            cnn_model = load_model(cnn_path)

            # Load the RF model
            rf_path = os.path.join('models', f"{dict_name.replace(' ','_')}_rf.joblib")
            if not os.path.exists(rf_path):
                return jsonify({'status': 'error', 'message': 'No RF model found for this dictionary'}), 400

            rf_classifier = RandomForestClassifier(model_dir='models')
            rf_classifier.load(filename=os.path.basename(rf_path))

            # Create an EnsembleClassifier object
            ensemble_model = EnsembleClassifier(
                rf_classifier=rf_classifier,
                cnn_model=cnn_model,
                class_names=class_names,
                rf_weight=0.5
            )

            # Create the SoundDetectorEnsemble

            sound_detector = SoundDetectorEnsemble(ensemble_model)

            # Start listening, same callback as your CNN/RF routes
            success = sound_detector.start_listening(callback=prediction_callback)
            if not success:
                return jsonify({'status': 'error', 'message': 'Failed to start Ensemble detection'}), 500

            return jsonify({'status': 'success', 'message': 'Real-time Ensemble started'})
        else:
            # default to CNN
            model_path = os.path.join('models', f"{dict_name.replace(' ','_')}_model.h5")
            if not os.path.exists(model_path):
                return jsonify({'status': 'error', 'message': f"No CNN model for {dict_name}"}), 400

            with tf.keras.utils.custom_object_scope({'BatchShape': lambda x: None}):
                model = tf.keras.models.load_model(model_path)

            sound_detector = SoundDetector(model, class_names)
            sound_detector.start_listening(callback=prediction_callback)

            return jsonify({'status': 'success'})

    except Exception as e:
        logging.error(f"Error in start_listening: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': str(e)})

@ml_bp.route('/stop_listening', methods=['POST'])
def stop_listening():
    if 'username' not in session:
        return jsonify({'status': 'error', 'message': 'Please log in first'}), 401

    global sound_detector
    try:
        if sound_detector:
            result = sound_detector.stop_listening()
            sound_detector = None
            return jsonify(result)
        else:
            return jsonify({"status":"error","message":"No active listener"})
    except Exception as e:
        logging.error(f"Error stopping listener: {e}", exc_info=True)
        return jsonify({"status":"error","message":str(e)})

@ml_bp.route('/prediction_stream')
def prediction_stream():
    """
    Server-sent events endpoint that streams new predictions/logs to the client.
    """
    if 'username' not in session:
        return jsonify({'status': 'error', 'message': 'Please log in first'}), 401

    def stream_predictions():
        last_index = 0
        while True:
            data = {}
            global LATEST_PREDICTION
            if LATEST_PREDICTION:
                data['prediction'] = LATEST_PREDICTION  
                LATEST_PREDICTION = None

            if len(debug_log_handler.logs) > last_index:
                data['log'] = debug_log_handler.logs[last_index]
                last_index = len(debug_log_handler.logs)

            if data:
                yield f"data: {json.dumps(data)}\n\n"
            else:
                yield ": heartbeat\n\n"
            time.sleep(0.1)

    return Response(stream_predictions(), mimetype='text/event-stream')

@ml_bp.route('/inference_statistics')
def inference_statistics():
    stats = current_app.inference_stats
    if not stats['confidence_levels']:
        avg_conf = 0.0
    else:
        avg_conf = sum(stats['confidence_levels']) / len(stats['confidence_levels'])

    # Class accuracy
    class_accuracy = {}
    cm = stats.get('confusion_matrix', {})
    for actual_sound in cm:
        total = sum(cm[actual_sound].values())
        correct = cm[actual_sound].get(actual_sound, 0)
        if total > 0:
            class_accuracy[actual_sound] = {
                'accuracy': correct / total,
                'total_samples': total,
                'correct_samples': correct
            }
        else:
            class_accuracy[actual_sound] = {
                'accuracy': 0.0,
                'total_samples': 0,
                'correct_samples': 0
            }

    misclass_patterns = []
    for a_sound in cm:
        for p_sound, count in cm[a_sound].items():
            if a_sound != p_sound and count>0:
                misclass_patterns.append({
                    'actual': a_sound,
                    'predicted': p_sound,
                    'count': count
                })
    misclass_patterns.sort(key=lambda x: x['count'], reverse=True)

    return jsonify({
        'total_predictions': stats.get('total_predictions',0),
        'average_confidence': avg_conf,
        'class_counts': stats.get('class_counts',{}),
        'class_accuracy': class_accuracy,
        'confusion_matrix': cm,
        'misclassification_patterns': misclass_patterns,
        'recent_misclassifications': stats.get('misclassifications', [])[-10:],
        'recent_correct_classifications': stats.get('correct_classifications', [])[-10:]
    })

@ml_bp.route('/record_feedback', methods=['POST'])
def record_feedback():
    if 'username' not in session:
        return jsonify({'status':'error','message':'Please log in first'}), 401

    data = request.get_json()
    predicted_sound = data.get('predicted_sound')
    actual_sound = data.get('actual_sound')
    confidence = data.get('confidence')

    if not all([predicted_sound, actual_sound, confidence is not None]):
        return jsonify({'status':'error','message':'Missing data'}), 400

    # Fire the callback with "actual" so the confusion matrix updates
    cb_pred = {
        'class': predicted_sound,
        'confidence': confidence,
        'actual_sound': actual_sound
    }
    prediction_callback(cb_pred)
    return jsonify({'status':'success'})

@ml_bp.route('/save_analysis', methods=['POST'])
def save_analysis():
    if 'username' not in session:
        return jsonify({'status':'error','message':'Please log in first'}), 401

    stats = current_app.inference_stats
    dict_name = Config.get_dictionary().get('name','unknown')
    analysis_data = {
        'timestamp': datetime.now().isoformat(),
        'dictionary': dict_name,
        'confusion_matrix': stats.get('confusion_matrix',{}),
        'misclassifications': stats.get('misclassifications',[]),
        'correct_classifications': stats.get('correct_classifications',[]),
        'total_predictions': (len(stats.get('misclassifications',[])) +
                              len(stats.get('correct_classifications',[]))),
        'confidence_levels': stats.get('confidence_levels',[]),
        'class_counts': stats.get('class_counts',{})
    }

    analysis_dir = os.path.join(Config.CONFIG_DIR, 'analysis')
    os.makedirs(analysis_dir, exist_ok=True)
    filename = f"analysis_{dict_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    filepath = os.path.join(analysis_dir, filename)
    with open(filepath, 'w') as f:
        json.dump(analysis_data, f, indent=2)

    return jsonify({'status':'success','message':'Analysis data saved'})

@ml_bp.route('/view_analysis')
def view_analysis():
    if 'username' not in session:
        return redirect(url_for('login'))

    analysis_dir = os.path.join(Config.CONFIG_DIR, 'analysis')
    if not os.path.exists(analysis_dir):
        return render_template('view_analysis.html', analysis_files=[])

    analysis_files = []
    for fname in os.listdir(analysis_dir):
        if fname.endswith('.json'):
            path = os.path.join(analysis_dir, fname)
            with open(path,'r') as f:
                data = json.load(f)
            analysis_files.append({
                'filename': fname,
                'timestamp': data.get('timestamp',''),
                'dictionary': data.get('dictionary',''),
                'total_predictions': data.get('total_predictions',0)
            })
    analysis_files.sort(key=lambda x: x['timestamp'], reverse=True)
    return render_template('view_analysis.html', analysis_files=analysis_files)

@ml_bp.route('/get_analysis/<filename>')
def get_analysis(filename):
    if 'username' not in session:
        return jsonify({'status':'error','message':'Please log in first'}), 401

    analysis_dir = os.path.join(Config.CONFIG_DIR, 'analysis')
    filepath = os.path.join(analysis_dir, filename)

    # Safety check
    if not os.path.abspath(filepath).startswith(os.path.abspath(analysis_dir)):
        return jsonify({'status':'error','message':'Invalid filename'}),400

    if not os.path.exists(filepath):
        return jsonify({'status':'error','message':'File not found'}),404

    try:
        with open(filepath,'r') as f:
            data = json.load(f)
        return jsonify(data)
    except Exception as e:
        return jsonify({'status':'error','message':str(e)}),500

@ml_bp.route('/training_status')
def training_status():
    # So your front-end can poll. If you prefer a purely JS solution, okay.
    if hasattr(current_app, 'training_progress'):
        return jsonify({
            'progress': current_app.training_progress,
            'status': current_app.training_status
        })
    return jsonify({'progress':0,'status':'Not started'})

@ml_bp.route('/record', methods=['POST'])
def record():
    if 'username' not in session:
        return redirect(url_for('index'))

    sound = request.form.get('sound')
    audio_data = request.files.get('audio')
    current_app.logger.debug(f"Recording attempt. sound={sound}, has audio={audio_data is not None}")
    if sound and audio_data:
        try:
            # Convert webm->wav with pydub
            audio = AudioSegment.from_file(audio_data, format="webm")
            wav_io = io.BytesIO()
            audio.export(wav_io, format="wav")
            wav_io.seek(0)
        except Exception as e:
            current_app.logger.error(f"Error converting audio: {e}", exc_info=True)
            return "Error converting audio", 500

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_filename = f"{sound}_{session['username']}_{timestamp}.wav"
            temp_path = os.path.join(Config.TEMP_DIR, temp_filename)
            with open(temp_path, 'wb') as f:
                f.write(wav_io.read())

            # Chop into chunks
            processor = Audio_Chunker()
            chopped_files = processor.chop_recording(temp_path)
            os.remove(temp_path)  # remove original big recording

            if not chopped_files:
                current_app.logger.error("No valid sound chunks found")
                flash("No valid sound chunks found in your recording.")
                return redirect(url_for('index'))

            return redirect(url_for('ml.verify_chunks', timestamp=timestamp))
        except Exception as e:
            current_app.logger.error(f"Error processing recording: {e}", exc_info=True)
            return "Error processing recording", 500

    flash("Sound or audio data missing.")
    return redirect(url_for('index'))

@ml_bp.route('/verify/<timestamp>')
def verify_chunks(timestamp):
    if 'username' not in session:
        return redirect(url_for('index'))

    chunks = [f for f in os.listdir(Config.TEMP_DIR) if timestamp in f]
    if not chunks:
        flash('No chunks found to verify or all processed.')
        return redirect(url_for('index'))
    return render_template('verify.html', chunks=chunks)

@ml_bp.route('/process_verification', methods=['POST'])
def process_verification():
    if 'username' not in session:
        return redirect(url_for('index'))

    chunk_file = request.form.get('chunk_file')
    is_good = request.form.get('is_good') == 'true'
    if not chunk_file:
        return redirect(url_for('index'))

    parts = chunk_file.split('_')
    if len(parts) < 3:
        flash("File name does not match expected pattern.")
        return redirect(url_for('index'))
    timestamp = parts[-2]

    if is_good:
        # Move chunk to goodsounds
        sound = parts[0]
        username = session['username']
        sound_dir = os.path.join(Config.GOOD_SOUNDS_DIR, sound)
        os.makedirs(sound_dir, exist_ok=True)

        # Count how many are in that folder for that user
        existing_count = len([
            f for f in os.listdir(sound_dir)
            if f.startswith(f"{sound}_{username}_")
        ])
        new_filename = f"{sound}_{username}_{existing_count + 1}.wav"
        new_path = os.path.join(sound_dir, new_filename)
        os.rename(os.path.join(Config.TEMP_DIR, chunk_file), new_path)
        flash(f"Chunk saved as {new_filename}")
    else:
        # Delete
        os.remove(os.path.join(Config.TEMP_DIR, chunk_file))
        flash("Chunk deleted.")

    return redirect(url_for('ml.verify_chunks', timestamp=timestamp))

@ml_bp.route('/manage_dictionaries')
def manage_dictionaries():
    if not session.get('is_admin'):
        return redirect(url_for('index'))

    dictionaries = Config.get_dictionaries()
    active_dictionary = Config.get_dictionary()

    # Build stats
    sound_stats = {}
    if active_dictionary and 'sounds' in active_dictionary:
        for sound in active_dictionary['sounds']:
            sound_stats[sound] = {
                'system_total': 0,
                'user_total': 0
            }
            # Count system recordings
            sound_dir = os.path.join(Config.GOOD_SOUNDS_DIR, sound)
            if os.path.exists(sound_dir):
                system_files = [
                    f for f in os.listdir(sound_dir)
                    if f.endswith('.wav') or f.endswith('.mp3')
                ]
                sound_stats[sound]['system_total'] = len(system_files)
            # user_total is not heavily used; left for completeness

    return render_template('manage_dictionaries.html',
                           dictionaries=dictionaries,
                           active_dictionary=active_dictionary,
                           sound_stats=sound_stats)

@ml_bp.route('/save_dictionary', methods=['POST'])
def save_dictionary():
    if not session.get('is_admin'):
        return redirect(url_for('index'))

    name = request.form.get('name')
    sounds_str = request.form.get('sounds')
    if not name or not sounds_str:
        flash("Please provide dictionary name and sounds")
        return redirect(url_for('ml.manage_dictionaries'))

    sounds = [s.strip() for s in sounds_str.split(',') if s.strip()]
    new_dict = {"name": name, "sounds": sounds}
    dictionaries = Config.get_dictionaries()

    found = False
    for d in dictionaries:
        if d['name'] == name:
            d['sounds'] = sounds
            found = True
            break
    if not found:
        dictionaries.append(new_dict)

    Config.save_dictionaries(dictionaries)
    Config.set_active_dictionary(new_dict)
    flash("Dictionary saved and activated.")
    return redirect(url_for('ml.manage_dictionaries'))

@ml_bp.route('/make_active', methods=['POST'])
def make_active():
    if not session.get('is_admin'):
        return redirect(url_for('index'))

    name = request.form.get('name')
    if not name:
        flash('Dictionary name is required')
        return redirect(url_for('ml.manage_dictionaries'))

    dictionaries = Config.get_dictionaries()
    selected_dict = None
    for d in dictionaries:
        if d['name'] == name:
            selected_dict = d
            break
    if not selected_dict:
        flash('Dictionary not found')
        return redirect(url_for('ml.manage_dictionaries'))

    Config.set_active_dictionary(selected_dict)
    # Create directories for each sound
    for sound in selected_dict['sounds']:
        os.makedirs(os.path.join(Config.GOOD_SOUNDS_DIR, sound), exist_ok=True)

    flash(f'Dictionary "{name}" is now active')
    return redirect(url_for('ml.manage_dictionaries'))

@ml_bp.route('/set_active_dictionary', methods=['POST'])
def set_active_dictionary():
    if not session.get('is_admin'):
        return redirect(url_for('index'))

    name = request.form.get('name')
    if name:
        dictionaries = Config.get_dictionaries()
        for d in dictionaries:
            if d['name'] == name:
                Config.set_active_dictionary(d)
                flash(f'Activated dictionary: {name}')
                break
    return redirect(url_for('ml.manage_dictionaries'))

@ml_bp.route('/ml.list_recordings')
def list_recordings():
    if 'username' not in session:
        return redirect(url_for('index'))

    if session.get('is_admin'):
        # Admin sees everything
        all_sounds = os.listdir(Config.GOOD_SOUNDS_DIR)
        # But each sound is a folder, so let's gather from subfolders
        # Or maybe you're storing files directly in goodsounds?
        # We'll just list from subfolders:
        recordings_by_sound = {}
        active_dict = Config.get_dictionary()
        if not active_dict:
            flash("No active dictionary.")
            return redirect(url_for('index'))

        for sound in active_dict['sounds']:
            sound_dir = os.path.join(Config.GOOD_SOUNDS_DIR, sound)
            if os.path.exists(sound_dir):
                files = [f for f in os.listdir(sound_dir) if f.endswith('.wav')]
                if files:
                    recordings_by_sound[sound] = files
        return render_template('list_recordings.html', recordings=recordings_by_sound)
    else:
        # Show only user's recordings
        user = session['username']
        recordings_by_sound = {}
        active_dict = Config.get_dictionary()
        if not active_dict:
            flash("No active dictionary.")
            return redirect(url_for('index'))

        for sound in active_dict['sounds']:
            sound_dir = os.path.join(Config.GOOD_SOUNDS_DIR, sound)
            if not os.path.exists(sound_dir):
                continue
            sound_files = [f for f in os.listdir(sound_dir) if f.endswith('.wav')]
            # Filter for user
            user_files = [f for f in sound_files if f.split('_')[1] == user]
            if user_files:
                recordings_by_sound[sound] = user_files

        return render_template('list_recordings.html', recordings=recordings_by_sound)

@ml_bp.route('/ml.get_sound_stats')
def get_sound_stats():
    """Return JSON stats for each sound in the active dictionary."""
    if not session.get('is_admin'):
        return redirect(url_for('index'))

    active_dict = Config.get_dictionary()
    if not active_dict:
        return jsonify({})

    sound_stats = {}
    for sound in active_dict['sounds']:
        sound_stats[sound] = {
            'system_total': 0,
            'user_total': 0
        }
        sound_dir = os.path.join(Config.GOOD_SOUNDS_DIR, sound)
        if os.path.exists(sound_dir):
            files = [f for f in os.listdir(sound_dir) if f.endswith('.wav')]
            sound_stats[sound]['system_total'] = len(files)
            # user_total is optional
    return json.dumps(sound_stats)

@ml_bp.route('/ml.upload_sounds')
def upload_sounds():
    if 'username' not in session:
        return redirect(url_for('index'))
    return render_template('upload_sounds.html',
                           sounds=Config.get_dictionary()['sounds'])

@ml_bp.route('/ml.process_uploads', methods=['POST'])
def process_uploads():
    if 'username' not in session:
        return redirect(url_for('index'))

    sound = request.form.get('sound')
    files = request.files.getlist('files')
    if not sound or not files:
        flash("Please select both a sound and files.")
        return redirect(url_for('ml.upload_sounds'))


    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    processor = SoundProcessor()
    all_chunks = []

    for file in files:
        if file.filename.lower().endswith('.wav'):
            temp_path = os.path.join(Config.TEMP_DIR,
                                     f"{sound}_{session['username']}_{timestamp}_temp.wav")
            file.save(temp_path)
            chunks = processor.chop_recording(temp_path)
            all_chunks.extend(chunks)
            os.remove(temp_path)

    if not all_chunks:
        flash("No valid sound chunks found in uploads.")
        return redirect(url_for('ml.upload_sounds'))

    return redirect(url_for('ml.verify_chunks', timestamp=timestamp))


********************************************************************************

### File: src/routes/train_app.py ###

from flask import Flask, request
import logging

# Create a separate Flask instance for training
train_app = Flask(__name__)

@train_app.route('/train_model', methods=['POST'])
def train_model():
    """
    Example route that handles model training.
    """
    logging.info("Starting CNN training in separate train_app (no debug reloader).")
    # Training logic goes here, for example:
    # model.fit(...)
    logging.info("Training completed.")
    return "Training done!"

# If run directly, launch on port 5002, no debug, no reloader
if __name__ == "__main__":
    train_app.run(port=5002, debug=False, use_reloader=False)

********************************************************************************

### File: src/static/css/style.css ###

/* Global styles */

<link rel="stylesheet" href="{{ url_for('', filename='css/style.css') }}">

:root {
    --primary-color: #4A90E2;    /* Professional blue */
    --secondary-color: #34C759;  /* Success green */
    --accent-color: #5856D6;     /* Purple accent */
    --text-color: #2C3E50;       /* Dark blue-gray */
    --background-light: #f8f9fa;
    --shadow: 0 4px 6px rgba(0,0,0,0.1);
}

body {
    font-family: 'Poppins', sans-serif;
    background-color: var(--background-light);
    color: var(--text-color);
    line-height: 1.6;
}

/* Branding */
.navbar-brand {
    font-weight: 700;
    font-size: 1.5rem;
    color: var(--primary-color) !important;
}

.navbar-brand:after {
    content: "SoundsEasy.ai";
    font-size: 0.8em;
    opacity: 0.8;
    margin-left: 0.5rem;
}

/* Login styles */
.login-container {
    max-width: 800px;
    margin: 2rem auto;
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 2rem;
}

.admin-login, .user-login {
    background: white;
    padding: 2.5rem;
    border-radius: 12px;
    box-shadow: var(--shadow);
    transition: transform 0.2s ease;
}

.admin-login:hover, .user-login:hover {
    transform: translateY(-2px);
}

/* Recording styles */
.record-container {
    max-width: 600px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Verification styles */
.verify-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Dictionary management styles */
.dictionary-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Enhanced Button Styles */
.btn {
    padding: 0.5rem 1.5rem;
    border-radius: 8px;
    transition: all 0.2s ease;
}

.btn-primary {
    background-color: var(--primary-color);
    border-color: var(--primary-color);
}

.btn-success {
    background-color: var(--secondary-color);
    border-color: var(--secondary-color);
}

/* Enhanced Form Styles */
.form-control {
    border-radius: 8px;
    border: 1px solid #e0e0e0;
    padding: 0.75rem 1rem;
}

.form-control:focus {
    border-color: var(--primary-color);
    box-shadow: 0 0 0 0.2rem rgba(74, 144, 226, 0.25);
}

/* Alert Enhancements */
.alert {
    border-radius: 8px;
    border: none;
    box-shadow: var(--shadow);
}

/* Card Enhancements */
.card {
    border-radius: 12px;
    border: none;
    box-shadow: var(--shadow);
} 
********************************************************************************

### File: src/templates/404.html ###

<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>404 - Page Not Found</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
</head>
<body class="container mt-5">
    <h1 class="display-4">404 - Page Not Found</h1>
    <p>Sorry, the page you requested was not found.</p>
    <a href="{{ url_for('index') }}" class="btn btn-primary">
        Return to Home
    </a>
</body>
</html>
********************************************************************************

### File: src/templates/base.html ###

<!DOCTYPE html>
<html>
<head>
    <title>Sound Classifier</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
    <style>
        .recording-controls { margin: 20px 0; }
        .dictionary-item { 
            border: 1px solid #ddd;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
        }
        .dictionary-item.active {
            border-color: #28a745;
            background-color: #f8fff8;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <a class="navbar-brand" href="{{ url_for('index') }}">Sound Classifier</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                {% if session.get('username') %}
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('index') }}">Record</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('ml.list_recordings') }}">My Recordings</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('ml.upload_sounds') }}">Upload Sounds</a>
                    </li>
                    {% if session.get('is_admin') %}
                        <li class="nav-item">
                            <a class="nav-link" href="{{ url_for('ml.manage_dictionaries') }}">Manage Dictionaries</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="{{ url_for('ml.train_model') }}">Train Model</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="{{ url_for('ml.predict') }}">Test Model</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="{{ url_for('ml.model_summary') }}">Model Summary</a>
                        </li>
                    {% endif %}
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('logout') }}">Logout ({{ session.username }})</a>
                    </li>
                {% else %}
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('register') }}">Register</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('login') }}">Login</a>
                    </li>
                {% endif %}
            </ul>
        </div>
    </nav>

    <div class="container">
        {% with messages = get_flashed_messages() %}
            {% if messages %}
                {% for message in messages %}
                    <div class="alert alert-info mt-3">{{ message }}</div>
                {% endfor %}
            {% endif %}
        {% endwith %}
        
        {% block content %}{% endblock %}
    </div>

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.min.js"></script>
    {% block scripts %}{% endblock %}
</body>
</html> 
********************************************************************************

### File: src/templates/css/style.css ###

/* Global styles */


:root {
    --primary-color: #4A90E2;    /* Professional blue */
    --secondary-color: #34C759;  /* Success green */
    --accent-color: #5856D6;     /* Purple accent */
    --text-color: #2C3E50;       /* Dark blue-gray */
    --background-light: #f8f9fa;
    --shadow: 0 4px 6px rgba(0,0,0,0.1);
}

body {
    font-family: 'Poppins', sans-serif;
    background-color: var(--background-light);
    color: var(--text-color);
    line-height: 1.6;
}

/* Branding */
.navbar-brand {
    font-weight: 700;
    font-size: 1.5rem;
    color: var(--primary-color) !important;
}

.navbar-brand:after {
    content: "SoundsEasy.ai";
    font-size: 0.8em;
    opacity: 0.8;
    margin-left: 0.5rem;
}

/* Login styles */
.login-container {
    max-width: 800px;
    margin: 2rem auto;
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 2rem;
}

.admin-login, .user-login {
    background: white;
    padding: 2.5rem;
    border-radius: 12px;
    box-shadow: var(--shadow);
    transition: transform 0.2s ease;
}

.admin-login:hover, .user-login:hover {
    transform: translateY(-2px);
}

/* Recording styles */
.record-container {
    max-width: 600px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Verification styles */
.verify-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Dictionary management styles */
.dictionary-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: white;
    border-radius: 12px;
    box-shadow: var(--shadow);
}

/* Enhanced Button Styles */
.btn {
    padding: 0.5rem 1.5rem;
    border-radius: 8px;
    transition: all 0.2s ease;
}

.btn-primary {
    background-color: var(--primary-color);
    border-color: var(--primary-color);
}

.btn-success {
    background-color: var(--secondary-color);
    border-color: var(--secondary-color);
}

/* Enhanced Form Styles */
.form-control {
    border-radius: 8px;
    border: 1px solid #e0e0e0;
    padding: 0.75rem 1rem;
}

.form-control:focus {
    border-color: var(--primary-color);
    box-shadow: 0 0 0 0.2rem rgba(74, 144, 226, 0.25);
}

/* Alert Enhancements */
.alert {
    border-radius: 8px;
    border: none;
    box-shadow: var(--shadow);
}

/* Card Enhancements */
.card {
    border-radius: 12px;
    border: none;
    box-shadow: var(--shadow);
} 
********************************************************************************

### File: src/templates/index.html ###

{% extends "base.html" %}

{% block content %}
<div class="container mt-5">
    <div class="row">
        <div class="col-md-8 offset-md-2 text-center">
            <h1>Welcome to Sound Classifier</h1>
            <div class="mt-4">
                <a href="{{ url_for('ml.train_model') }}" class="btn btn-primary btn-lg m-2">
                    <i class="fas fa-play"></i> Train Model
                </a>
                <a href="{{ url_for('ml.predict') }}" class="btn btn-success btn-lg m-2">
                    <i class="fas fa-microphone"></i> Make Prediction
                </a>
            </div>
            
            {% with messages = get_flashed_messages() %}
                {% if messages %}
                    {% for message in messages %}
                        <div class="alert alert-info mt-3">
                            {{ message }}
                        </div>
                    {% endfor %}
                {% endif %}
            {% endwith %}
        </div>
    </div>
</div>
{% endblock %} 
********************************************************************************

### File: src/templates/inference.html ###

{% extends "base.html" %}

{% block styles %}
<link rel="stylesheet" href="{{ url_for('static', filename='Frontend/css/style.css') }}">

<style>
    .predict-container {
        max-width: 1200px;
        margin: 2rem auto;
        padding: 2rem;
        background: white;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .recording-status {
        display: flex;
        align-items: center;
        gap: 1rem;
        margin-bottom: 1rem;
    }
    .status-indicator {
        width: 1rem;
        height: 1rem;
        border-radius: 50%;
        background: #dc3545;
    }
    .status-indicator.recording {
        background: #28a745;
        animation: pulse 1s infinite;
    }
    .predictions-list {
        background: #f8f9fa;
        border-radius: 4px;
        padding: 1rem;
        margin-bottom: 1rem;
        max-height: 300px;
        overflow-y: auto;
    }
    .prediction-item {
        display: flex;
        justify-content: space-between;
        padding: 0.5rem;
        margin-bottom: 0.5rem;
        background: white;
        border-radius: 4px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.05);
    }
    .prediction-item .probability {
        color: #666;
    }
    .feedback-form {
        background: #f8f9fa;
        border-radius: 4px;
        padding: 1rem;
        margin-top: 1rem;
    }
    .sound-buttons {
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        margin: 1rem 0;
    }
    .sound-button {
        min-width: 80px;
    }
    .sound-button.correct {
        background-color: #28a745;
        color: white;
    }
    .sound-button.incorrect {
        background-color: #dc3545;
        color: white;
    }
    .predicted-sound {
        font-weight: bold;
        font-size: 1.1em;
    }
    .statistics {
        background: #f8f9fa;
        border-radius: 4px;
        padding: 1rem;
    }
    #perSoundStats {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
        gap: 0.5rem;
        margin-top: 0.5rem;
    }
    .sound-stat {
        background: white;
        padding: 0.5rem;
        border-radius: 4px;
        text-align: center;
    }
    .debug-container {
        background: #f8f9fa;
        border-radius: 4px;
        padding: 1rem;
        height: 300px;
        overflow-y: auto;
    }
    .debug-container .log-entry {
        padding: 0.25rem 0;
        border-bottom: 1px solid #dee2e6;
    }
    @keyframes pulse {
        0% { transform: scale(0.95); opacity: 0.9; }
        70% { transform: scale(1.1); opacity: 0.8; }
        100% { transform: scale(0.95); opacity: 0.9; }
    }
    .sounds-list {
        margin: 1rem 0;
    }
    .sound-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        margin: 0.25rem;
        background: #e9ecef;
        border-radius: 1rem;
        font-size: 0.9em;
    }
    .confusion-matrix {
        background: white;
        border-radius: 4px;
        margin-top: 1rem;
    }
    .confusion-matrix th, .confusion-matrix td {
        text-align: center;
        font-size: 0.9em;
        padding: 0.3rem !important;
    }
    .confusion-matrix td.correct {
        background-color: #d4edda;
    }
    .confusion-matrix td.error {
        background-color: #f8d7da;
    }
    .patterns-list,
    .recent-errors-list {
        max-height: 200px;
        overflow-y: auto;
    }
    .pattern-item,
    .error-item {
        background: white;
        padding: 0.5rem;
        margin: 0.25rem 0;
        border-radius: 4px;
    }
    .error-item .timestamp {
        font-size: 0.8em;
        color: #666;
    }

    .toggle-bar {
        display: flex;
        align-items: center;
        gap: 1rem;
        margin-bottom: 1rem;
    }

    /* Error Logs styling */
    #errorLogsContainer {
        background: #fff3cd;
        border: 1px solid #ffeeba;
        border-radius: 4px;
        padding: 1rem;
        margin-top: 1rem;
        max-height: 200px;
        overflow-y: auto;
    }
    #errorLogsContainer h4 {
        margin-top: 0;
    }
    .error-log-entry {
        background: #fff;
        border: 1px solid #ffeeba;
        border-radius: 4px;
        margin-bottom: 0.5rem;
        padding: 0.5rem;
    }
    .model-section {
        border: 1px solid #ccc;
        margin: 15px 0;
        padding: 10px;
        border-radius: 5px;
    }
    .model-section h2 {
        margin-top: 0;
    }
    #results-container {
        margin-top: 20px;
        padding: 10px;
        border: 1px dashed #666;
    }
    #logs {
        white-space: pre-wrap;
        background: #f7f7f7;
        padding: 1em;
        margin-top: 10px;
        height: 200px;
        overflow-y: scroll;
        border: 1px solid #ccc;
    }
</style>
{% endblock styles %}

{% block content %}
<div class="container mt-5">

    <!-- ================== MODEL & FEEDBACK OPTIONS ================== -->
    <div class="toggle-bar">
        <label for="sseModelSelect"><strong>SSE Model:</strong></label>
        <select id="sseModelSelect" class="form-control" style="width: 150px;">
            <option value="cnn" selected>CNN</option>
            <option value="rf">RF</option>
            <option value="ensemble">Ensemble</option>
        </select>
    </div>

    <!-- ================== SSE-BASED "PREDICT" SECTION ================== -->
    <div class="model-section" id="cnn-section">
        <h2>CNN Inference (Streamed via SSE)</h2>
        <div class="predict-container mb-5">
            <h2><i class="fas fa-microphone"></i> SSE Real-Time Prediction</h2>

            <!-- Dictionary display -->
            <div class="current-dictionary">
                <h4>Current Dictionary: {{ active_dict.name }}</h4>
                <div class="sounds-list">
                    {% for sound in active_dict.sounds %}
                    <span class="sound-badge" style="font-size: 2rem; color: red; font-weight: bold;">
                        {{ sound }}{% if not loop.last %},&nbsp;&nbsp;&nbsp;{% endif %}
                    </span>
                    {% endfor %}
                </div>
            </div>

            <div class="prediction-box">
                <div class="recording-status">
                    <div class="status-indicator" id="sseStatusIndicator"></div>
                    <span id="statusTextSSE">Click to start recording (SSE mode)</span>
                </div>

                <button id="listenButton" class="btn btn-primary btn-lg">
                    <i class="fas fa-microphone"></i> Start Listening
                </button>

                <div class="row mt-4">
                    <div class="col-md-6">
                        <!-- SSE predictions list -->
                        <div id="predictions" class="predictions-list">
                            <h4>Predictions</h4>
                            <div id="predictionsList" style="font-size: 2rem;color: red;">
                            </div>
                        </div>

                        <!-- Single Feedback Form (SSE approach) -->
                        <div id="feedbackForm" class="feedback-form d-none">
                            <h4>Was this prediction correct?</h4>
                            <p>Predicted: <span id="predictedSound" class="predicted-sound"></span></p>
                            <p>What sound did you actually make?</p>
                            <div id="soundButtons" class="sound-buttons">
                                {% for sound in active_dict.sounds %}
                                <button class="btn btn-outline-primary sound-button" data-sound="{{ sound }}">
                                    {{ sound }}
                                </button>
                                {% endfor %}
                            </div>
                            <div class="mt-3">
                                <button id="skipFeedback" class="btn btn-secondary">Skip</button>
                            </div>
                        </div>

                        <!-- Stats/Confusion Matrix -->
                        <div id="statistics" class="statistics mt-4">
                            <h4>Accuracy Statistics</h4>
                            <div id="overallAccuracy">
                                Overall Accuracy: <span>0%</span>
                            </div>
                            <div id="perSoundAccuracy">
                                <h5>Per Sound Accuracy:</h5>
                                <div id="perSoundStats"></div>
                            </div>
                            <div id="confusionMatrix" class="mt-4">
                                <h5>Confusion Matrix</h5>
                                <div class="table-responsive">
                                    <table class="table table-sm confusion-matrix">
                                        <thead>
                                            <tr>
                                                <th>Actual ↓ / Predicted →</th>
                                            </tr>
                                        </thead>
                                        <tbody></tbody>
                                    </table>
                                </div>
                            </div>
                            <div id="misclassificationPatterns" class="mt-4">
                                <h5>Common Misclassifications</h5>
                                <div class="patterns-list"></div>
                            </div>
                            <div id="recentErrors" class="mt-4">
                                <h5>Recent Misclassifications</h5>
                                <div class="recent-errors-list"></div>
                            </div>
                        </div>

                        <div class="mt-4">
                            <button id="saveAnalysisBtn" class="btn btn-outline-primary">Save Analysis Data</button>
                            <a href="{{ url_for('ml.view_analysis') }}" class="btn btn-outline-secondary">View Analysis History</a>
                        </div>
                    </div>

                    <div class="col-md-6">
                        <div class="debug-container">
                            <h4>Debug Logs</h4>
                            <div id="debugLogs"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- ================== CHUNK-BASED RECORDING SECTION ================== -->
    <div class="toggle-bar">

        <label for="chunkModelSelect"><strong>Chunk Model:</strong></label>
        <select id="chunkModelSelect" class="form-control" style="width: 150px;">
            <option value="cnn">CNN</option>
            <option value="rf" selected>RF</option>
            <option value="ensemble">Ensemble</option>
        </select>

        <label for="feedbackModeSelect"><strong>Feedback Mode (Chunk):</strong></label>
        <select id="feedbackModeSelect" class="form-control" style="width: 150px;">
            <option value="dictionary" selected>Dictionary UI</option>
            <option value="prompt">Prompt</option>
        </select>
    </div>

    <div class="model-section" id="rf-section">
        <h2 id="chunkedHeading">Chunked Inference</h2>
        <div class="predict-container">
            <h2>Chunk-Based Overlapping Recording</h2>
            <p class="mb-3">This mode records overlapping short chunks on the client side, then calls predict route each time.</p>

            <div class="recording-status">
                <div class="status-indicator" id="chunkStatusIndicator"></div>
                <span id="chunkStatusText">Not recording (chunk mode)</span>
            </div>

            <button id="start-button" class="btn btn-primary">Start Overlapping Recording</button>
            <button id="stop-button" class="btn btn-danger" disabled>Stop</button>

            <div id="predictionsChunk" class="mt-4 predictions-list"></div>
            
            <!-- Optional debug logs for chunk-based approach -->
            <div class="debug-container mt-3">
                <h4>Chunk Debug Logs</h4>
                <div id="debugLogsChunk"></div>
            </div>
        </div>

        <!-- Chunk-based feedback form (dictionary-based) -->
        <div id="chunkFeedbackForm" class="feedback-form d-none mt-3">
            <h4>Chunk-Based Feedback</h4>
            <p>Predicted: <span id="chunkPredictedSound" class="predicted-sound"></span></p>
            <p>What sound did you actually make?</p>
            <div id="chunkSoundButtons" class="sound-buttons">
                {% for sound in active_dict.sounds %}
                <button class="btn btn-outline-primary chunk-sound-button" data-sound="{{ sound }}">
                    {{ sound }}
                </button>
                {% endfor %}
            </div>
            <div class="mt-3">
                <button id="skipChunkFeedback" class="btn btn-secondary">Skip</button>
            </div>
        </div>
    </div>

    <!-- ================== Error Logs Section ================== -->
    <div id="errorLogsContainer" class="mt-3 d-none">
        <h4>Client-Side Error Logs</h4>
        <div id="errorLogs"></div>
    </div>

    <!-- RESULTS AREA -->
    <div id="results-container">
        <h3>Prediction / Output:</h3>
        <p>When a model finishes inference, its results will be shown here or in the logs below.</p>
        <!-- 
           Optionally, add an element for final predictions. 
           Your SSE or chunked code can insert text here:
        -->
        <div id="finalOutput" style="margin-top: 10px; font-weight: bold;">
            <!-- e.g., "CNN predicted: X" or "RF predicted: Y" -->
        </div>
    </div>

    <!-- LOG AREA -->
    <div id="logs" aria-label="Logs console">
        <!-- SSE and chunked logs appear here -->
    </div>

</div>
{% endblock content %}

{% block scripts %}
<script>
/* =========================
   UTILITY: ERROR LOGGING
   ========================= */
function logError(msg, errorObj=null) {
    console.error("ERROR:", msg, errorObj || "");
    const container = document.getElementById('errorLogsContainer');
    const logs = document.getElementById('errorLogs');
    container.classList.remove('d-none');

    const div = document.createElement('div');
    div.className = 'error-log-entry';
    div.textContent = msg + (errorObj ? " | " + errorObj : "");
    logs.prepend(div);
}

/* 
   We'll also patch window.addEventListener('error') and 'unhandledrejection'
   so that any uncaught exceptions show up in error logs.
*/
window.addEventListener('error', (event)=>{
    logError(`Global error caught: ${event.message} @ ${event.filename}:${event.lineno}`, event.error);
});
window.addEventListener('unhandledrejection', (event)=>{
    logError(`Unhandled promise rejection: ${event.reason}`, null);
});


/* =========================
   GLOBAL UI SELECTORS
   ========================= */
const sseModelSelect = document.getElementById('sseModelSelect');
const chunkModelSelect = document.getElementById('chunkModelSelect');
const feedbackModeSelect = document.getElementById('feedbackModeSelect');


/* =========================
   1) SSE-based real-time
   ========================= */
let statistics = {
    total: 0,
    correct: 0,
    perSound: {}
};
let lastPredictionConfidence = 0;  // store confidence of last SSE prediction

const soundList = JSON.parse('{{ active_dict.sounds|tojson|safe }}');
soundList.forEach(sound => {
    statistics.perSound[sound] = {
        total: 0,
        correct: 0,
        predictions: {}
    };
});

function updateStatistics() {
    const overallAccuracy = statistics.total > 0
        ? (statistics.correct / statistics.total * 100).toFixed(1)
        : 0;
    document.querySelector('#overallAccuracy span').textContent = `${overallAccuracy}%`;

    const perSoundStats = document.getElementById('perSoundStats');
    if (!perSoundStats) return; // safety check
    perSoundStats.innerHTML = '';

    Object.entries(statistics.perSound).forEach(([sound, stats]) => {
        const accuracy = stats.total > 0
            ? (stats.correct / stats.total * 100).toFixed(1)
            : 0;
        const div = document.createElement('div');
        div.className = 'sound-stat';
        div.innerHTML = `<strong>${sound}</strong><br>
                         ${accuracy}% (${stats.correct}/${stats.total})`;
        perSoundStats.appendChild(div);
    });
}

function updateConfusionMatrix(infStats) {
    const matrix = infStats.confusion_matrix;
    if (!matrix) return;
    const sounds = Object.keys(statistics.perSound);
    const table = document.querySelector('.confusion-matrix');
    const thead = table.querySelector('thead');
    const tbody = table.querySelector('tbody');
    if (!thead || !tbody) return;

    thead.innerHTML = '<tr><th>Actual ↓ / Predicted →</th></tr>';
    tbody.innerHTML = '';

    const headerRow = thead.querySelector('tr');
    sounds.forEach(sound => {
        const th = document.createElement('th');
        th.textContent = sound;
        headerRow.appendChild(th);
    });

    sounds.forEach(actualSound => {
        const row = document.createElement('tr');
        const header = document.createElement('th');
        header.textContent = actualSound;
        row.appendChild(header);

        sounds.forEach(predictedSound => {
            const td = document.createElement('td');
            const count = matrix[actualSound]?.[predictedSound] || 0;
            td.textContent = count;

            if (count > 0) {
                if (actualSound === predictedSound) {
                    td.classList.add('correct');
                } else {
                    td.classList.add('error');
                }
            }
            row.appendChild(td);
        });
        tbody.appendChild(row);
    });
}

function updateMisclassificationPatterns(infStats) {
    const patterns = infStats.misclassification_patterns || [];
    const container = document.querySelector('.patterns-list');
    if (!container) return;
    container.innerHTML = '';

    patterns.forEach(pattern => {
        const div = document.createElement('div');
        div.className = 'pattern-item';
        div.innerHTML = `
            <span>"${pattern.actual}" misclassified as "${pattern.predicted}"</span>
            <span class="badge bg-secondary">${pattern.count} times</span>
        `;
        container.appendChild(div);
    });
}

function updateRecentErrors(infStats) {
    const errors = infStats.recent_misclassifications || [];
    const container = document.querySelector('.recent-errors-list');
    if (!container) return;
    container.innerHTML = '';

    errors.forEach(error => {
        const div = document.createElement('div');
        div.className = 'error-item';
        div.innerHTML = `
            <div>Predicted "${error.predicted}" but was "${error.actual}"</div>
            <div>Confidence: ${(error.confidence * 100).toFixed(1)}%</div>
            <div class="timestamp">${error.timestamp}</div>
        `;
        container.appendChild(div);
    });
}

updateStatistics(); // Initialize stats

const listenButton = document.getElementById('listenButton');
if (listenButton) {
    listenButton.addEventListener('click', function() {
        const button = this;
        const predictionsList = document.getElementById('predictionsList');
        const debugLogs = document.getElementById('debugLogs');
        const statusIndicator = document.getElementById('sseStatusIndicator');
        const statusText = document.getElementById('statusTextSSE');
        const feedbackForm = document.getElementById('feedbackForm');

        // We'll attach the selected model as a query param
        const selectedModel = sseModelSelect.value;
        const startListeningURL = "{{ url_for('ml.start_listening') }}" + "?model=" + encodeURIComponent(selectedModel);

        if (button.textContent.includes('Start')) {
            // Start SSE
            fetch(startListeningURL, { method: 'POST' })
                .then(r => r.json())
                .then(data => {
                    if (data.status === 'success') {
                        button.innerHTML = '<i class="fas fa-stop"></i> Stop Listening';
                        button.classList.replace('btn-primary', 'btn-danger');
                        statusIndicator.classList.add('recording');
                        statusText.textContent = 'Listening for sounds...';
                        feedbackForm.classList.add('d-none');

                        const eventSource = new EventSource("{{ url_for('ml.prediction_stream') }}");
                        eventSource.onmessage = function(event) {
                            if (event.data === 'heartbeat') return;
                            try {
                                const d = JSON.parse(event.data);

                                // Show predictions
                                if (d.prediction) {
                                    const item = document.createElement('div');
                                    item.className = 'prediction-item';
                                    item.innerHTML = `
                                        <span class="sound">${d.prediction.class}</span>
                                        <span class="probability">${(d.prediction.confidence*100).toFixed(1)}%</span>
                                    `;
                                    predictionsList.insertBefore(item, predictionsList.firstChild);

                                    lastPredictionConfidence = d.prediction.confidence;
                                    document.getElementById('predictedSound').textContent = d.prediction.class;
                                    feedbackForm.classList.remove('d-none');

                                    // limit displayed predictions
                                    while (predictionsList.children.length > 10) {
                                        predictionsList.removeChild(predictionsList.lastChild);
                                    }
                                }

                                // Show log lines
                                if (d.log) {
                                    const logItem = document.createElement('div');
                                    logItem.className = 'log-entry';
                                    logItem.textContent = d.log;
                                    debugLogs.insertBefore(logItem, debugLogs.firstChild);
                                    while (debugLogs.children.length > 50) {
                                        debugLogs.removeChild(debugLogs.lastChild);
                                    }
                                }
                            } catch (err) {
                                logError("Error parsing SSE message data", err);
                            }
                        };
                        eventSource.onerror = function(e) {
                            logError("SSE error event", e);
                        };
                        button.eventSource = eventSource;
                    } else {
                        alert(data.message || 'Error starting listener');
                        logError("start_listening responded with error", data.message);
                    }
                })
                .catch(e => {
                    logError("Error starting SSE listening fetch", e);
                });
        } else {
            // Stop SSE
            if (button.eventSource) {
                button.eventSource.close();
                delete button.eventSource;
            }

            fetch("{{ url_for('ml.stop_listening') }}", { method: 'POST' })
                .then(r=>r.json())
                .then(data => {
                    button.innerHTML = '<i class="fas fa-microphone"></i> Start Listening';
                    button.classList.replace('btn-danger', 'btn-primary');
                    statusIndicator.classList.remove('recording');
                    statusText.textContent = 'Click to start recording (SSE mode)';
                    feedbackForm.classList.add('d-none');

                    // final stats
                    fetch("{{ url_for('ml.inference_statistics') }}")
                        .then(r => r.json())
                        .then(infStats => {
                            const statsDiv = document.createElement('div');
                            statsDiv.className = 'prediction-item stats';
                            statsDiv.innerHTML = `
                                <div class="stats-content">
                                    <h4>Session Statistics</h4>
                                    <p>Total Predictions: ${infStats.total_predictions}</p>
                                    <p>Average Confidence: ${(infStats.average_confidence * 100).toFixed(1)}%</p>
                                    <p>Class Distribution: ${JSON.stringify(infStats.class_counts)}</p>
                                </div>
                            `;
                            predictionsList.insertBefore(statsDiv, predictionsList.firstChild);

                            updateConfusionMatrix(infStats);
                            updateMisclassificationPatterns(infStats);
                            updateRecentErrors(infStats);
                        })
                        .catch(e => {
                            logError("Error fetching final SSE stats", e);
                        });
                })
                .catch(e => {
                    logError("Error stopping SSE listening fetch", e);
                });
        }
    });
}

// SSE feedback buttons
const sseSoundButtons = document.getElementById('soundButtons');
if (sseSoundButtons) {
    sseSoundButtons.addEventListener('click', e => {
        if (!e.target.classList.contains('sound-button')) return;
        const actualSound = e.target.dataset.sound;
        const predictedSound = document.getElementById('predictedSound').textContent;
        const isCorrect = (actualSound === predictedSound);

        // send feedback to the server
        fetch("{{ url_for('ml.record_feedback') }}", {
            method: 'POST',
            headers: { 'Content-Type':'application/json' },
            body: JSON.stringify({
                predicted_sound: predictedSound,
                actual_sound: actualSound,
                confidence: lastPredictionConfidence
            })
        })
        .catch(err => {
            logError("Error recording SSE feedback", err);
        });

        // update local stats
        statistics.total++;
        if (isCorrect) statistics.correct++;
        statistics.perSound[actualSound].total++;
        if (isCorrect) statistics.perSound[actualSound].correct++;
        if (!statistics.perSound[actualSound].predictions[predictedSound]) {
            statistics.perSound[actualSound].predictions[predictedSound] = 0;
        }
        statistics.perSound[actualSound].predictions[predictedSound]++;

        updateStatistics();

        // flash correct/incorrect color
        e.target.classList.add(isCorrect ? 'correct' : 'incorrect');
        setTimeout(() => {
            e.target.classList.remove('correct','incorrect');
            document.getElementById('feedbackForm').classList.add('d-none');
        }, 1000);
    });
}

// skip button
const skipFeedbackBtn = document.getElementById('skipFeedback');
if (skipFeedbackBtn) {
    skipFeedbackBtn.addEventListener('click', () => {
        document.getElementById('feedbackForm').classList.add('d-none');
    });
}

// Save analysis data
const saveAnalysisBtn = document.getElementById('saveAnalysisBtn');
if (saveAnalysisBtn) {
    saveAnalysisBtn.addEventListener('click', () => {
        fetch("{{ url_for('ml.save_analysis') }}", {
            method:'POST',
            headers:{ 'Content-Type':'application/json'}
        })
        .then(r=>r.json())
        .then(data=>{
            if(data.status==='success'){
                alert('Analysis data saved successfully');
            } else {
                alert('Error saving analysis data: '+ data.message);
                logError("Error saving analysis data", data.message);
            }
        })
        .catch(e=>logError("Error in saveAnalysis fetch", e));
    });
}


/* =========================
   2) CHUNK-BASED Overlapping
   ========================= */
let recordingChunk = false;
let mediaRecorder = null;
let audioContext = null;
let chunkQueue = [];
let streamGlobal = null;
let lastChunkConfidence = 0; 
let lastChunkPredicted = null;

// UI
const startChunkBtn = document.getElementById('start-button');
const stopChunkBtn = document.getElementById('stop-button');
const chunkStatusIndicator = document.getElementById('chunkStatusIndicator');
const chunkStatusText = document.getElementById('chunkStatusText');

if (startChunkBtn && stopChunkBtn) {
    startChunkBtn.onclick = async function() {
        if (recordingChunk) return;
        recordingChunk = true;
        startChunkBtn.disabled = true;
        stopChunkBtn.disabled = false;

        chunkStatusIndicator.classList.add('recording');
        chunkStatusText.textContent = 'Recording (chunk mode)...';

        try {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            streamGlobal = await navigator.mediaDevices.getUserMedia({ audio: true });
            startOverlappingRecording(streamGlobal);
        } catch (err) {
            logError("Error starting chunk-based recording (getUserMedia)", err);
            recordingChunk = false;
            startChunkBtn.disabled = false;
            stopChunkBtn.disabled = true;
            chunkStatusIndicator.classList.remove('recording');
            chunkStatusText.textContent = 'Not recording (chunk mode)';
        }
    };

    stopChunkBtn.onclick = function() {
        if (!recordingChunk) return;
        recordingChunk = false;
        startChunkBtn.disabled = false;
        stopChunkBtn.disabled = true;

        chunkStatusIndicator.classList.remove('recording');
        chunkStatusText.textContent = 'Not recording (chunk mode)';

        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        if (audioContext) {
            audioContext.close();
        }
        stopChunkRecording();
    };
}

function startOverlappingRecording(stream) {
    const debugLogsChunk = document.getElementById('debugLogsChunk');
    mediaRecorder = new MediaRecorder(stream);

    mediaRecorder.ondataavailable = function(event) {
        if (event.data.size > 0) {
            chunkQueue.push(event.data);

            // keep only the last two chunks (overlap)
            if (chunkQueue.length > 2) {
                chunkQueue.shift();
            }
            // combine last two chunks
            const combinedChunks = new Blob(chunkQueue, { type: 'audio/webm' });
            processAudioChunk(combinedChunks);
        }
    };

    mediaRecorder.onstop = function() {
        if (debugLogsChunk) {
            const logItem = document.createElement('div');
            logItem.className = 'log-entry';
            logItem.textContent = 'MediaRecorder stopped (chunk-based).';
            debugLogsChunk.appendChild(logItem);
        }
    };

    // Record in short intervals to get overlapping chunks
    mediaRecorder.start(250); // chunk length in ms
    if (debugLogsChunk) {
        const logItem = document.createElement('div');
        logItem.className = 'log-entry';
        logItem.textContent = 'Chunk-based MediaRecorder started (250ms chunks).';
        debugLogsChunk.appendChild(logItem);
    }
}

function processAudioChunk(audioBlob) {
    // read from <select>
    const modelChoice = chunkModelSelect.value; // "cnn" or "rf" or "ensemble"

    let targetRoute;
    if (modelChoice === 'rf') {
        targetRoute = "{{ url_for('ml.predict_rf') }}"; 
    } else if (modelChoice === 'ensemble') {
        targetRoute = "{{ url_for('ml.predict_ensemble') }}"; 
    } else {
        // default = CNN
        targetRoute = "{{ url_for('ml.predict_sound_endpoint') }}";
    }

    const formData = new FormData();
    formData.append('audio', audioBlob);

    fetch(targetRoute, {
        method:'POST',
        body: formData
    })
    .then(r => r.json())
    .then(data => {
        if (data.error) {
            logError("Server error in chunk-based route", data.error);
        } else if (data.predictions){
            displayPredictionsChunk(data.predictions);
        }
    })
    .catch(e => {
        logError("Error in chunk-based fetch", e);
    });
}

function displayPredictionsChunk(predictions) {
    const container = document.getElementById('predictionsChunk');
    if (!container) return;
    
    // We'll just replace or prepend the new set of predictions
    // For demonstration, let's always prepend
    const groupDiv = document.createElement('div');
    groupDiv.className = 'prediction-item';
    groupDiv.style.flexDirection = 'column';

    predictions.forEach(pred => {
        const { sound, probability } = pred;
        const row = document.createElement('div');
        row.className = 'prediction-item';
        row.innerHTML = `
            <span>Predicted: <strong>${sound}</strong></span>
            <span class="probability">${(probability*100).toFixed(1)}%</span>
            <button class="btn btn-sm btn-info ml-2 feedback-btn" 
                    data-predicted="${sound}"
                    data-confidence="${probability}">
                Mark as correct/incorrect
            </button>
        `;
        groupDiv.appendChild(row);
    });

    container.prepend(groupDiv);

    // Attach click listeners for chunk-based feedback
    groupDiv.querySelectorAll('.feedback-btn').forEach(btn => {
       btn.addEventListener('click', e => {
         const predictedSound = e.target.dataset.predicted;
         const conf = parseFloat(e.target.dataset.confidence);

         const mode = feedbackModeSelect.value; // "dictionary" or "prompt"
         if (mode === 'prompt') {
             const actual = prompt(`We predicted "${predictedSound}". What was it really?`, predictedSound);
             if (!actual) return;
             recordFeedbackChunk(predictedSound, actual, conf);
         } else {
             lastChunkPredicted = predictedSound;
             lastChunkConfidence = conf;
             document.getElementById('chunkPredictedSound').textContent = predictedSound;
             document.getElementById('chunkFeedbackForm').classList.remove('d-none');
         }
       });
    });
}

// If dictionary-based feedback
const chunkSoundButtons = document.getElementById('chunkSoundButtons');
if (chunkSoundButtons) {
    chunkSoundButtons.addEventListener('click', e => {
        if (!e.target.classList.contains('chunk-sound-button')) return;
        const actualSound = e.target.dataset.sound;
        const predictedSound = lastChunkPredicted;
        const confidence = lastChunkConfidence;
        if (!predictedSound) return;

        recordFeedbackChunk(predictedSound, actualSound, confidence);

        // flash correct/incorrect
        const isCorrect = (actualSound === predictedSound);
        e.target.classList.add(isCorrect ? 'correct' : 'incorrect');
        setTimeout(() => {
            e.target.classList.remove('correct','incorrect');
            document.getElementById('chunkFeedbackForm').classList.add('d-none');
        }, 1000);
    });
}

const skipChunkFeedback = document.getElementById('skipChunkFeedback');
if (skipChunkFeedback) {
    skipChunkFeedback.addEventListener('click', () => {
        document.getElementById('chunkFeedbackForm').classList.add('d-none');
    });
}

// Actually record the chunk-based feedback to the server
function recordFeedbackChunk(predicted, actual, confidence) {
    fetch("{{ url_for('ml.record_feedback') }}", {
        method: 'POST',
        headers: { 'Content-Type':'application/json' },
        body: JSON.stringify({
            predicted_sound: predicted,
            actual_sound: actual,
            confidence: confidence
        })
    })
    .then(r => r.json())
    .then(resp => {
       if (resp.status === 'success') {
         console.log(`Chunk feedback recorded! Predicted=${predicted}, actual=${actual}`);
       } else {
         logError("Error recording chunk feedback", resp.message);
       }
    })
    .catch(err => {
        logError("Feedback error chunk-based", err);
    });
}

// Called on chunk-based stop
function stopChunkRecording(){
    const debugLogsChunk = document.getElementById('debugLogsChunk');
    if (debugLogsChunk) {
        const logItem = document.createElement('div');
        logItem.className = 'log-entry';
        logItem.textContent = 'Stopped chunk-based recording.';
        debugLogsChunk.appendChild(logItem);
    }
    console.log('Stopped chunk-based recording.');
}

const chunkSelect = document.getElementById('chunkModelSelect');
const chunkHeading = document.getElementById('chunkedHeading');
chunkSelect.addEventListener('change', () => {
    const val = chunkSelect.value;
    // Adjust heading:
    chunkHeading.textContent = val.toUpperCase() + " Inference (Chunked)";
});
</script>
{% endblock scripts %}

********************************************************************************

### File: src/templates/inference_statistics.html ###

{% extends "base.html" %}

{% block content %}
<div class="container mt-5">
    <h1>Inference Statistics</h1>
    <p><strong>Total Predictions Made:</strong> {{ inference_stats.total_predictions }}</p>
    <p><strong>Average Confidence Level:</strong> {{ avg_confidence | round(4) }}</p>

    <h2>Class Counts</h2>
    <table class="table">
        <thead>
            <tr>
                <th>Class Name</th>
                <th>Count</th>
            </tr>
        </thead>
        <tbody>
            {% for class_name, count in inference_stats.class_counts.items() %}
            <tr>
                <td>{{ class_name }}</td>
                <td>{{ count }}</td>
            </tr>
            {% endfor %}
        </tbody>
    </table>

    <a href="/predict" class="btn btn-primary">Back to Prediction</a>
</div>
{% endblock %} 
********************************************************************************

### File: src/templates/list_recordings.html ###

{% extends "base.html" %}

{% block content %}
<div class="recordings-container">
    <h2><i class="fas fa-list"></i> Your Recordings</h2>
    
    {% for sound, files in recordings.items() %}
    <div class="sound-group">
        <h3>{{ sound }}</h3>
        <div class="recordings-list">
            {% for file in files %}
            <div class="recording-item">
                <audio controls src="{{ url_for('static', filename='goodsounds/' + file) }}"></audio>
                <span class="filename">{{ file }}</span>
            </div>
            {% endfor %}
        </div>
    </div>
    {% endfor %}
</div>

<style>
.recordings-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2rem;
    background: white;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.sound-group {
    margin-bottom: 2rem;
}

.recordings-list {
    display: flex;
    flex-direction: column;
    gap: 1rem;
}

.recording-item {
    display: flex;
    align-items: center;
    gap: 1rem;
    padding: 1rem;
    background: #f8f9fa;
    border-radius: 8px;
}

.filename {
    font-family: monospace;
    color: #666;
}
</style>
{% endblock %} 
********************************************************************************

### File: src/templates/login.html ###

{% extends "base.html" %}

{% block content %}
<div class="login-container">
    <div class="admin-login">
        <h2><i class="fas fa-user-shield"></i> Admin Login</h2>
        <form method="POST" action="{{ url_for('login') }}" class="mt-3">
            <input type="hidden" name="type" value="admin">
            <div class="form-group mb-3">
                <input type="password" 
                       name="password" 
                       class="form-control" 
                       placeholder="Admin Password" 
                       required>
            </div>
            <button type="submit" class="btn btn-primary w-100">
                <i class="fas fa-sign-in-alt"></i> Login as Admin
            </button>
        </form>
    </div>
    
    <div class="user-login">
        <h2><i class="fas fa-user"></i> User Login</h2>
        <form method="POST" action="{{ url_for('login') }}" class="mt-3">
            <input type="hidden" name="type" value="user">
            <div class="form-group mb-3">
                <input type="text" 
                       name="username" 
                       class="form-control" 
                       placeholder="Enter Username" 
                       required>
            </div>
            <button type="submit" class="btn btn-success w-100">
                <i class="fas fa-sign-in-alt"></i> Login as User
            </button>
        </form>
        
        <div class="mt-3 text-center">
            <p>First time here?</p>
            <a href="{{ url_for('register') }}" class="btn btn-outline-primary">
                <i class="fas fa-user-plus"></i> Register New User
            </a>
        </div>
    </div>
</div>
{% endblock %} 
********************************************************************************

### File: src/templates/manage_dictionaries.html ###

{% extends "base.html" %}

{% block content %}
<div class="dictionary-layout">
    <div class="dictionary-main">
        <h2><i class="fas fa-book"></i> Manage Dictionaries</h2>
        
        <div class="available-dictionaries mb-4">
            <h3>Available Dictionaries</h3>
            <div class="dictionary-list">
                {% for dict in dictionaries %}
                <div class="dictionary-item {% if dict.name == active_dictionary.name %}active{% endif %}">
                    <div class="dictionary-info">
                        <h4>{{ dict.name }}</h4>
                        <small>{{ dict.sounds|length }} sounds</small>
                    </div>
                    <form method="POST" action="{{ url_for('ml.set_active_dictionary') }}" class="d-inline">
                        <input type="hidden" name="name" value="{{ dict.name }}">
                        <button type="submit" class="btn btn-sm {% if dict.name == active_dictionary.name %}btn-success{% else %}btn-outline-primary{% endif %}">
                            {% if dict.name == active_dictionary.name %}
                            <i class="fas fa-check"></i> Active
                            {% else %}
                            <i class="fas fa-check-circle"></i> Make Active
                            {% endif %}
                        </button>
                    </form>
                </div>
                {% endfor %}
            </div>
        </div>

        <div class="current-dictionary">
            <h3>Current Dictionary: {{ active_dictionary.name }}</h3>
            <div class="sounds-list">
                {% for sound in active_dictionary.sounds %}
                <span class="sound-badge">{{ sound }}</span>
                {% endfor %}
            </div>
        </div>

        <div class="edit-dictionary mt-4">
            <h3>Edit Dictionary</h3>
            <form method="POST" action="{{ url_for('ml.save_dictionary') }}">
                <div class="form-group mb-3">
                    <label for="name">Dictionary Name:</label>
                    <input type="text" 
                           id="name" 
                           name="name" 
                           class="form-control" 
                           value="{{ active_dictionary.name }}" 
                           required>
                </div>
                
                <div class="form-group mb-3">
                    <label for="sounds">Sounds (comma-separated):</label>
                    <input type="text" 
                           id="sounds" 
                           name="sounds" 
                           class="form-control" 
                           value="{{ active_dictionary.sounds|join(', ') }}" 
                           required>
                    <small class="form-text text-muted">
                        Example: ah, eh, ee, oh, oo
                    </small>
                </div>
                
                <button type="submit" class="btn btn-primary">
                    <i class="fas fa-save"></i> Save Dictionary
                </button>
            </form>
        </div>
    </div>

    <div class="stats-panel">
        <h4>
            <i class="fas fa-chart-bar"></i> Sound Statistics
            <small class="refresh-status" id="refreshStatus"></small>
        </h4>
        <div class="stats-table-container">
            <table class="stats-table" id="statsTable">
                <thead>
                    <tr>
                        <th>Sound</th>
                        <th>System</th>
                        <th>User</th>
                    </tr>
                </thead>
                <tbody>
                    {% for sound in active_dictionary.sounds %}
                    <tr>
                        <td>{{ sound }}</td>
                        <td>{{ sound_stats.get(sound, {}).get('system_total', 0) }}</td>
                        <td>{{ sound_stats.get(sound, {}).get('user_total', 0) }}</td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
        </div>
    </div>
</div>

<style>
.dictionary-layout {
    display: grid;
    grid-template-columns: 2fr 1fr;
    gap: 2rem;
    max-width: 1200px;
    margin: 0 auto;
}

.dictionary-main {
    background: white;
    padding: 2rem;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.stats-panel {
    background: white;
    padding: 1rem;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    height: fit-content;
}

.stats-table-container {
    margin-top: 1rem;
    font-size: 0.85rem;
}

.stats-table {
    width: 100%;
    border-collapse: collapse;
}

.stats-table th,
.stats-table td {
    padding: 0.25rem 0.5rem;
    text-align: center;
    border-bottom: 1px solid #dee2e6;
}

.stats-table th {
    background: #f8f9fa;
    font-weight: 600;
    font-size: 0.8rem;
    text-transform: uppercase;
}

.stats-table tr:hover {
    background: #f8f9fa;
}

.sound-badge {
    background: #e9ecef;
    padding: 0.25rem 0.75rem;
    border-radius: 20px;
    font-size: 0.9em;
    color: #495057;
    margin: 0.25rem;
    display: inline-block;
}

.refresh-status {
    font-size: 0.7rem;
    color: #666;
    margin-left: 0.5rem;
    font-weight: normal;
}

.dictionary-list {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
    margin-top: 1rem;
}

.dictionary-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.5rem 1rem;
    background: #f8f9fa;
    border: 1px solid #dee2e6;
    border-radius: 4px;
}

.dictionary-item.active {
    border-color: #28a745;
    background: #f0fff4;
}

.dictionary-info h4 {
    margin: 0;
    font-size: 1rem;
}

.dictionary-info small {
    color: #666;
}
</style>

{% block scripts %}
<script>
async function refreshStats() {
    try {
        const response = await fetch("{{ url_for('ml.get_sound_stats') }}");
        const stats = await response.json();
        
        const tbody = document.querySelector('#statsTable tbody');
        tbody.innerHTML = '';
        
        for (const sound of {{ active_dictionary.sounds|tojson }}) {
            const row = document.createElement('tr');
            row.innerHTML = `
                <td>${sound}</td>
                <td>${stats[sound].system_total}</td>
                <td>${stats[sound].user_total}</td>
            `;
            tbody.appendChild(row);
        }
        
        document.getElementById('refreshStatus').textContent = 
            `Updated: ${new Date().toLocaleTimeString()}`;
    } catch (error) {
        console.error('Error refreshing stats:', error);
    }
}

// Refresh every 5 seconds
setInterval(refreshStats, 5000);
</script>
{% endblock scripts %}

{% endblock content %} 
********************************************************************************

### File: src/templates/model_status.html ###

{% extends "base.html" %}

{% block content %}
<h1>Model Status</h1>

{% if model_trained %}
    <p>The model is trained and ready for use.</p>
    <!-- Display metrics if available -->
    {% if metrics %}
        <!-- Render metrics here -->
    {% endif %}
{% else %}
    <p>No trained model found. Please train a model.</p>
{% endif %}

<a href="{{ url_for('ml.train_model') }}">Train Model</a>
{% endblock %} 
********************************************************************************

### File: src/templates/model_summary.html ###

{% extends "base.html" %}

{% block content %}
<div class="container mt-5">
    <!-- Add navigation buttons -->
    <div class="mb-4">
        <a href="{{ url_for('ml.predict') }}" class="btn btn-primary">Go to Prediction</a>
        <a href="{{ url_for('ml.train_model') }}" class="btn btn-secondary">Train Another Model</a>
        <a href="{{ url_for('index') }}" class="btn btn-info">Back to Home</a>
    </div>

    <h1>{{ "CNN Training Summary" if training_stats and training_stats.get("classes") else "Training Summary" }}</h1>

    <h2>Data Statistics</h2>
    <table class="table">
        <thead>
            <tr>
                <th>Statistic</th>
                <th>Value</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Input Shape</td>
                <td>
                    {{ training_stats.input_shape if training_stats.input_shape else '(13, 32, 1)' }}
                </td>
                <td>
                    The shape of the input data. 
                    <br/>
                    <strong>Explanation:</strong> Think of each sample as a small grid of numbers that describe the sound's frequency (height) and time segments (width), plus 1 channel for amplitude. 
                    <br/>
                    For example, (13,32,1) means we have 13 frequency bins, 32 "time steps," and 1 channel. It's like a black-and-white image of 13 by 32 pixels for each audio.
                </td>
            </tr>
            <tr>
                <td>Input Range</td>
                <td>
                    {{ training_stats.input_range if training_stats.input_range else '(No numeric range found)' }}
                </td>
                <td>
                    The minimum and maximum values in the input data (helpful to understand scaling).
                    {% if not training_stats.input_range %}
                        <br/><strong>Note:</strong> We only see "MFCC Features" or generic text instead of a numeric range because the code did not store exact min/max values. 
                    {% endif %}
                </td>
            </tr>
            <tr>
                <td>Label Shape</td>
                <td>{{ training_stats.total_samples if training_stats.total_samples else '??' }} samples</td>
                <td>
                    This means we have that many labeled examples. 
                    <br/><strong>Explanation:</strong> We had {{ training_stats.total_samples }} short sounds, each tagged with the correct word ("oh" or "eh"), so the computer knows what each example is.
                </td>
            </tr>
            <tr>
                <td>Unique Labels</td>
                <td>
                    {{ training_stats.original_counts.keys()|list if training_stats.original_counts else training_stats.get('classes', []) }}
                </td>
                <td>
                    The distinct class labels present in the dataset. 
                    <br/>
                    <strong>Why is this important?</strong> It tells us how many different words or categories we're teaching the CNN to recognize.
                </td>
            </tr>
            <tr>
                <td>Label Mapping</td>
                <td>
                    {% if training_stats.original_counts %}
                        {% for class_name, count in training_stats.original_counts.items() %}
                            {{ class_name }}: {{ count }} original, {{ training_stats.augmented_counts[class_name] }} augmented<br>
                        {% endfor %}
                    {% elif training_stats.classes %}
                        {% for cls in training_stats.classes %}
                            {{ loop.index0 }} -> {{ cls }} <br/>
                        {% endfor %}
                    {% else %}
                        No mapping available
                    {% endif %}
                </td>
                <td>
                     A list mapping label indices (0,1,2...) to word classes (like "eh", "oh").
                     <br/>
                     <strong>Explanation (9th grade):</strong> The computer uses numbers (0 or 1) to represent each word. This row shows how each number matches a word.
                </td>
            </tr>
        </tbody>
    </table>

    <h2>Feature Statistics</h2>
    {% if feature_stats %}
    <h3>MFCC Features</h3>
    <table class="table">
        <tr>
            <th>Feature Type</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>Max</th>
        </tr>
        <tr>
            <td>First MFCC (Energy)</td>
            <td>{{ '%.4f'|format(feature_stats.first_mfcc.mean) }}</td>
            <td>{{ '%.4f'|format(feature_stats.first_mfcc.std) }}</td>
            <td>{{ '%.4f'|format(feature_stats.first_mfcc.min) }}</td>
            <td>{{ '%.4f'|format(feature_stats.first_mfcc.max) }}</td>
        </tr>
        <tr>
            <td>Other MFCCs</td>
            <td>{{ '%.4f'|format(feature_stats.other_mfcc.mean) }}</td>
            <td>{{ '%.4f'|format(feature_stats.other_mfcc.std) }}</td>
            <td>{{ '%.4f'|format(feature_stats.other_mfcc.min) }}</td>
            <td>{{ '%.4f'|format(feature_stats.other_mfcc.max) }}</td>
        </tr>
    </table>

    <h3>Other Features</h3>
    <table class="table">
        <tr>
            <th>Feature Type</th>
            <th>Shape</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>Max</th>
        </tr>
        {% for feature_name in ['delta', 'delta2', 'centroid', 'rolloff', 'rms'] %}
        <tr>
            <td>{{ feature_name }}</td>
            <td>{{ feature_stats[feature_name].shape }}</td>
            <td>{{ '%.4f'|format(feature_stats[feature_name].mean) }}</td>
            <td>{{ '%.4f'|format(feature_stats[feature_name].std) }}</td>
            <td>{{ '%.4f'|format(feature_stats[feature_name].min) }}</td>
            <td>{{ '%.4f'|format(feature_stats[feature_name].max) }}</td>
        </tr>
        {% endfor %}
    </table>

    <h3>Normalization Effect</h3>
    <table class="table">
        <tr>
            <th>Stage</th>
            <th>Shape</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>Max</th>
        </tr>
        <tr>
            <td>Pre-normalization</td>
            <td>{{ feature_stats.pre_normalization.shape }}</td>
            <td>{{ '%.4f'|format(feature_stats.pre_normalization.mean) }}</td>
            <td>{{ '%.4f'|format(feature_stats.pre_normalization.std) }}</td>
            <td>{{ '%.4f'|format(feature_stats.pre_normalization.min) }}</td>
            <td>{{ '%.4f'|format(feature_stats.pre_normalization.max) }}</td>
        </tr>
        <tr>
            <td>Post-normalization</td>
            <td>N/A</td>
            <td>{{ '%.4f'|format(feature_stats.post_normalization.mean) }}</td>
            <td>{{ '%.4f'|format(feature_stats.post_normalization.std) }}</td>
            <td>{{ '%.4f'|format(feature_stats.post_normalization.min) }}</td>
            <td>{{ '%.4f'|format(feature_stats.post_normalization.max) }}</td>
        </tr>
    </table>
    {% else %}
    <p>No feature statistics available. This might mean we did not compute MFCC or other features in the code.</p>
    {% endif %}

    {% if training_stats and training_stats.get('energy_comparison') %}
    <h3>Energy Coefficient Comparison Between Classes</h3>
    <table class="table">
        <tr>
            <th>Class</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>Max</th>
        </tr>
        <tr>
            <td>'eh' sounds</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('eh', {}).get('mean', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('eh', {}).get('std', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('eh', {}).get('min', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('eh', {}).get('max', 0)) }}</td>
        </tr>
        <tr>
            <td>'oh' sounds</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('oh', {}).get('mean', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('oh', {}).get('std', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('oh', {}).get('min', 0)) }}</td>
            <td>{{ '%.4f'|format(training_stats.get('energy_comparison', {}).get('oh', {}).get('max', 0)) }}</td>
        </tr>
    </table>
    {% else %}
    <p>No energy coefficient comparison found. Possibly the code to compute it was never called or is commented out.</p>
    {% endif %}

    {% if training_stats %}
    <h3>MFCC Coefficients Comparison Between Classes</h3>
    {% set any_data = false %}
    <table class="table">
        <tr>
            <th>MFCC Feature</th>
            <th>Class</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>Max</th>
            <th>Mean Difference</th>
        </tr>
        {% for i in range(13) %}
        {% set mfcc_key = 'mfcc_' ~ i ~ '_comparison' if i > 0 else 'energy_comparison' %}
        {% if training_stats.get(mfcc_key) %}
        {% set any_data = true %}
        <tr>
            <td rowspan="2">MFCC_{{ i }} {% if i == 0 %}(Energy){% endif %}</td>
            <td>'eh' sounds</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].eh.mean) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].eh.std) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].eh.min) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].eh.max) }}</td>
            <td rowspan="2">{{ '%.4f'|format(training_stats[mfcc_key].eh.mean - training_stats[mfcc_key].oh.mean) }}</td>
        </tr>
        <tr>
            <td>'oh' sounds</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].oh.mean) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].oh.std) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].oh.min) }}</td>
            <td>{{ '%.4f'|format(training_stats[mfcc_key].oh.max) }}</td>
        </tr>
        {% endif %}
        {% endfor %}
    </table>
    {% if not any_data %}
    <p>No MFCC comparison data to display. This could be because your code to compute MFCC differences is disabled or didn't run.</p>
    {% endif %}
    {% endif %}

    <h2>Model Architecture</h2>
    <pre>{{ model_summary }}</pre>
    <p><strong>Hover over layer names to learn more:</strong></p>
    <p>
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="The input layer defines the shape of data entering the network. 'None' means the batch can be any size. (63,64,1) means each sample has 63 'time/frequency steps' by 64 'frequency bins' and 1 channel.">
            InputLayer
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Conv2D (2D Convolution) filters your audio spectrogram image, extracting patterns of frequencies over time. 16 or 32 filters each produce a new 'channel' of features.">
            Conv2D
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Batch Normalization normalizes the outputs of a layer, stabilizing training and potentially speeding it up.">
            BatchNormalization
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Activation applies a non-linear function (like ReLU) so your model can learn complex patterns.">
            Activation
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="MaxPooling2D cuts down on height/width, reducing computation and helping to avoid overfitting.">
            MaxPooling2D
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Dropout randomly sets a fraction of inputs to zero, helping the model generalize instead of memorize.">
            Dropout
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Flatten collapses your 2D feature maps into a 1D vector so they can be fed into dense (fully connected) layers.">
            Flatten
        </span>, 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Dense means every input unit connects to every output unit in this layer. '256' is the number of neurons in that layer.">
            Dense(256)
        </span>, 
        ... 
        <span data-bs-toggle="tooltip" data-bs-placement="right" title="Non-trainable params are things like scaling factors for batch norm that may not update or remain frozen if set so. Trainable params are all the weights the model can adjust.">
            Trainable vs Non-Trainable
        </span>.
    </p>
</div>

<h2>Explanation of Model Parameters</h2>
<p>The model is a Convolutional Neural Network (CNN) designed for audio classification. Here's a breakdown of each parameter:</p>
<ul>
    <li><strong>Input Shape:</strong> We see something like (63,64,1). This means each audio spectrogram is 63 "rows," 64 "columns," and 1 channel (like grayscale).</li>
    <li><strong>Convolutional Layers:</strong> They take small patches of the input (like 3×3 squares) and learn filters that respond to certain frequency/time patterns.</li>
    <li><strong>Pooling Layers:</strong> We reduce the size of the representation, so we don't get stuck with too huge a matrix. This helps control overfitting.</li>
    <li><strong>Dense Layers:</strong> Classic fully-connected layers that combine everything learned by the convolutional layers to decide which class it is.</li>
    <li><strong>Activation Functions:</strong> They make the model non-linear, so it can detect more complex patterns.</li>
    <li><strong>Dropout Layer:</strong> Means that some fraction (like 25%) of neurons are temporarily ignored during training, which helps reduce memorization.</li>
</ul>

<h2>Expectations from the Model</h2>
{% if training_stats['num_classes'] == 1 %}
    <p>Since the model is trained only on one sound class ("{{ training_stats['label_mapping'][0] }}"), it will learn to recognize patterns associated with that sound. However, it won't be able to distinguish "{{ training_stats['label_mapping'][0] }}" from other sounds because it hasn't been trained on any other class. Essentially, the model will classify any input it receives as "{{ training_stats['label_mapping'][0] }}" because that's the only class it knows.</p>

    <h2>Recommendations</h2>
    <p>For the model to be useful in distinguishing "{{ training_stats['label_mapping'][0] }}" from other sounds, you need to introduce additional classes or a negative class representing other sounds.</p>
    <ul>
        <li><strong>Add More Classes:</strong> Include other sounds you wish the model to distinguish from "{{ training_stats['label_mapping'][0] }}".</li>
        <li><strong>Binary Classification:</strong> Introduce a "not {{ training_stats['label_mapping'][0] }}" class with various other sounds, allowing the model to learn the difference between "{{ training_stats['label_mapping'][0] }}" and "not {{ training_stats['label_mapping'][0] }}".</li>
    </ul>
{% else %}
    <p>The model is trained on {{ training_stats['num_classes'] }} classes: {{ training_stats['label_mapping'] if training_stats['label_mapping'] else training_stats.get('classes', []) }}. It has learned to recognize patterns associated with each sound and can distinguish between them based on the training data provided.</p>

    <h2>Recommendations</h2>
    <p>To improve the model:</p>
    <ul>
        <li><strong>Increase Data Quality:</strong> Ensure that each class has sufficient and diverse examples.</li>
        <li><strong>Data Augmentation:</strong> Apply techniques like noise addition, time-stretching, and pitch shifting to augment your dataset.</li>
        <li><strong>Balance Classes:</strong> Make sure that the number of samples in each class is balanced to prevent bias.</li>
        <li><strong>Regularization Techniques:</strong> Use techniques like dropout and weight decay to prevent overfitting.</li>
    </ul>
{% endif %}

<!-- Include training performance if available -->
{% if training_history %}
<h2>Training Performance</h2>
<table class="table">
    <tr>
        <th>Epoch</th><th>Accuracy</th><th>Validation Accuracy</th><th>Loss</th><th>Validation Loss</th>
    </tr>
    {% for i in range(training_history['epochs']) %}
    <tr>
        <td>{{ i+1 }}</td>
        <td>{{ '%.4f'|format(training_history['accuracy'][i]) }}</td>
        <td>{{ '%.4f'|format(training_history['val_accuracy'][i]) }}</td>
        <td>{{ '%.4f'|format(training_history['loss'][i]) }}</td>
        <td>{{ '%.4f'|format(training_history['val_loss'][i]) }}</td>
    </tr>
    {% endfor %}
</table>
<p>
    <strong>What do these numbers mean?</strong><br/>
    - <strong>Accuracy / Validation Accuracy:</strong> A percentage (0 to 1) of how many examples the model got right. "Validation" is on unseen data.<br/>
    - <strong>Loss / Validation Loss:</strong> A measure of how wrong the model's predictions are. Lower is better.<br/>
    - If your <em>validation accuracy</em> is close to training accuracy, you likely have a well-generalized model. If training accuracy is very high but validation accuracy is notably lower, watch out for overfitting.
</p>
{% endif %}

{% if training_stats.rf_summary %}
<hr/>
<h2>Random Forest Summary</h2>
<p>Below are details of the trained Random Forest:</p>
<table class="table">
    <tr>
        <th>Statistic</th>
        <th>Value</th>
    </tr>
    <tr>
        <td>Number of Samples</td>
        <td>{{ training_stats.rf_summary.num_samples }}</td>
    </tr>
    <tr>
        <td>Number of Classes</td>
        <td>{{ training_stats.rf_summary.num_classes }}</td>
    </tr>
    <tr>
        <td>Model Path</td>
        <td>{{ training_stats.rf_summary.rf_model_path }}</td>
    </tr>
</table>

<h3>Random Forest Recommendations</h3>
<ul>
    <li><strong>Parameter Tuning:</strong> Consider tuning the number of trees, max depth, or min samples split for better performance.</li>
    <li><strong>Feature Importance Analysis:</strong> RF can provide an importance ranking of features (e.g., MFCCs) to see which are most relevant.</li>
</ul>
{% endif %}

{% if training_stats.ensemble_summary %}
<hr/>
<h2>Ensemble Summary</h2>
<p>Below are details of the trained Ensemble Model:</p>
<table class="table">
    <tr>
        <th>Method</th>
        <td>{{ training_stats.ensemble_summary.method }}</td>
    </tr>
    <tr>
        <th>CNN Path</th>
        <td>{{ training_stats.ensemble_summary.cnn_path }}</td>
    </tr>
    <tr>
        <th>RF Path</th>
        <td>{{ training_stats.ensemble_summary.rf_path }}</td>
    </tr>
</table>

<h3>Ensemble Recommendations</h3>
<ul>
    <li><strong>Weight Adjustments:</strong> Adjust the weighting between CNN and RF to see if it improves performance.</li>
    <li><strong>Data Quality:</strong> Ensure both CNN and RF have sufficient data to avoid ensemble bias.</li>
</ul>
{% endif %}

{% if training_stats.cnn_acc and training_stats.rf_acc and training_stats.ensemble_acc %}
<hr/>
<h2>Comparison of CNN, RF, and Ensemble</h2>
<table class="table">
    <tr>
        <th>Method</th>
        <th>Accuracy</th>
    </tr>
    <tr>
        <td>CNN</td>
        <td>{{ '%.3f'|format(training_stats.cnn_acc) }}</td>
    </tr>
    <tr>
        <td>RF</td>
        <td>{{ '%.3f'|format(training_stats.rf_acc) }}</td>
    </tr>
    <tr>
        <td>Ensemble</td>
        <td>{{ '%.3f'|format(training_stats.ensemble_acc) }}</td>
    </tr>
</table>
{% endif %}

<h2>Model Summary</h2>

{% if training_stats.simple_explanation %}
  <p>{{ training_stats.simple_explanation }}</p>
{% endif %}

{% if training_stats.classes %}
  <p>The model was trained on these classes:</p>
  <ul>
    {% for cls in training_stats.classes %}
      <li>{{ cls }}</li>
    {% endfor %}
  </ul>
{% endif %}

<!-- If you want to show training_history as well -->
<h3>Training History</h3>
{% if training_history %}
  <p>Trained for {{ training_history.epochs }} epochs.</p>
  <p>Final Training Accuracy: {{ training_history.accuracy|last|round(3) }}</p>
  <p>Final Validation Accuracy: {{ training_history.val_accuracy|last|round(3) }}</p>
{% endif %}
{% endblock %}

{% block styles %}
<style>
    pre { background: #f4f4f4; padding: 10px; }
    table { border-collapse: collapse; width: 70%; }
    td, th { border: 1px solid #ddd; padding: 8px; vertical-align: top; }
    th { background-color: #f4f4f4; }
</style>
{% endblock %}

{% block scripts %}
<script>
  $(function () {
    $('[data-bs-toggle="tooltip"]').tooltip()
  })
</script>
{% endblock %} 
********************************************************************************

### File: src/templates/model_summary_enhanced.html ###

{% extends "base.html" %}

{% block content %}
<!-- 
  model_summary_enhanced_v2.html
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Enhanced version that:
    1) Removes "(9th grade)" references
    2) Explains the "None" input dimension more clearly
    3) Shows MFCC min/max (if provided in training_stats['mfcc_range'])
    4) Adds a short performance summary at the bottom
    5) Retains all existing functionality
-->

<div class="container mt-5">
    <!-- Add navigation buttons -->
    <div class="mb-4">
        <a href="{{ url_for('ml.predict') }}" class="btn btn-primary">Go to Prediction</a>
        <a href="{{ url_for('ml.train_model') }}" class="btn btn-secondary">Train Another Model</a>
        <a href="{{ url_for('index') }}" class="btn btn-info">Back to Home</a>
    </div>

    <!-- Dynamic title based on model type -->
    <h1>{{ "CNN Training Summary" if training_stats and training_stats.get("classes") else "Training Summary" }}</h1>

    <!-- Data Statistics Section -->
    <h2>Data Statistics</h2>
    <table class="table">
        <thead>
            <tr>
                <th>Statistic</th>
                <th>Value</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>

            <!-- Input Shape -->
            <tr>
                <td>Input Shape</td>
                <td>{{ training_stats.input_shape if training_stats.input_shape else '(Unknown)' }}</td>
                <td>
                    The shape of the input data. 
                    <br/>
                    <strong>Explanation:</strong> Each sample can be thought of as a small grid of numbers describing the sound's frequency (height) vs. time (width). 
                    <br/>
                    If you see something like (63, 64, 1):
                    <ul>
                      <li>63 = number of frequency bins (vertical)</li>
                      <li>64 = number of time steps (horizontal)</li>
                      <li>1 = number of channels (similar to grayscale)</li>
                    </ul>
                    The <code>None</code> in TensorFlow shapes usually indicates the <em>batch size</em>, which is flexible. Each run picks the batch size (often 32) so the model can process batches of different sizes if needed.
                </td>
            </tr>

            <!-- Input Range -->
            <tr>
                <td>Input Range</td>
                <td>
                    {% if training_stats.mfcc_range %}
                        {{ "From {:.3f} to {:.3f}".format(training_stats.mfcc_range[0], training_stats.mfcc_range[1]) }}
                    {% else %}
                        MFCC Features
                    {% endif %}
                </td>
                <td>
                    The minimum and maximum values in the MFCC features. These are Mel-frequency cepstral coefficients, 
                    representing log-energy in various frequency bands. They are dimensionless but can be negative as they 
                    represent a logarithmic scale.
                </td>
            </tr>

            <!-- Label Shape -->
            <tr>
                <td>Label Shape</td>
                <td>{{ training_stats.total_samples if training_stats.total_samples else '0' }} samples</td>
                <td>
                    The number of labeled examples in our dataset.
                    <br/><strong>Simple Explanation:</strong> We have this many short sound clips, each labeled with its correct word (like "oh" or "eh").
                </td>
            </tr>

            <!-- Unique Labels -->
            <tr>
                <td>Unique Labels</td>
                <td>{{ training_stats.classes if training_stats.classes else [] }}</td>
                <td>The different words or sounds we're teaching the computer to recognize.</td>
            </tr>

            <!-- Label Mapping -->
            <tr>
                <td>Label Mapping</td>
                <td>{{ training_stats.label_mapping if training_stats.label_mapping else 'No mapping available' }}</td>
                <td>
                    How we convert between numbers (0, 1, etc.) and actual words ("oh", "eh", etc.).
                    {% if not training_stats.label_mapping %}
                        <br/><strong>Note:</strong> If no mapping is shown, we're using the class names directly.
                    {% endif %}
                </td>
            </tr>
        </tbody>
    </table>

    <!-- Feature Statistics Section -->
    <h2>Feature Statistics</h2>
    {% if training_stats and training_stats.get('mfcc_stats') %}
        <h3>MFCC Coefficients Comparison Between Classes</h3>
        <table class="table">
            <tr>
                <th>MFCC Feature</th>
                <th>Class</th>
                <th>Mean</th>
                <th>Std</th>
                <th>Min</th>
                <th>Max</th>
                <th>Mean Difference</th>
            </tr>
            {% for feature in training_stats.mfcc_stats %}
                <tr>
                    <td>{{ feature.name }}</td>
                    <td>{{ feature.class }}</td>
                    <td>{{ '%.4f'|format(feature.mean) }}</td>
                    <td>{{ '%.4f'|format(feature.std) }}</td>
                    <td>{{ '%.4f'|format(feature.min) }}</td>
                    <td>{{ '%.4f'|format(feature.max) }}</td>
                    <td>{{ '%.4f'|format(feature.mean_diff) }}</td>
                </tr>
            {% endfor %}
        </table>
    {% else %}
        <p>No detailed MFCC statistics found.</p>
    {% endif %}

    <!-- Training Progress Graph -->
    {% if training_history %}
        <h2>Training Progress</h2>
        <canvas id="trainingChart" width="600" height="300"></canvas>
    {% endif %}

    <!-- Performance Summary -->
    <h2>Performance Summary</h2>
    <div class="alert alert-info">
        <ul>
            <li><strong>Model Strengths:</strong> 
              <ul>
                <li>Identifies main sound patterns effectively under normal conditions.</li>
                <li>Fairly balanced accuracy across known classes (based on training data).</li>
              </ul>
            </li>
            <li><strong>Potential Concerns:</strong>
              <ul>
                <li>Model may overfit if the dataset is small or lacks variety.</li>
                <li>Background noise or untrained classes might lower real-world performance.</li>
              </ul>
            </li>
            <li><strong>Suggestions for Improvement:</strong>
              <ul>
                <li>Collect more varied samples to handle noise or different recording conditions.</li>
                <li>Experiment with data augmentation and hyperparameter tuning for better generalization.</li>
                <li>Monitor validation loss closely to catch overfitting early.</li>
              </ul>
            </li>
        </ul>
    </div>
</div>
{% endblock %}


{% block styles %}
{{ super() }}
<style>
    pre { background: #f4f4f4; padding: 10px; }
    table { border-collapse: collapse; width: 70%; }
    td, th { border: 1px solid #ddd; padding: 8px; vertical-align: top; }
    th { background-color: #f4f4f4; }
    .alert {
        padding: 15px;
        margin-bottom: 20px;
        border: 1px solid transparent;
        border-radius: 4px;
    }
    .alert-info {
        background-color: #e9f7fe;
        border-color: #b6effb;
        color: #31708f;
    }
</style>
{% endblock %}


{% block scripts %}
{{ super() }}
<!-- Initialize Bootstrap tooltips (ensure Popper.js and Bootstrap JS are loaded) -->
<script>
    $(function () {
        $('[data-bs-toggle="tooltip"]').tooltip()
    })
</script>

{% if training_history %}
    <!-- Chart.js for visualizing training progress -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        (function() {
            const ctx = document.getElementById("trainingChart").getContext('2d');
            const epochs = {{ training_history.epochs }};
            const accuracyData = {{ training_history.accuracy|tojson }};
            const valAccuracyData = {{ training_history.val_accuracy|tojson }};
            const lossData = {{ training_history.loss|tojson }};
            const valLossData = {{ training_history.val_loss|tojson }};

            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: Array.from({length: epochs}, (_, i) => `Epoch ${i+1}`),
                    datasets: [
                        {
                            label: 'Training Accuracy',
                            data: accuracyData,
                            borderColor: 'blue',
                            fill: false,
                            yAxisID: 'yAcc'
                        },
                        {
                            label: 'Validation Accuracy',
                            data: valAccuracyData,
                            borderColor: 'green',
                            fill: false,
                            yAxisID: 'yAcc'
                        },
                        {
                            label: 'Training Loss',
                            data: lossData,
                            borderColor: 'red',
                            fill: false,
                            yAxisID: 'yLoss'
                        },
                        {
                            label: 'Validation Loss',
                            data: valLossData,
                            borderColor: 'orange',
                            fill: false,
                            yAxisID: 'yLoss'
                        }
                    ]
                },
                options: {
                    responsive: true,
                    scales: {
                        yAcc: {
                            type: 'linear',
                            position: 'left',
                            min: 0,
                            max: 1,
                            title: {
                                display: true,
                                text: 'Accuracy'
                            }
                        },
                        yLoss: {
                            type: 'linear',
                            position: 'right',
                            title: {
                                display: true,
                                text: 'Loss'
                            }
                        }
                    }
                }
            });
        })();
    </script>
{% endif %}
{% endblock %} 
********************************************************************************

### File: src/templates/process_flow.html ###

{% extends "base.html" %}

{% block content %}
<div class="process-container">
    <h2><i class="fas fa-tasks"></i> Recording Process</h2>
    
    <div class="process-steps">
        <div class="step {% if active_step >= 1 %}active{% endif %}">
            <div class="step-number">1</div>
            <div class="step-content">
                <h3>Record Sounds</h3>
                <p>Record yourself saying each sound from the dictionary.</p>
                {% if not has_recordings %}
                    <a href="{{ url_for('index') }}" class="btn btn-primary">
                        <i class="fas fa-microphone"></i> Start Recording
                    </a>
                {% else %}
                    <span class="badge bg-success"><i class="fas fa-check"></i> Completed</span>
                {% endif %}
            </div>
        </div>

        <div class="step {% if active_step >= 2 %}active{% endif %}">
            <div class="step-number">2</div>
            <div class="step-content">
                <h3>Verify Recordings</h3>
                <p>Listen to each recording and verify its quality.</p>
                {% if has_recordings and not has_verified %}
                    <a href="{{ url_for('ml.verify_chunks', timestamp='latest') }}" class="btn btn-primary">
                        <i class="fas fa-check-circle"></i> Start Verifying
                    </a>
                {% elif has_verified %}
                    <span class="badge bg-success"><i class="fas fa-check"></i> Completed</span>
                {% else %}
                    <span class="badge bg-secondary">Not Available</span>
                {% endif %}
            </div>
        </div>

        <div class="step {% if active_step >= 3 %}active{% endif %}">
            <div class="step-number">3</div>
            <div class="step-content">
                <h3>Review Collection</h3>
                <p>Review your collection of verified recordings.</p>
                {% if has_verified %}
                    <a href="{{ url_for('ml.list_recordings') }}" class="btn btn-primary">
                        <i class="fas fa-list"></i> View Recordings
                    </a>
                {% else %}
                    <span class="badge bg-secondary">Not Available</span>
                {% endif %}
            </div>
        </div>
    </div>
</div>

<style>
.process-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2rem;
    background: white;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.process-steps {
    display: flex;
    flex-direction: column;
    gap: 2rem;
    margin-top: 2rem;
}

.step {
    display: flex;
    gap: 1rem;
    padding: 1rem;
    border: 1px solid #dee2e6;
    border-radius: 8px;
    opacity: 0.7;
    transition: all 0.3s ease;
}

.step.active {
    opacity: 1;
    border-color: #007bff;
    background: #f8f9fa;
}

.step-number {
    width: 40px;
    height: 40px;
    background: #007bff;
    color: white;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: bold;
    font-size: 1.2em;
}

.step-content {
    flex: 1;
}

.step-content h3 {
    margin: 0 0 0.5rem 0;
    font-size: 1.2em;
}

.step-content p {
    margin: 0 0 1rem 0;
    color: #666;
}

.badge {
    padding: 0.5rem 1rem;
}
</style>
{% endblock %} 
********************************************************************************

### File: src/templates/record.html ###

{% extends "base.html" %}

{% block content %}
<div class="record-container">
    <h2><i class="fas fa-microphone"></i> Record Sounds</h2>
    <p class="instructions">
        1. Select a sound from the list<br>
        2. Click "Start Recording" and say the sound clearly, with brief pauses<br>
        3. Click "Stop Recording" when done<br>
        4. Verify each recorded sound chunk
        <small class="text-muted d-block mt-2">
            Tip: For best results, say the sound multiple times (3-7 recommended) with clear pauses between each.
            The system will automatically detect and separate the sounds.
        </small>
    </p>

    <div class="record-form">
        <div class="form-group mb-4">
            <label for="sound"><i class="fas fa-music"></i> Select Sound:</label>
            <select id="sound" name="sound" class="form-control" required>
                <option value="">Choose a sound...</option>
                {% for sound in sounds %}
                <option value="{{ sound }}">{{ sound }}</option>
                {% endfor %}
            </select>
        </div>

        <div class="recording-controls">
            <button id="startRecording" class="btn btn-primary">
                <i class="fas fa-microphone"></i> Start Recording
            </button>
            <button id="stopRecording" class="btn btn-danger" style="display: none;">
                <i class="fas fa-stop-circle"></i> Stop Recording
            </button>
        </div>

        <div id="recordingStatus" class="recording-status" style="display: none;">
            <div class="pulse-ring"></div>
            <span>Recording in progress...</span>
        </div>
    </div>
</div>

<style>
.record-container {
    max-width: 600px;
    margin: 2rem auto;
    padding: 2rem;
    border: 1px solid #ddd;
    border-radius: 8px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.instructions {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 4px;
    margin-bottom: 2rem;
    color: #666;
}

.recording-status {
    display: flex;
    align-items: center;
    gap: 1rem;
    margin-top: 1rem;
    color: #dc3545;
}

.pulse-ring {
    width: 1rem;
    height: 1rem;
    background: #dc3545;
    border-radius: 50%;
    animation: pulse 1s infinite;
}

@keyframes pulse {
    0% { transform: scale(0.95); opacity: 0.9; }
    70% { transform: scale(1.1); opacity: 0.8; }
    100% { transform: scale(0.95); opacity: 0.9; }
}

.recording-controls {
    display: flex;
    gap: 1rem;
}
</style>
{% endblock %}

{% block scripts %}
<script>
// Declare variables in a scope to avoid global conflicts
const recordingApp = {
    mediaRecorder: null,
    audioChunks: [],

    async playBeep() {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = audioContext.createOscillator();
        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(880, audioContext.currentTime); // A5 note
        oscillator.connect(audioContext.destination);
        oscillator.start();
        oscillator.stop(audioContext.currentTime + 0.2); // 200ms beep
        return new Promise(resolve => setTimeout(resolve, 220)); // Wait for beep + 20ms
    },

    async startRecording() {
        const sound = document.querySelector('select[name="sound"]').value;
        if (!sound) {
            alert('Please select a sound first');
            return;
        }

        try {
            await this.playBeep(); // Play beep before starting recording
            console.log('Starting recording for sound:', sound);
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            this.mediaRecorder = new MediaRecorder(stream);
            this.audioChunks = [];
            
            this.mediaRecorder.ondataavailable = (event) => {
                console.log('Audio data available:', event.data.size, 'bytes');
                this.audioChunks.push(event.data);
            };
            
            this.mediaRecorder.onstop = async () => {
                console.log('Recording stopped, processing...');
                const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
                console.log('Created audio blob:', audioBlob.size, 'bytes');
                const formData = new FormData();
                formData.append('audio', audioBlob);
                formData.append('sound', sound);
                
                document.getElementById('recordingStatus').style.display = 'none';
                
                try {
                    console.log('Sending recording to server...');
                    const response = await fetch("{{ url_for('ml.predict') }}", {
                        method: 'POST',
                        body: formData
                    });
                    
                    console.log('Server response status:', response.status);
                    if (response.ok) {
                        window.location.href = response.url;
                    } else {
                        const errorText = await response.text();
                        alert(`Error saving recording: ${errorText}`);
                        console.error('Server error:', errorText);
                    }
                } catch (error) {
                    console.error('Network error:', error);
                    alert('Network error while saving recording');
                }
            };
            
            this.mediaRecorder.start();
            console.log('MediaRecorder started');
            document.getElementById('startRecording').style.display = 'none';
            document.getElementById('stopRecording').style.display = 'block';
            document.getElementById('recordingStatus').style.display = 'flex';
        } catch (error) {
            console.error('Error:', error);
            alert('Error accessing microphone. Please ensure microphone permissions are granted.');
        }
    },

    stopRecording() {
        this.mediaRecorder.stop();
        this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
        document.getElementById('startRecording').style.display = 'block';
        document.getElementById('stopRecording').style.display = 'none';
        document.getElementById('recordingStatus').style.display = 'none';
    }
};

// Set up event listeners
document.getElementById('startRecording').onclick = () => recordingApp.startRecording();
document.getElementById('stopRecording').onclick = () => recordingApp.stopRecording();
</script>
{% endblock %} 
********************************************************************************

### File: src/templates/register.html ###

{% extends "base.html" %}

{% block content %}
<div class="register-container">
    <div class="register-box">
        <h2><i class="fas fa-user-plus"></i> Register New User</h2>
        <form method="POST" action="{{ url_for('register') }}" class="mt-3">
            <div class="form-group mb-3">
                <label for="username">Choose a Username</label>
                <input type="text" 
                       id="username"
                       name="username" 
                       class="form-control" 
                       placeholder="Enter Username" 
                       required>
            </div>
            <div class="d-grid gap-2">
                <button type="submit" class="btn btn-primary">
                    <i class="fas fa-user-plus"></i> Register
                </button>
                <a href="{{ url_for('index') }}" class="btn btn-outline-secondary">
                    <i class="fas fa-arrow-left"></i> Back to Login
                </a>
            </div>
        </form>
    </div>
</div>

<style>
.register-container {
    max-width: 400px;
    margin: 2rem auto;
}

.register-box {
    padding: 2rem;
    border: 1px solid #ddd;
    border-radius: 8px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
</style>
{% endblock %} 
********************************************************************************

### File: src/templates/train_model.html ###

{% extends "base.html" %}

{% block content %}
<div class="container mt-5">
    <div class="row">
        <div class="col-md-8 offset-md-2">
            <div class="card">
                <div class="card-header">
                    <h2>Train Model</h2>
                </div>
                <div class="card-body">
                    <p>Current sounds in dictionary:</p>
                    <ul>
                        {% for sound in sounds %}
                        <li>{{ sound }}</li>
                        {% endfor %}
                    </ul>

                    <!-- Adjust the form's action to point to your blueprint route -->
                    <form id="train-form" method="POST" action="{{ url_for('ml.train_model') }}">
                        <div class="form-group mb-3">
                            <label for="train_method">Select Training Method:</label>
                            <select name="train_method" id="train_method" class="form-control">
                                <option value="cnn">CNN Only</option>
                                <option value="rf">Random Forest Only</option>
                                <option value="ensemble">Ensemble Only</option>
                                <option value="all">Train Both + Ensemble</option>
                            </select>
                        </div>
                        <button type="submit" class="btn btn-primary">
                            <i class="fas fa-play"></i> Start Training
                        </button>
                    </form>

                    <div id="training-status" class="d-none mt-4">
                        <div class="progress mb-3">
                            <div class="progress-bar progress-bar-striped progress-bar-animated"
                                 role="progressbar" style="width: 0%">
                            </div>
                        </div>
                        <p id="status-text" class="text-center">Training in progress...</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
document.getElementById('train-form').addEventListener('submit', function() {
    // Show progress bar
    document.getElementById('training-status').classList.remove('d-none');
    // Optionally start polling after a short delay
    setTimeout(updateProgress, 1000);
});

function updateProgress() {
    // If your /training_status route is under the ml blueprint, use url_for('ml.training_status').
    // If it's still in app.py, keep it as fetch('/training_status').
    fetch("{{ url_for('ml.training_status') }}")
        .then(res => res.json())
        .then(data => {
            const bar = document.querySelector('.progress-bar');
            const statusText = document.getElementById('status-text');
            bar.style.width = data.progress + '%';
            statusText.textContent = data.status;
            if (data.progress < 100) {
                setTimeout(updateProgress, 1000);
            }
        })
        .catch(err => console.error(err));
}
</script>
{% endblock %}

********************************************************************************

### File: src/templates/upload_sounds.html ###

{% extends "base.html" %}

{% block content %}
<div class="upload-container">
    <h2><i class="fas fa-upload"></i> Upload Sound Files</h2>
    <p class="instructions">
        1. Select the sound type from the list<br>
        2. Choose one or more .wav files to upload<br>
        3. Click Upload to process the files<br>
        4. Verify the detected sound chunks
    </p>

    <div class="upload-form">
        <form method="POST" action="{{ url_for('ml.process_uploads') }}" enctype="multipart/form-data">
            <div class="form-group mb-4">
                <label for="sound"><i class="fas fa-music"></i> Select Sound:</label>
                <select id="sound" name="sound" class="form-control" required>
                    <option value="">Choose a sound...</option>
                    {% for sound in sounds %}
                    <option value="{{ sound }}">{{ sound }}</option>
                    {% endfor %}
                </select>
            </div>
            
            <div class="form-group mb-4">
                <label for="files">Select WAV Files:</label>
                <input type="file" 
                       id="files" 
                       name="files" 
                       class="form-control" 
                       accept=".wav" 
                       multiple 
                       required>
                <small class="form-text text-muted">
                    You can select multiple .wav files at once
                </small>
            </div>

            <button type="submit" class="btn btn-primary">
                <i class="fas fa-upload"></i> Upload and Process
            </button>
        </form>
    </div>
</div>

<style>
.upload-container {
    max-width: 600px;
    margin: 2rem auto;
    padding: 2rem;
    border: 1px solid #ddd;
    border-radius: 8px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.instructions {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 4px;
    margin-bottom: 2rem;
    color: #666;
}
</style>
{% endblock %} 
********************************************************************************

### File: src/templates/verify.html ###

{% extends "base.html" %}

{% block content %}
<div class="verify-container">
    <h2><i class="fas fa-check-circle"></i> Verify Sound Chunks</h2>
    <p class="instructions">
        Listen to each chunk and verify if it's a good recording of the sound.
        Click "Keep" for good recordings, "Delete" for bad ones.
    </p>

    {% with messages = get_flashed_messages() %}
        {% if messages %}
            {% for message in messages %}
                <div class="alert alert-info">
                    <i class="fas fa-info-circle"></i> {{ message }}
                </div>
            {% endfor %}
        {% endif %}
    {% endwith %}

    {% if chunks %}
    <div class="chunks-list">
        {% for chunk in chunks %}
        <div class="chunk-item">
            <div class="chunk-info">
                <span class="sound-name">{{ chunk.split('_')[0] }}</span>
                <span class="chunk-count">({{ loop.index }}/{{ chunks|length }})</span>
            </div>
            
            <audio controls src="{{ url_for('static', filename='temp/' + chunk) }}" class="chunk-audio"></audio>
            
            <div class="chunk-actions">
                <form method="POST" action="{{ url_for('ml.process_verification') }}" class="d-inline">
                    <input type="hidden" name="chunk_file" value="{{ chunk }}">
                    <button type="submit" name="is_good" value="true" class="btn btn-success">
                        <i class="fas fa-check"></i> Keep
                    </button>
                    <button type="submit" name="is_good" value="false" class="btn btn-danger">
                        <i class="fas fa-trash"></i> Delete
                    </button>
                </form>
            </div>
        </div>
        {% endfor %}
    </div>
   
    <div class="mt-4">
        <a href="{{ url_for('ml.list_recordings') }}" class="btn btn-primary">
            <i class="fas fa-list"></i> View All Recordings
        </a>
    </div>
    {% else %}
    <div class="alert alert-info">
        <i class="fas fa-info-circle"></i> No chunks to verify.
        <a href="{{ url_for('ml.list_recordings') }}" class="btn btn-link">View your recordings</a>
    </div>
    {% endif %}
</div>

<style>
.verify-container {
    max-width: 800px;
    margin: 2rem auto;
    padding: 2rem;
    background: #fff;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.instructions {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 4px;
    margin-bottom: 2rem;
    color: #666;
}

.chunks-list {
    display: flex;
    flex-direction: column;
    gap: 1rem;
}

.chunk-item {
    display: flex;
    align-items: center;
    gap: 1rem;
    padding: 1rem;
    border: 1px solid #ddd;
    border-radius: 8px;
    background: #f8f9fa;
}

.chunk-info {
    min-width: 100px;
    display: flex;
    flex-direction: column;
    align-items: center;
}

.sound-name {
    font-weight: 500;
    color: #007bff;
    font-size: 1.2em;
    text-transform: uppercase;
}

.chunk-count {
    color: #666;
    font-size: 0.9em;
}

.chunk-audio {
    flex-grow: 1;
    max-width: 300px;
}

.chunk-actions {
    display: flex;
    gap: 0.5rem;
}

.btn {
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.5rem 1rem;
}

.btn i {
    font-size: 0.9em;
}
</style>
{% endblock %} 
********************************************************************************

### File: src/templates/view_analysis.html ###

 
********************************************************************************

